[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "",
    "text": "Dear student and aspiring data engineer\nThe goal of this course is not to help you learn how to tune the parameters of the latest kind of deep network, or how to efficiently handle large amounts of data, or how to do cross-validation in the fastest way, or what is the latest improvement in random-forest algorithms.\nThe goal of this course is to help you learn the principles to build the machine-learning algorithms and AI devices of the future. And, as a side effect, you’ll also learn how to concretely improve present-day algorithms, and also how to determine if any of them has already reached its maximal theoretical performance.\nHow can such a goal be achieved?\nThere is a small set of rules and one method that are mathematically guaranteed to output the optimal solution of any inference, prediction, classification, regression, and decision problem. You can think of this as the “unbeatable universal machine”. Or, from an AI point of view, you can think of these rules and method as the “laws of robotics” that should govern any ideal AI designed to draw inferences and make decisions.\nApproximations evolve toward the maximally optimal ideal method. The approximations used at any given time in history exploit the computational technologies then available. Deep networks, for instance, would have been a useless approximation 50 years ago, before the introduction of Graphical Processing Units.\nEvery new technological advance (think of possibly forthcoming quantum computers) opens up possibilities for new approximations that get us closer to the ideal optimum. To see and realize these possibilities, or to judge whether they have already been realized, a data scientist needs at the very least:\n-   to know the foundation of the maximally optimal method\n-   to think outside the box\nWithout the first requirement, how do you know what is the target to approximate towards, and how far you are from it? You risk:\n making an approximation that leads to worse results than before;\n evaluating the approximation in the wrong way, so you don’t even realize it’s worse than before;\n trying to improve an approximation that has already attained the theoretical optimum. Think about an engine that has already the maximal efficiency dictated by thermodynamics; and an engineer, ignorant of thermodynamics, who wastes effort in trying to improve it further.\nWithout the second requirement, you risk missing to take full advantage of the new technological possibilities. Consider the evolution of transportation: if you keep thinking in terms of how to improve a horse-carriage wooden wheels, you’ll never conceive a combustion engine; if you keep thinking in terms of how to improve combustion fuel, you’ll never conceive an electric motor. Existing approximations may of course be good starting points; but you need to clearly understand how they approximate the ideal optimum – so we’re back to the first requirement.\nIf you want to make advances in machine learning and AI, you must know how the ideal universal algorithm looks like, and you must not limit yourself to thinking of “training sets”, “cross-validation”, “supervised learning”, “overfitting”, “models”, and similar notions. In this course you’ll see for yourself that such notions are anchored to the present-day box of approximations.\nAnd we want to think outside that box.\nBut don’t worry: this course does not only want to prepare you for the future. With the knowledge and insights acquired, you will be able to propose and implement concrete improvements to present-day methods as well, or calculate whether they can’t be improved further."
  },
  {
    "objectID": "index.html#your-role-in-the-course-bugs-features",
    "href": "index.html#your-role-in-the-course-bugs-features",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "Your role in the course Bugs & features",
    "text": "Your role in the course Bugs & features\nThis course is still in an experimental, “alpha” version. So you will not only learn something from it (hopefully), but also test it together with us, and help improving it for future students. Thank you for this in advance!\nFor this reason it’s good to clarify some goals and guidelines of this course: \n\n  Light maths requirements\n\nWe believe that the fundamental rules and methods can be understood and also used (at least in not too complex applications) without complex mathematics. Indeed the basic laws of inference and decision-making involve only the four basic operations \\(+ - \\times /\\). So this course only requires maths at a beginning first-year undergraduate level.\n\n\n\n  Informal style\n\nThe course notes are written in an informal style; for example they are not developed along “definitions”, “lemmata”, “theorems”. This does not mean that they are inexact. We will warn you about parts that are oversimplified or that only cover special contexts.\n\n\n\n  Names don’t constitute knowledge\n\n\n\n\nIn these course notes you’ll often stumble upon terms in blue bold and definitions in blue Italics. This typographic emphasis does not mean that those terms and definitions should be memorized: rather, it means that there are important ideas around there which you must try to understand and use. In fact we don’t care which terminology you adopt. Instead of the term statistical population, feel free to use the term pink apple if you like, as long you explain the terms you use by means of a discussion and examples.2 What’s important is that you know, can recognize, and can correctly use the ideas behind technical terms.\nMemorizing terms, definitions, and where to use them, is how large language models (like chatGPT) operate. If your study is just memorization of terms, you’ll have difficulties finding jobs in the future, because there will be algorithms that can do that better and at a cheaper cost than you.\n2 Some standard technical terms are no better. The common term random variable, for instance, often denotes something that is actually not “random” and not variable. Go figure. Using the term green banana would be less misleading!\n\n  Diverse textbooks\n\nThis course does not have only one textbook: it refers to and merges together parts from several books and articles. As you read these works, you will notice that they adopt quite different terminologies, employ different symbolic notations, give different definitions for similar ideas, and sometimes even contradict each other.\nThese differences and contradictions are a feature, not a bug!\nYou might think that this makes studying more difficult; but it actually helps you to really understand an idea and acquire real knowledge, because it forces you to go beyond words, symbols, and specific points of view and examples. This point connects with the previous point, “names don’t constitute knowledge”. The present course notes will help you build comprehension bridges across those books.\n\n\n\n  Artificial intelligence\n\nIn order to grasp and use the fundamental laws of inference and decision-making, we shall use notions that are also at the foundations of Artificial Intelligence (and less common in present-day machine learning). So you’ll also get a light introduction to AI for free. Indeed, a textbook that we’ll draw frequently from is Russell & Norvig’s Artificial Intelligence: A Modern Approach (we’ll avoid part V on machine learning, however, because it’s poorly explained and written).\n\n\n\n\n  Concrete examples\n\nSome students find it easier to grasp an idea by starting from an abstract description and then examining concrete examples; some find it easier the other way around. We try to make both happy by alternating between the two approaches. Ideas and notions are always accompanied by examples that we try to keep simple yet realistic, drawing from scenarios ranging from glass forensics to hotel booking.\n\n\n\n  Code\n\nWe shall perform inferences on concrete datasets, also comparing different methodologies. Most of these can be performed with any specific programming language, so you can use your favourite one – remember that we want to try to think outside the box of present-day technologies, and that includes present-day programming languages. Most examples in class and in exercises will be given in R and Python, but are easily translated into other languages.\n\n\n\n  Extra material\n\nThe course has strong connections with many other disciplines, such as formal logic, proof theory, psychology, philosophy, physics, and environmental sciences. We have tried to provide a lot of extra reading material in “For the extra curious” side boxes, for those who want to deepen their understanding of topics covered or just connected to the present course. Maybe you’ll stumble into a new passion or even into your life call?\n\n\n\n\n\n\n\n\n For the extra curious"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "ADA511 0.1 Data science and data-driven engineering",
    "section": "Course structure",
    "text": "Course structure\nThe course structure reflects the way in which the ideal universal decision-making machine works. It can be roughly divided into three or four parts, illustrated as follows (this is just a caricature, don’t take this diagram too literally):\n\n\n\n\nflowchart TB\n  P{{probability}} --o Z[/max expected utility\\]\n  subgraph probability [ ]\n  S([sentences]) --&gt; P\n  end\n  subgraph data [ ]\n  A[quantity] --o S\n%%  D[value] --o S\n  E[population] --o S\n  F[...] -.-o S\n  end\n  G[(problem)] --x A & E %% & D\n  G -.-x F\n  U{{utility}} --o Z\n  subgraph utility [ ]\n  C[decisions] --o S\n  S --&gt; U\n  T([gains]) --&gt; U\n  Q([costs]) --&gt; U\n  end\n  G --x C & T & Q\n  subgraph exp [ ]\n  Z --&gt; W(optimal solution)\n  end\n  %%\n  style G fill:#e67\n  style A fill:#cb4,stroke-width:0pt\n%%  style D fill:#cb4,stroke-width:0pt\n  style E fill:#cb4,stroke-width:0pt\n  style F fill:#cb4,stroke-width:0pt\n  style S fill:#283,color:#fff\n  style P fill:#283,stroke-width:3pt,color:#fff\n  style C fill:#6ce,stroke-width:0pt\n  style T fill:#6ce,stroke-width:0pt\n  style Q fill:#6ce,stroke-width:0pt\n  style U fill:#6ce,stroke-width:3pt\n  style Z fill:#47a,color:#fff,stroke-width:0pt,color:#fff\n  style W fill:#47a,color:#fff,stroke-width:0pt,color:#fff\n  style probability fill:#fff,stroke:#283,stroke-width:1px\n  style data fill:#fff,stroke:#cb4,stroke-width:1px\n  style utility fill:#fff,stroke:#6ce,stroke-width:1px\n  style exp fill:#fff,stroke:#47a,stroke-width:1px\n\n\n\n\n\n\nData parts develop the language in which a problem can be fed into the decision-making machine. Here you will also learn about important pitfalls in handling data.\nInference parts develop the “inference engine” of the machine. Here you will learn ideas at the foundation of AI; and you will also meet probability, but from a point of view that may be quite novel to you – and much more fun.\n\nThese two parts will alternate so that their development proceeds almost in parallel.\n\nThe utility part develops the “decision engine” of the machine. Here you will meet several ideas that will probably be quite new to you – but also very simple and intuitive.\nThe final solution part simply shows how the inference and utility engines combine together to yield the optimal solution to the problem. This part is simple, short, intuitive; it will be a breeze.\n\n\n\nAs soon as the inference and data parts are complete, you will be able to apply the machine to real inference problems, and also learn how the solution is approximated in some popular machine-learning algorithms.\nThese applications will immediately extend to decision problems as the short utility and solution part are covered. Again you will also see how this solution is approximated in other machine-learning algorithms, classification and regression ones."
  },
  {
    "objectID": "preface.html#mechanics-and-engineers",
    "href": "preface.html#mechanics-and-engineers",
    "title": "Preface",
    "section": "Mechanics and engineers",
    "text": "Mechanics and engineers\nWhat is the difference between a car mechanic and an automotive engineer?\nBoth have knowledge about cars, but their knowledge domains are different and focus on different goals.\nA car mechanic can keep your car in top-notch condition; can do different kinds of easy and difficult repairs if problems arise with it; knows whether a particular brand of valve can be used as a replacement for another brand; can recommend the optimal kind of tyres to use in a given season for different brands of cars. But a car mechanic would face difficulties in calculating the theoretical maximal efficiency of an engine; or predicting the temperature increase caused by a new kind of fuel; or exploiting the phase transition of a new kind of foam to design a safer airbag system; or calculating the optimal surface curvature for a spoiler. A car mechanic typically possesses a large amount of case-specific knowledge, and doesn’t need to know in depth the principles of electromechanics and thermochemistry, or the laws of balance of momentum, energy, entropy.\n\n\n\nVice versa, an automotive engineer can assess how to use the electromechanical properties of a new material in order to design a more efficient and environmentally friendly engine; can calculate how a new material-surface handling would affect air drag and speed; and ultimately can research how to exploit new physical phenomena to build completely new means of transportation. Yet, an automotive engineer could be completely incapable of changing a pipe in your car, or tell you whether it can use a particular brand of lubricant oil. An automotive engineer typically possesses knowledge about the principles of electromagnetism, mechanics, or thermochemistry; is acquainted with relevant physical laws; and doesn’t need to have in-depth case-specific kinds of knowledge.\n\n\n      \nNote that the differences just sketched do not imply a judgement of value. Both professions, kinds of knowledge, and goals are necessary, interesting, and couldn’t exist without each other. Choice between them is a subjective matter of personal passions and aspirations.\nIn fact there isn’t a clear divide between these two kinds of knowledge, but rather a continuum between two vague extremities. A car mechanic can have knowledge and insight about new technologies, and an automotive engineer can know how to fix a carburettor. The two sketches above are meant to expose and emphasize the existence of such a continuum of knowledge and of goals."
  },
  {
    "objectID": "preface.html#data-mechanics-and-data-engineers",
    "href": "preface.html#data-mechanics-and-data-engineers",
    "title": "Preface",
    "section": "Data mechanics and data engineers",
    "text": "Data mechanics and data engineers\nA continuum with two similar extremities can also be drawn in data science.\nSome data scientists have in-depth knowledge on, for instance, how to optimally store and read large amounts data; what kind of machine-learning algorithm to use in a given task; how to fine-tune an algorithm’s parameters, and the currently best software for this purpose. Their particular knowledge is fundamental for the working of today’s technological infrastructure.\nAt the same time, these data scientists typically face difficulties, for instance, in:\n\ncalculating the theoretical maximal accuracy or performance achievable – by any possible algorithm – in a given inference problem\nexplaining how the fundamental rules of inference and decision-making are implemented in a particular machine-learning algorithm\nidentifying which sub-optimal approximations to the fundamental rules are made by popular machine-learning algorithms\nexploiting new technologies to build new algorithms that do calculations closer to the exact theoretical ones, thereby achieving a performance closer to the theoretical optimum\n\nAnd it is also possible that they are not aware of, and maybe would be surprised by, some basic facts of data science. For instance:\n\nthere is an optimal, universal inference & decision algorithm, of which all machine-learning algorithms (from support vector machines and deep networks to random forests and large language models), are an approximation\nthere are only five or six fundamental laws upon which any inference, prediction, classification, regression, decision task is (or ought to be) based upon\nsplittings of data into “training set”, “validation set”, and similar sets, are not part of the exact application of the laws of inference and decision-making; such splittings arise as coarse approximations of the exact method.\ncross-validation and related techniques are not part of the exact method either; they also arise as approximations\noverfitting, underfitting and related notions are not problems that appear in the exact method (which takes care of them automatically); they also arise from approximations\nit is possible to calculate, within probable bounds, the maximal accuracy (or other performance metric) achievable by any classification or regression algorithm for a given application\nsome evaluation metrics, such as precision or the area under the curve of the receiver operating characteristic (AUC), have intrinsic flaws and may attribute higher values to worse-performing algorithms\n\n…because this is a kind of general and principled knowledge that these data scientists don’t need in their jobs. Their knowledge is more case-specific.\nDrawing a parallel with the car example, a data scientist with this kind of case-specific knowledge is like a “data mechanic”.\nA “data engineer”, on the other hand, is the kind of data scientist who has no difficulties with the knowledge and skills implicit in the bullet points above; but at the same time might not know what software to use for tuning parameters of a particular class of deep networks, or the best format to store particular kinds of data.\nJust like in the case of the automotive industry, the difference just sketched does not imply any judgement of value. Both kinds of knowledge and goals are important and can’t exist without each other."
  },
  {
    "objectID": "preface.html#goals-of-this-course",
    "href": "preface.html#goals-of-this-course",
    "title": "Preface",
    "section": "Goals of this course",
    "text": "Goals of this course\nThere is a plethora of academic courses, in all kinds of format, that target knowledge and goals for the “data mechanic”. Those courses are usually inadequate to cover the knowledge and goals for the “data engineer”. Some courses, misleadingly, even present approximations and recipes that are only valid for particular situations as if they were universal rules or methods instead.\nCourses that target the “data engineer” seem to be more rare. One possible reason is that this kind of knowledge is actually hidden in courses on probability, statistics, and risk analysis, presented with a language which makes only opaque and confusing connections with fields in data science and their goals; or, worse, with a language which emphasizes connections that are actually superficial and misleading.\nWe believe that it is important to teach and keep alive the less “mechanic” and more “engineer” side of data science:\n\nContinuous advances in computational technology – think of quantum computers – will offer completely novel and superior ways to approximate the exact method of inference and decision. Only the data scientist who knows the exact method and theory, and understands how present-day algorithms approximate it, will be able to exploit new technologies.\nEven without looking at the future, several present-day machine-learning algorithms could already be greatly optimized by any data engineer who is acquainted with the basic principles underlying data science.\nThe foundations of data science are the bridge to the sibling discipline of Artificial Intelligence.\n\nThe present course aspires to give an introduction to the “data engineer” side, rather than “data mechanic” one, of data science, but using a point of view more familiar to data scientists than to, say, statisticians.\nMore details about its aims, structure, and features are already given in the Dear student introduction."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Accept or discard?",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\nLet’s start with a question that could arise in a particular engineering problem:\n\nA particular kind of electronic component is produced on an assembly line. At the end of the line, there is an automated inspection device that works as follows with every newly produced component coming out of the line.\nThe inspection device first makes some tests on the new component. The tests give an uncertain forecast of whether that component will fail within its first year of use, or after.\nThen the device decides whether the component is accepted and packaged for sale, or discarded and thrown away.\nWhen a new electronic component is sold, the manufacturer has a net gain of \\(1\\$\\). If the component fails within a year of use, however, the manufacturer incur net loss of \\(11\\$\\) (12$ loss, minus the 1$ gained at first), owing to warranty refunds and damage costs to be paid to the buyer. When a new electronic component is discarded, the manufacturer has \\(0\\$\\) net gain.\nFor a new electronic component just come out of the assembly line, the tests of the automated inspection device indicate that there is a \\(10\\%\\) probability that the component will fail within its first year of use.\n\n\n\n\nShould the inspection device accept the new component? or discard it?\n\nTry to give and motivate an answer:\n\n\n\n\n\n\n Very first exercise!\n\n\n\n\nShould the inspection device accept or discard the new component?\n\nIt doesn’t matter if you don’t get the correct answer; not even if you don’t manage to get an answer at all. The purpose here is for you to do some introspection about your own reasoning.\nThen examine and discuss the following points:\n\nWhich numerical elements in the problem seem to affect the answer?\nCan these numerical elements be clearly separated? How would you separate them?\nHow would the answer change, if these numerical elements were changed? Feel free to change them, also in extreme ways, and see how the answer would change.\nCould we solve the problem if we didn’t have the probabilities? Why?\nCould we solve the problem if we didn’t know the various gains and losses? Why?\nCan this problem be somehow abstracted, and then transformed into another one with completely different details? For instance, consider translating along these lines:\n\ninspection device → computer pilot of self-driving car\ntests → camera image\nfail within a year → pedestrian in front of car\naccept/discard → keep on going/ break"
  },
  {
    "objectID": "framework.html#what-does-the-intro-problem-tell-us",
    "href": "framework.html#what-does-the-intro-problem-tell-us",
    "title": "2  Framework",
    "section": "2.1 What does the intro problem tell us?",
    "text": "2.1 What does the intro problem tell us?\nLet’s approach the “accept or discard?” problem of the previous chapter 1 in an intuitive way.\n\n\nWe’re jumping the gun here, because we haven’t learned the method to solve this problem yet!\nFirst let’s say that we accept the component. What happens?\nWe must try to make sense of the \\(10\\%\\) probability that the component fails within a year. Different people do this with different imagination tricks. We can imagine, for instance, that this situation is repeated 100 times. In 10 of these repetitions the accepted electronic component is sold and fails within a year after selling. In the remaining 90 repetitions, the component is sold and works fine for at least a year.\nIn each of the 10 imaginary repetitions in which the component fails early, the manufacturer loses \\(\\color[RGB]{238,102,119}11\\$\\). That’s a total loss of \\({\\color[RGB]{204,187,68}10} \\cdot {\\color[RGB]{238,102,119}11\\$} = {\\color[RGB]{238,102,119}110\\$}\\). In each of the 90 imaginary repetitions in which the component doesn’t fail early, the manufacturer gains \\(\\color[RGB]{34,136,51}1\\$\\). That’s a total gain of \\({\\color[RGB]{204,187,68}90} \\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{34,136,51}90\\$}\\). So over all 100 imaginary repetitions the manufacturer gains\n\\[\n{\\color[RGB]{204,187,68}10}\\cdot ({\\color[RGB]{238,102,119}-11\\$}) + {\\color[RGB]{204,187,68}90}\\cdot {\\color[RGB]{34,136,51}1\\$} = {\\color[RGB]{238,102,119}-20\\$}\n\\]\nthat is, the manufacturer has not gained, but lost \\(20\\$\\)! That’s an average of \\(0.2\\$\\) lost per repetition.\nNow let’s say that we discard the component instead. What happens? In this case we don’t need to invoke imaginary repetitions, but even if we do, it’s clear that the manufacturer doesn’t gain or lose anything – that is, the “gain” is \\(0\\$\\) – in each and all of the repetitions.\nThe conclusion is that if in a situation like this we accept the component, then we’ll lose \\(0.2\\$\\) on average; whereas if we discard it, then on average we’ll lose (or gain) \\(0\\$\\) on average.\nObviously the best, or “least worst”, decision to make is to discard the component.\n\n\n\n\n\n\n Exercises\n\n\n\n\nNow that we have an idea of the general reasoning, check what happens with different values of the probability of failure and of the failure cost: is it still best to discard? For instance, try with\n\nfailure probability 10% and failure cost 5$;\nfailure probability 5% and failure cost 11$;\nfailure probability 10%, failure cost 11$, non-failure gain 2$.\n\nFeel free to get wild and do plots.\nIdentify the failure probability at which accepting the component doesn’t lead to any loss or any gain, so it doesn’t matter whether we discard or accept. (You can solve this as you prefer: analytically with an equation, visually with a plot, by trial & error on several cases, or whatnot.)\nConsider the special case with failure probability 0% and failure cost 10$. This means no new component will ever fail. To decide in such a case we do not need imaginary repetitions; but confirm that we arrive at the same logical conclusion whether we reason through imaginary repetitions or not.\nConsider this completely different problem:\n\nA patient is examined by a brand-new medical diagnostics AI system.\nThe AI first performs some clinical tests on the patient. The tests give an uncertain forecast of whether the patient has a particular disease or not.\nThen the AI decides whether the patient should be dismissed without treatment, or treated with a particular medicine.\nIf the patient is dismissed, then the life expectancy doesn’t increase or decrease if the disease is not present, but it decreases by 10 years if the disease is actually present. If the patient is treated, then the life expectancy decreases by 1 year if the disease is not present (owing to treatment side-effects), but also if the disease is present (because it cures the disease, so the life expectancy doesn’t decrease by 10 years; but it still decreases by 1 year owing to the side effects).\nFor this patient, the clinical tests indicate that there is a \\(10\\%\\) probability that the patient has the disease.\n\nShould the diagnostic AI dismiss or treat the patient? Find differences and similarities, even numerical, with the assembly-line problem.\n\n\n\n\n\nFrom the solution of the problem and from the exploring exercises, we gather some instructive points:\n\nIs it enough if we simply know that the component is less likely to fail than not? in other words, if we simply know that the probability of failure is less than 50% but don’t know its exact value?\nObviously not. We found that if the failure probability is \\(10\\%\\) then it’s best to discard; but if it’s \\(5\\%\\) then it’s best to accept. In both cases the probability of failure was less than 50%, but the decisions were different. Moreover, we found that the probability affected amount of loss if one made the non-optimal decision. Therefore:\nKnowledge of exact probabilities is absolutely necessary for making the best decision\nIs it enough if we simply know that failure leads to a loss, and non-failure leads to a gain, without knowing the exact amounts of loss and gain?\nObviously not. In the exercise we found that if the failure cost is \\(11\\$\\) then it’s best to discard; but if it’s \\(5\\$\\) then it’s best to accept. It’s also best to accept if the failure cost is \\(11\\$\\) but the non-failure gain is \\(2\\$\\). Therefore:\nKnowledge of the exact gains and losses is absolutely necessary for making the best decision\nIs this kind of decision situation only relevant to assembly lines and sales?\nBy all means not. We found a clinical situation that’s exactly analogous: there’s uncertainty, there are gains and losses (of lifetime rather than money), and the best decision depends on both."
  },
  {
    "objectID": "framework.html#our-focus-decision-making-inference-and-data-science",
    "href": "framework.html#our-focus-decision-making-inference-and-data-science",
    "title": "2  Framework",
    "section": "2.2 Our focus: decision-making, inference, and data science",
    "text": "2.2 Our focus: decision-making, inference, and data science\nEvery data-driven engineering project is unique, with its unique difficulties and problems. But there are also problems common to all engineering projects.\nIn the scenarios we explored above, we found an extremely important problem-pattern. There is a decision or choice to make (and “not deciding” is not an option, or it’s just another kind of choice). Making a particular decision will lead to some consequences, some leading to something desirable, others leading to something undesirable. The decision is difficult because its consequences are not known with certainty, given the information and data available in the problem. We may lack information and data about past or present details, about future events and responses, and so on. This is what we call a problem of decision-making under uncertainty or under risk1, or simply a “decision problem” for short.1 We’ll avoid the word “risk” because it has several different technical meanings in the literature, some even contradictory.\nThis problem-pattern appears literally everywhere. But our explored scenarios also suggest that this problem-pattern has a sort of systematic solution method.\nIn this course we’re going to focus on decision problems and their systematic solution method. We’ll learn a framework and some abstract notions that allow us to frame and analyse this kind of problem, and we’ll learn a universal set of principles to solve it. This set of principles goes under the name of Decision Theory. \nBut what do decision-making under uncertainty and Decision Theory have to do with data and data science? The three are profoundly and tightly related on many different planes:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nDecision theory in expert systems and artificial intelligence\n\n\n\nData science is based on the laws of Decision Theory. These laws are similar to what the laws of physics are to a rocket engineer. Failure to account for these fundamental laws leads at best to sub-optimal solutions, at worst to disasters.\nMachine-learning algorithms, in particular, are realizations or approximations of the rules of Decision Theory. This is clear, for instance, considering that a machine-learning classifier is actually choosing among possible output labels or classes.\nThe rules of Decision Theory are also the foundations upon which artificial-intelligence agents, which must make optimal inferences and decisions, are built.\nWe saw that probability values are essential to a decision problem. How do we find them? Data play an important part in their calculation. In our intro example, the failure probability must come from observations or experiments on similar electronic components.\nWe saw that also the values of gains and losses are essential. Data play an important part in their calculation as well.\n\nThese five planes will constitute the major parts and motivations of the present course.\n\n\nThere are other important aspects in engineering problems, besides the one of making decisions under uncertainty. For instance the discovery or the invention of new technologies and solutions. Aspects such as these two can barely be planned or decided. Their drive and direction, however, rest on a strive for improvement and optimization – and the fundamental laws of decision theory tell us what’s optimal and what’s not.\nArtificial intelligence is proving to be a valuable aid in these more creative aspects too. This kind of use of AI is outside the scope of the present notes. Some aspects of this creativity-assisting use, however, do fall within the domain of the present notes. A pattern-searching algorithm, for example, can be optimized by means of the method we are going to study."
  },
  {
    "objectID": "framework.html#our-goal-optimality-not-success",
    "href": "framework.html#our-goal-optimality-not-success",
    "title": "2  Framework",
    "section": "2.3 Our goal: optimality, not “success”",
    "text": "2.3 Our goal: optimality, not “success”\nWhat should we demand from a systematic method for solving decision problems?\nBy definition, in a decision problem under uncertainty there is generally no method to determine the decision that surely leads to the desired consequence – if such a method existed, then the problem would not have any uncertainty! Therefore, if there is a method to deal with decision problems, its goal cannot be the determination of the successful decision. This also means that a priori we cannot blame an engineer for making an unsuccessful decision in a situation of uncertainty. Then what should be the goal of such a method?\nImagine two persons, Henry and Tina, who must bet on “heads” or “tails” under the following conditions (but who otherwise don’t get any special thrill from betting):\n\nIf the bet is “heads” and the coin lands heads, the person wins a small amount of money; but if it lands tails, they lose a large amount of money.\nIf the bet is “tails” and the coin lands tails, the person wins a small amount of money; if it lands heads, they lose the same small amount of money.\n\n\n\n\n\nflowchart LR\n  C[choose a bet!] ---|heads| H([toss coin])\n  C ---|tails| T([toss coin])\n  H ---|heads| HH{{+ $}}\n  H ---|tails| HT{{- $$$}}\n  T ---|heads| TH{{- $}}\n  T ---|tails| TT{{+ $}}\n  %%\n  linkStyle 0 stroke:#cb4,color:#cb4\n  linkStyle 1 stroke:#6ce,color:#6ce\n  linkStyle 2 stroke:#283,color:#283\n  linkStyle 3 stroke:#e67,color:#e67\n  linkStyle 4 stroke:#e67,color:#e67\n  linkStyle 5 stroke:#283,color:#283\n  style HH fill:#283,color:#fff,stroke-width:0px\n  style HT fill:#e67,color:#fff,stroke-width:0px\n  style TT fill:#283,color:#fff,stroke-width:0px\n  style TH fill:#e67,color:#fff,stroke-width:0px\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWhich bet would you choose? why?\n\n\nHenry chooses the “heads” bet. Tina chooses the “tails” bet. The coin comes down heads. So Henry wins the small amount of money, while Tina loses the same small amount.\nWhat would we say about their decisions?\nHenry’s decision was lucky, and yet irrational: he risked losing much more money than he could win. Tina’s decision was unlucky, and yet rational: she wasn’t risking to lose more than she could win. Said otherwise, the two bets had the same winning prospects, but the “heads” bet had more risk of loss than the “tails” bet.\nWe expect that any person making Henry’s decision in similar, future bets will eventually lose more money than any person making Tina’s decision.\nThis example shows two points:\n\n“success” is generally not a good criterion to judge a decision under uncertainty; success can be the pure outcome of luck, not of smarts\neven if there is no method to determine which decision is successful, there is a method to determine which decision is rational or optimal, given the particular gains, losses, and uncertainties involved in the decision problem\n\nWe had a glimpse of this method in our introductory scenarios.\nLet us emphasize, however, that we are not giving up on “success”, or trading it for “optimality”. Indeed we’ll find that Decision Theory automatically leads to the successful decision in problems where uncertainty is not present or is irrelevant. It’s a win-win. It’s important to keep this point in mind:\n\n\n\n\n\n\n\n\n\n\n\nAiming to find the solutions that are successful can make us fail to find those that are optimal when the successful ones cannot be determined.\nAiming to find the solutions that are optimal makes us automatically find those that are successful when those can be determined.\n\n\n\nWe shall later witness this fact with our own eyes, and will take it up again in the discussion of some misleading techniques to evaluate machine-learning algorithms."
  },
  {
    "objectID": "framework.html#decision-theory",
    "href": "framework.html#decision-theory",
    "title": "2  Framework",
    "section": "2.4 Decision Theory",
    "text": "2.4 Decision Theory\nSo far we have mentioned that Decision Theory has the following features:\n\n it tells us what’s optimal and, when possible, what’s successful\n it takes into consideration decisions, consequences, costs and gains\n it is able to deal with uncertainties\n\nWhat other kinds of features should we demand from it, in order to be applied to as many kinds of decision problems as possible, and to be relevant for data science?\n\nIf we find an optimal decision in regards to some outcome, it may still happen that the decision can be realized in several ways that are equivalent in regard to the outcome, but inequivalent in regard to time or resources. In the assembly-line scenario, for example, the decision discard could be carried out by burning, recycling, and so on. We thus face a decision within a decision. In general, a decision problem may involve several decision sub-problems, in turn involving decision sub-sub-problems, and so on.\nIn data science, a common engineering goal is to design and build an automated or AI-based device capable of making an optimal decision in a specific kind of uncertain situations. Think for instance of an aeronautic engineer designing an autopilot system, or a software company designing an image classifier.\n\nDecision Theory turns out to meet these two demands too, thanks to the following features:\n\n it is susceptible to recursive, sequential, and modular application\n it can be used not only for human decision-makers, but also for automated or AI devices\n\n\n\nDecision Theory has a long history, going back to Leibniz in the 1600s and partly even to Aristotle in the −300s, and appearing in its present form around 1920–1960. What’s remarkable about it is that it is not only a framework, but the framework we must use. A logico-mathematical theorem shows that any framework that does not break basic optimality and rationality criteria has to be equivalent to Decision Theory. In other words, any “alternative” framework may use different technical terminology and rewrite mathematical operations in a different way, but it boils down to the same notions and operations of Decision Theory. So if you wanted to invent and use another framework, then either (a) it would lead to some irrational or illogical consequences, or (b) it would lead to results identical to Decision Theory’s. Many frameworks that you are probably familiar with, such as optimization theory or Boolean logic, are just specific applications or particular cases of Decision Theory.\nThus we list one more important characteristic of Decision Theory:\n\n it is normative\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nJudgment under uncertainty\nHeuristics and Biases\nThinking, Fast and Slow\n\n\n\nNormative contrasts with descriptive. The purpose of Decision Theory is not to describe, for example, how human decision-makers typically make decisions. Because human decision-makers typically make irrational, sub-optimal, or biased decisions. That’s exactly what we want to avoid and improve!"
  },
  {
    "objectID": "basic_decisions.html#graphical-representation-and-elements",
    "href": "basic_decisions.html#graphical-representation-and-elements",
    "title": "3  Basic decision problems",
    "section": "3.1 Graphical representation and elements",
    "text": "3.1 Graphical representation and elements\nA basic decision problem can be represented by a diagram like this:\n\nIt has one decision node, usually represented by a square , from which the available decisions depart as lines. Each decision leads to an uncertainty node, usually represented by a circle , from which the possible outcomes depart as lines. Each outcome leads to a particular utility value. The uncertainty of each outcome is quantified by a probability.\nA basic decision problem is analysed in terms of these elements:\n\n Agent, and background or prior information. The agent is the person or device that has to make the decision. An agent always possesses (or has been programmed with) specific background information that is used and taken for granted in the decision-making process. This background information determines the probabilities and utilities of the outcomes, together with other available data and information. Since different agents typically have different background information, we shall somehow conflate agents and prior information.\n\n\n\nWe’ll use the neutral pronouns it/its when referring to an agent, since an agent could be a person or a machine.\n\n Decisions, also called courses of actions, available to the agent. They are assumed to be mutually exclusive and exhaustive; this can always be achieved by recombining them if necessary, as we’ll discuss later.\n Outcomes of the possible decisions. Every decision can have a different set of outcomes, or some outcomes can appear for several or all decisions (in this case they are reported multiple times in the decision diagram). Note that even if an outcome can happen for two or more different decisions, its probabilities can still be different depending on the decision.\n Probabilities for each of the outcomes. Their values typically depend on the background information, the decision, and the additional data.\n Utilities: the gains or losses associated with each of the possible outcomes. Their values also depend on the background information, the decision, and the additional data.\n Data and other additional information, sometimes called evidence. They differ from the background information in that they can change with every decision instance made by the same agent, while the background information stays the same. In the assembly-line scenario, for example, the test results could be different for every new electric component.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Remember: What matters is to be able to identify these elements in a concrete problem, understanding their role. Their technical names don’t matter.\n\n\nNote that it is not always the case that the outcomes are unknown and the data are known. As we’ll discuss later, in some situations we reason in hypothetical or counterfactual ways, using hypothetical data and considering outcomes which have already occurred.\n\n\n\n\n\n\n Study reading\n\n\n\n§ 1.1.4 in Artificial Intelligence\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nIdentify the elements above in the assembly-line decision problem of the introduction 1.\nSketch the diagram of the assembly-line decision problem.\n\n\n\nSome of the decision-problem elements listed above may need to be in turn analysed by a decision sub-problem. For instance, the utilities could depend on uncertain factors: thus we have a decision sub-problem to determine the optimal values to be used for the utilities of the main problem. This is an example of the modular character of decision theory.\nWe shall soon see how to mathematically represent these elements.\nThe elements above must be identified unambiguously in every decision problem. The analysis into these elements greatly helps in making the problem and its solution well-defined.\nAn advantage of decision theory is that its application forces us to make sense of an engineering problem. A useful procedure is to formulate the general problem in terms of the elements above, identifying them clearly. If the definition of any of the terms involves uncertainty of further decisions, then we analyse it in turn as a decision sub-problem, and so on.\n\nSuppose someone (probably a politician) says: “We must solve the energy crisis by reducing energy consumption or producing more energy”. From a decision-making point of view, this person has effectively said nothing whatsoever. By definition the “energy crisis” is the problem that energy production doesn’t meet demand. So this person has only said “we would like the problem to be solved”, without specifying any solution. A decision-theory approach to this problem requires us to specify which concrete courses of action should be taken for reducing consumption or increasing productions, and what their probable outcomes, costs, and gains would be.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nSee MacKay’s options-vs-costs rational analysis in Sustainable Energy – without the hot air"
  },
  {
    "objectID": "basic_decisions.html#inference-utility-maximization",
    "href": "basic_decisions.html#inference-utility-maximization",
    "title": "3  Basic decision problems",
    "section": "3.2 Inference, utility, maximization",
    "text": "3.2 Inference, utility, maximization\nThe solution of a basic decision-making problem can be roughly divided into three main stages: inference, utility assessment, and expected-utility maximization.\n Inference is the stage where the probabilities of the possible outcomes are calculated. Its rules are given by the Probability Calculus. Inference is independent from decision: in some situations we may simply wish to assess whether some hypotheses, conjectures, or outcomes are more or less plausible than others, without making any decision. This kind of assessment can be very important in problems of communication and storage, and it is specially considered by Information Theory.\nThe calculation of probabilities can be the part that demands most thinking, time, and computational resources in a decision problem. It is also the part that typically makes most use of data – and where data can be most easily misused.\nRoughly half of this course will be devoted in understanding the laws of inference, their applications, uses, and misuses.\n\n Utility assesment is the stage where the gains or losses of the possible outcomes are calculated. Often this stage requires further inferences and further decision-making sub-problems. The theory underlying utility assessment is still much underdeveloped, compared to probability theory.\n\n Expected-utility maximization is the final stage where the probabilities and gains or costs of the possible outcomes are combined, in order to determine the optimal decision."
  },
  {
    "objectID": "inference.html#sec-inference-scenarios",
    "href": "inference.html#sec-inference-scenarios",
    "title": "4  What is an inference?",
    "section": "4.1 The wide scope and characteristics of inferences",
    "text": "4.1 The wide scope and characteristics of inferences\nLet’s see a couple more informal examples of inference problems. For some of them an underlying decision-making problem is also alluded to:\n\nLooking at the weather we try to assess if it’ll rain today, to decide whether to take an umbrella.\nConsidering a patient’s symptoms, test results, and medical history, a clinician tries to assess which disease affects a patient, so as to decide on the optimal treatment.\nLooking at the present game position  the X-player, which moves next, wonders whether placing the next X on the mid-right position leads to a win.\nFrom the current set of camera frames, the computer of a self-driving car needs to assess whether a particular patch of colours in the frames is a person, so as to slow down the car and stop.\nGiven that \\(G=6.67 \\cdot 10^{-11}\\,\\mathrm{m^3\\,s^{-2}\\,kg^{-1}}\\), \\(M = 5.97 \\cdot 10^{24}\\,\\mathrm{kg}\\) (mass of the Earth), and \\(r = 6.37 \\cdot 10^{6}\\,\\mathrm{m}\\) (radius of the Earth), a rocket engineer needs to know how much is \\(\\sqrt{2\\,G\\,M/r\\,}\\).\nWe’d like to know whether the rolled die is going to show .\nAn aircraft’s autopilot system needs to assess how much the aircraft’s roll will change if the right wing’s angle of attack is increased by \\(0.1\\,\\mathrm{rad}\\).\nBy looking at the dimensions, shape, texture of a newly dug-out fossil bone, an archaeologist wonders whether it belonged to a Tyrannosaurus rex.\nA voltage test on a newly produced electronic component yields a reading of \\(100\\,\\mathrm{mV}\\). The electronic component turns out to be defective. An engineer wants to assess whether the voltage-test reading could have been \\(100\\,\\mathrm{mV}\\), if the component had not been defective.\nSame as above, but the engineer wants to assess whether the voltage-test reading could have been \\(80\\,\\mathrm{mV}\\), if the component had not been defective.\n\n\n\n\nFrom measurements of the Sun’s energy output and of concentrations of various substances in the Earth’s atmosphere over the past 500 000 years, and of the emission rates of various substances in the years 1900–2022, climatologists and geophysicists try to assess the rate of mean-temperature increase in the years 2023–2100.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nCh. 10 in A Survival Guide to the Misinformation Age.\n\n\n\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example above, pinpoint what has to be inferred, and also the agent interested in the inference.\nPoint out which of the examples above explicitly give data or information that should be used for the inference.\nFor the examples that do not give explicit data or information, speculate what information could be implicitly assumed. For those that do give explicit data, speculate which other additional information could be implicitly assumed.\nCan any of the inferences above be done perfectly, that is, without any uncertainty, based the data given explicitly or implicitly?\nFind the examples that explicitly involve a decision. In which of them does the decision affect the results of the inference? In which it does not?\nAre any of the inferences “one-time only” – that is, their object or the data on which they are based have never happened before and will never happen again?\nAre any of the inferences based on data and information that come chronologically after the object of the inference?\nAre any of the inferences about something that is actually already known to the agent that’s making the inference?\nAre any of the inferences about something that actually did not happen?\nDo any of the inferences use “data” or “information” that are actually known (within the scenario itself) to be fictive, that is, not real?\n\n\n\nFrom the examples and from your answers to the exercise we observe some very important characteristics of inferences:\n\nSome inferences can be made exactly, that is, without uncertainty: it is possible to say whether the object of the inference is true or false. Other inferences, instead, involve an uncertainty.\nAll inferences are based on some data and information, which may be explicitly expressed or only implicitly understood.\nAn inference can be about something past, but based on present or future data and information: inferences can show all sorts of temporal relations.\nAn inference can be essentially unrepeatable, because it’s about something unrepeatable or based on unrepeatable data and information.\nThe data and information on which an inference is based can actually be unknown; that is, they can be only momentarily contemplated as real. Such an inference is said to be based on hypothetical reasoning.\nThe object of an inference can actually be something already known to be false or not real: the inference tries to assess it in the case that some data or information had been different. Such an inference is said to be based on counterfactual reasoning."
  },
  {
    "objectID": "inference.html#where-are-inferences-drawn-from",
    "href": "inference.html#where-are-inferences-drawn-from",
    "title": "4  What is an inference?",
    "section": "4.2 Where are inferences drawn from?",
    "text": "4.2 Where are inferences drawn from?\nThis question is far from trivial. In fact it has connections with the earth-shaking development and theorems in the foundations of mathematics of the 1900s.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMathematics: The Loss of Certainty.\n\n\nThe proper answer to this question will take up the next sections. But a central point can be emphasized now:\n\n\n\n\n\n\n\n\n\n\n\nInferences can only be drawn from other inferences.\n\n\n\nIn order to draw an inference – calculate a probability – we usually go up a chain: we must first draw other inferences, and for drawing those we must draw yet other inferences, and so on.\nAt some point we must stop at inferences that we take for granted without further proof. These typically concern direct experiences and observations. For instance, you see a tree in front of you, so you can take “there’s a tree here” as a true fact. Yet, notice that the situation is not so clear-cut: how do you know that you aren’t hallucinating, for example, and there’s actually no tree there? That is taken for granted. If you analyse the possibility of hallucination, you realize that you are taking other things for granted, and so on. Probably most philosophical research in the history of humanity has been about grappling with this runaway process – which is also a continuous source of sci-fi films. In logic and mathematical logic, this corresponds to the fact that to prove some theorem, we must always start from some axioms. There are “inferences” – tautologies – that can be drawn without requiring others; but they are all trivial, such as “this component failed early, or it didn’t”. They are of little use in a real problem, although they have a deep theoretical importance.\n\n\n\n\n\nSci-fi films like The Matrix ultimately draw on the fact that we must take some inferences for granted without further proof.\n\n\n\n\nIn concrete applications we start from many inferences upon which everyone, luckily, agrees. But sometimes we must also use starting inferences that are more dubious or not agreed upon by anyone. In this case the final inference has a somewhat contingent character, and we accept it (as well as the solution of any underlying decision problem) as the best available for the moment. This is partly the origin of the term “model”."
  },
  {
    "objectID": "inference.html#basic-elements-of-an-inference",
    "href": "inference.html#basic-elements-of-an-inference",
    "title": "4  What is an inference?",
    "section": "4.3 Basic elements of an inference",
    "text": "4.3 Basic elements of an inference\nLet us start to introduce some mathematical notation and more precise terminology for inferences.\nEvery inference has an “object” – what is to be assessed – as well as data, information, or hypotheses on which it is based. We call proposal the object of the inference, and conditional what the inference is based upon. We separate them with a vertical bar  “\\(\\pmb{\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}}\\)”,  which can be pronounced “given” or “conditional on”:\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\\n\\textit{[conditional]}\n\\]\nWe have seen that to calculate the probability for an inference, we must start from the probabilities of other inferences. A basic inference process therefore can be schematized like this:\n\n\n\n\n\nProposal is Johnson’s (1924) terminology; Keynes (1921) uses “conclusion”; modern textbooks do not seem to use any specialized term. Conditional is modern terminology; other terms used: “evidence”, “premise”, “supposal”. The vertical bar, originally a solidus, was introduced by Keynes (1921).\n\n\n\nThe next important task ahead of us is to introduce a flexible and enough general mathematical representation for the objects and the bases of an inference. Then we shall finally study the rules for drawing correct inferences."
  },
  {
    "objectID": "sentences.html#sec-central-comps",
    "href": "sentences.html#sec-central-comps",
    "title": "5  Sentences",
    "section": "5.1 The central components of knowledge representation",
    "text": "5.1 The central components of knowledge representation\nWhen speaking of “data”, what comes to mind to many people is basically numbers or collections of numbers. Maybe numbers, then, could be used to represent all the variety of items exemplified above. This option, however, turns out to be too restrictive.\nI give you this number: “\\(8\\)”, saying that it is “data”. But what is it about? You, as an agent, can hardly call this number a piece of information, because you have no clue what to do with it.\nInstead, if I tell you: “The number of official planets in the solar system is 8”, then we can say that I’ve given you data. You can do different things with this piece of information (for instance: if you had decided to send one probe to each official planet, now you know you have to build eight probes).\nSo, “data” is not just numbers: a number is not “data” unless there’s an additional verbal and non-numeric context accompanying it – even if only implicitly. Sure, we could represent this meta-data information as numbers too; but this move would only shift the problem one level up: we would need an auxiliary verbal context explaining what the meta-data numbers are about.\nData can, moreover, be completely non-numeric. A clinician saying “The patient has fully recovered from the disease” (we imagine to know who’s the patient and what was the disease) is giving us a piece of information that we could further use, for instance, to make prognoses about other, similar patients. The clinician’s statement surely is “data”, but essentially non-numeric data. Sure, in some situations we could represent it as “1”, while “0” would represent “not recovered”; but the opposite convention could also be used, or the numbers “0.3” and “174” could be used. These numbers have intrinsically nothing to do with the clinician’s “recovery” data.\n\n\nBut the examples above actually reveal the answer to our needs! In the examples we expressed the data by means of sentences. Clearly any measurement result, decision outcome, hypothesis, not-real event, assumption, data, and any piece of information can be expressed by a sentence.\nWe shall therefore use sentences, also called propositions or statements,1 to represent and communicate all the kinds of “items” that can be the proposal or conditional of an inference. In some cases we can of course summarize a sentence by a number, as a shorthand, when the full meaning of the sentence is understood.\n\n1 These three terms are not always equivalent in formal logic, but here we’ll use them as synonyms.\nSentences are the central components of knowledge representation in AI agents. For example they appear at the heart of automated control programs and fault-management systems in NASA spacecrafts.\n\n\n (From the SMART paper)\n\n\n\n\n\n\n Study reading\n\n\n\n\n§ 7.1 in Artificial Intelligence.\nTake a quick look at these:\n\nSMART: A propositional logic-based trade analysis and risk assessment tool for a complex mission\naround p. 22 in No More Band-Aids: Integrating FM into the Onboard Execution Architecture\n§ 2.1 in Deliberation for autonomous robots: A survey\npart IV in Model-based programming of intelligent embedded systems and robotic space explorers"
  },
  {
    "objectID": "sentences.html#identifying-and-working-with-sentences",
    "href": "sentences.html#identifying-and-working-with-sentences",
    "title": "5  Sentences",
    "section": "5.2 Identifying and working with sentences",
    "text": "5.2 Identifying and working with sentences\nBut what is a sentence, more exactly? The everyday meaning of this word will work for us, even though there are more precise definitions – and still a lot of research in logic an artificial intelligence on how to define and use sentences. We shall adopt this useful definition:\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nPropositions\n\n\n\n\n\n\n\n\n\n\n\n\nA “sentence” is a verbal message for which we can determine whether it is true or false, at least in principle and in such a way that all interested receivers of the message would agree.\n\n\nFor instance, in most engineering contexts the phrase “This valve will operate for at least two months” is a sentence; whereas the phrase “Apples are much tastier than pears” is not, because it’s a matter of personal taste – there’s no objective criterion to determine its truth or falsity (however, the phrase “Rita finds apples tastier than pears” could be a sentence; its truth is found by asking Rita). In a data-science context, the phrase “The neural-network algorithm has better performance than the random-forest one” is not a sentence unless we have objectively specified what “better” means, for example by using a particular comparison metric.\nSome expressions, even involving technical terms, may in fact appear to be sentences at first; but a deeper analysis may reveal that they are not. A famous example is the sentence “The two events (at different spatial locations) are simultaneous”. Einstein showed that there’s no physical way to determine whether such an expression is true or false. Its truth turns out to be a matter of convention (also in Newtonian mechanics). The Theory of Relativity was born from this observation.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the electrodynamics of moving bodies.\n\n\nOne sentence can be expressed by many different phrases and in different languages. For instance, “The temperature is 248.15 K”, “Temperaturen ligger på minus 25 grader”, and “25 °C is the value of the temperature” all represent the same sentence.\nA sentence can contain numbers, pictures, and graphs.\nWorking with sentences, and keeping in mind that inference is about sentences, is important in several respects:\nFirst, it leads to clarity in engineering problems and makes them more goal-oriented. A data engineer must acquire information and convey information. “Acquiring information” does not simply consist in making measurements or counting something: the engineer must understand what is being measured and why. If data is gathered from third parties, the engineer must ask what exactly the data mean and how they were acquired. In designing and engineering a solution, it is important to understand what information or outcomes the end user exactly wants. The “what”, “why”, “how” are expressed by sentences. A data engineer will often ask “wait, what do you mean by that?”. This question is not just an unofficial parenthesis in the official data-transfer workflow between the engineer and someone else. It is an integral part of that workflow: it means that some information has not been completely transferred yet.\nSecond, it is extremely important in AI and machine-learning design. A (human) engineer may proceed informally when drawing inferences, without worrying about “sentences” unless a need for disambiguation arises. A data engineer who’s designing or programming an algorithm that will do inferences automatically, must instead be unambiguous and cover beforehand all possible cases that the algorithm will face.\n\n\nWe agree that the proposal and the conditional of an inference have to be sentences. This means that the proposal of the inference must be something that can only be true or false. Many inferences, especially when they concern numerical measurements, are actually collections of inferences. For example, an inference about the result of rolling a die actually consists of six separate inferences with the proposals\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The result of the roll is 1'}\n\\\\\n&\\textsf{\\small`The result of the roll is 2'}\n\\\\\n&\\dotso\n\\\\\n&\\textsf{\\small`The result of the roll is 6'}\n\\end{aligned}\n\\]\nLater on we shall see how to work with more complex inferences without thinking about this detail. In real applications it can be useful, on some occasions, to pause and reduce an inference to its basic set of true/false inferences; this analysis may reveal contradictions in our inference. A simple way to do this is to reduce the complex inference into a set of yes/no questions.\nThis kind of analysis is also important in information-theoretic situations: the information content provided by an inference, when measured in Shannons, is related to the minimal amount of yes/no questions that the inference answers.\n\n\n\n\n\n\n Exercise\n\n\n\nRewrite each inference scenario of § 4.1 in a formal way, as one or more inferences\n\\[\n\\textit{[proposal]}\\ \\pmb{\\nonscript\\:\\Big\\vert\\nonscript\\:\\mathopen{}}\\ \\textit{[conditional]}\n\\]\nwhere proposal and conditional are well-defined sentences.\nIn ambiguous cases, use your judgement and motivate your choices."
  },
  {
    "objectID": "sentences.html#sec-sentence-notation",
    "href": "sentences.html#sec-sentence-notation",
    "title": "5  Sentences",
    "section": "5.3 Notation and abbreviations",
    "text": "5.3 Notation and abbreviations\nWriting full sentences would take up a lot of space. Even an expression such as “The speed is 10 m/s” is not a sentence, strictly speaking, because it leaves unspecified the speed of what, when it was measured and in which frame of reference, what we mean by “speed”, how the unit “m/s” is defined, and so on.\nTypically we leave the full content of a sentence to be understood from the context, and we denote the sentence by a simple expression such as the one above,\n\\[\n\\textsf{\\small The speed is 10\\,m/s}\n\\]\nor even more compactly introducing physical symbols:\n\\[\nv = 10\\,\\mathrm{m/s}\n\\]\nwhere \\(v\\) is a physical variable denoting the speed; or even writing simply\n\\[\n10\\,\\mathrm{m/s}\n\\]\nIn some problems it’s useful to introduce symbols to denote sentences. In these notes we’ll use sans-serif italic letters: \\(\\mathsfit{A},\\mathsfit{B},\\mathsfit{a},\\mathsfit{b},\\dotsc\\),, possibly with sub- or super-scripts. For instance, the sentence “The speed is 10 m/s” could be denoted by the symbol \\(\\mathsfit{S}_{10}\\). We abbreviate such a definition like this:\n\\[\n\\mathsfit{S}_{10} \\coloneqq\\textsf{\\small`The speed is 10\\,m/s'}\n\\]\nwhich means “the symbol \\(\\mathsfit{S}_{10}\\) is defined to be the sentence \\(\\textsf{\\small`The speed is 10\\,m/s'}\\)”.\n\n\n\n\n\n\n We must be wary of how much we shorten sentences\n\n\n\nConsider these three:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The speed is measured to be 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is set to 10\\,m/s'}\n\\\\\n&\\textsf{\\small`The speed is reported, by a third party, to be 10\\,m/s'}\n\\end{aligned}\n\\]\nThe quantity “10 m/s” is the same in all three sentences, but their meanings are very different. They represent different kinds of data. These differences greatly affect any inference about or from these data. For instance, in the third case an engineer may not take the indirectly-reported speed “10 m/s” at face value, unlike the first case. In a scenario where all three sentences can occur, it would be ambiguous to simply write “\\(v = 10\\,\\mathrm{m/s}\\)”: would the equal-sign mean “measured”, “set”, or “indirectly reported”?\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nHow would you denote the three sentences above, to make their differences clear?\n\n\n\n\n\n\n\n\n\n\nGet familiar with abbreviations of sentences\n\n\n\nTo summarize, a sentence like\n\\[\n\\textsf{\\small`The temperature $T$ has value $x$'}\n\\]\ncould be abbreviated in these different ways:\n\nA symbol for the sentence (note the sans-serif font):\n\\[\n\\mathsfit{S}\n\\]\nSome key word appearing in the sentence:\n\\[\n\\textsf{\\small temperature}\n\\]\nAn equality:\n\\[\nT\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x\n\\]\nThe quantity appearing in the sentence:\n\\[\nT\n\\]\nThe value appearing in the sentence:\n\\[\nx\n\\]\n\nGet familiar with these kinds of abbreviations because they’re very common. Some text may even jump from one abbreviation to another in the same page or paragraph!"
  },
  {
    "objectID": "sentences.html#sec-connecting-sentences",
    "href": "sentences.html#sec-connecting-sentences",
    "title": "5  Sentences",
    "section": "5.4 Connecting sentences",
    "text": "5.4 Connecting sentences\n\nAtomic sentences\nIn analysing the measurement results, decision outcomes, hypotheses, assumptions, data and information that enter into an inference problem, it is convenient to find a collection of basic sentences or, using a more technical term, atomic sentences out of which all other sentences of interest can be constructed. These atomic sentences often represent elementary pieces of information in the problem.\nConsider for instance the following complex sentence, which could appear in our assembly-line scenario:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\nIn this statement we can identify at least four atomic sentences, which we denote by these symbols:\n\\[\\begin{aligned}\n\\mathsfit{s} &\\coloneqq\\textsf{\\small`The component is whole after the shock test'}\n\\\\\n\\mathsfit{h} &\\coloneqq\\textsf{\\small`The component is whole after the heating test'}\n\\\\\n\\mathsfit{v}_{90} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 90\\,mV'}\n\\\\\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 110\\,mV'}\n\\end{aligned}\n\\]\nThe inference may actually require additional atomic sentences. For instance, it might become necessary to consider atomic sentences with other values for the reported voltage, such as\n\\[\\begin{aligned}\n\\mathsfit{v}_{110} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 100\\,mV'}\n\\\\\n\\mathsfit{v}_{80} &\\coloneqq\\textsf{\\small`The power-test voltage reading is 80\\,mV'}\n\\end{aligned}\\]\nand so on.\n\n\nConnectives\nHow do we construct complex sentences, like the one above, out of atomic sentences?\nWe consider three ways: one operation to change a sentence into another related to it, and two operations to combine two or more sentences together. These operations are called connectives; you may have encountered them already in Boolean algebra. Our natural language offers many more operations to combine sentences, but these three connectives turn out to be all we need in virtually all engineering and data-science problems:\n\n\n\n\n\n\n\n\n\n\n\n\nNot:  \\(\\lnot\\)\n\nexample:\n\n\n\\[\n\\lnot \\mathsfit{s} = \\textsf{\\small`The component is broken after the shock test'}\n\\]\n\nAnd:  \\(\\land\\)  also  \\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\)\n\nexample:\n\n\n\\[\n\\begin{aligned}\n\\mathsfit{s} \\land \\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\\\\n\\mathsfit{s} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{h} &= \\textsf{\\small`The component is whole after the shock and heating tests'}\n\\end{aligned}\n\\]\n\nOr:  \\(\\lor\\)\n\nexample:\n\n\n\\[\n\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110} = \\textsf{\\small`The power-test voltage reading is 90\\,mV, or 110\\,mV, or both'}\n\\]\n\n\n\nThese connectives can be applied multiple times, to form increasingly complex sentences.\nThe and connective appears very frequently in probability formulae. Using its standard symbol “\\(\\land\\)” would consume a lot of horizontal space. For this reason a comma “\\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\)” is often used as an alternative symbol. So the expressions \\(\\mathsfit{s} \\land \\mathsfit{h}\\) and \\(\\mathsfit{s} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{h}\\) are completely equivalent.\n\n\n\n\n\n\n Important subtleties of the connectives:\n\n\n\n\nThere is no strict correspondence between the words “not”, “and”, “or” in natural language and the three connectives. For instance the and connective could correspond to the words “but” or “whereas”, or just to a comma “ , ”.\nNot means not some kind of complementary quality, but the denial. For instance,  \\(\\lnot\\textsf{\\small`The chair is black'}\\)  generally does not mean  \\(\\textsf{\\small`The chair is white'}\\) ,   (although in some situations these two sentences could amount to the same thing).\nIt’s always best to declare explicitly what the not of a sentence concretely means. In our example we take\n\\[\n  \\lnot\\textsf{\\small`The component is whole'} \\coloneqq\\textsf{\\small`The component is broken'}\n  \\]\nBut in other examples the negation of “being whole” could comprise several different conditions. A good guideline is to always state the not of a sentence in positive terms.\nOr does not exclude that both the sentences it connects can be true. So in our example  \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\)  does not exclude, a priori, that the reported voltage could be both 90 mV and 110 mV. (There is a connective for that: “exclusive-or”, but it can be constructed out of the three we already have.)\n\n\n\nFrom the last remark we see that the sentence\n\\[\n\\textsf{\\small`The power-test voltage reading is 90\\,mV or 110\\,mV'}\n\\]\ndoes not correspond to   \\(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110}\\) .  It is implicitly understood that a voltage reading cannot yield two different values at the same time. Convince yourself that the correct way to write that sentence is this:\n\\[\n(\\mathsfit{v}_{90} \\lor \\mathsfit{v}_{110})\n\\land\n\\lnot(\\mathsfit{v}_{90} \\land \\mathsfit{v}_{110})\n\\]\nFinally, the full complex sentence of the present example can be written in symbols as follows:\n\n“The electronic component is still whole after the shock test and the subsequent heating test. The voltage reported in the final power test is either 90 mV or 110 mV.”\n\n\\[\n\\textcolor[RGB]{102,204,238}{\\mathsfit{s}} \\land \\textcolor[RGB]{34,136,51}{\\mathsfit{h}} \\land\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\lor \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\land\n\\lnot\n(\\textcolor[RGB]{238,102,119}{\\mathsfit{v}_{90}} \\land \\textcolor[RGB]{170,51,119}{\\mathsfit{v}_{110}})\n\\]\n\n\n\n\n\n\n\n\n Study reading\n\n\n\nJust take a quick look at § 7.4.1 in Artificial Intelligence and note the similarities with what we’ve just learned. In these notes we follow a faster approach leading directly to probability logic."
  },
  {
    "objectID": "sentences.html#if-then",
    "href": "sentences.html#if-then",
    "title": "5  Sentences",
    "section": "5.5 “If… then…”",
    "text": "5.5 “If… then…”\nSentences expressing data and information in natural language also appear connected with if… then…. For instance: “If the voltage reading is 200 mV, then the component is defective”. This kind of expression actually indicates that the following inference\n\\[\n\\textsf{\\small`The component is defective'} \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} \\textsf{\\small`The voltage reading is 200\\,mV'}\n\\]\nis true.\nThis kind of information is very important because it often is the starting point from which to arrive at the final inferences we’re interested in. We shall discuss it more in detail in the next sections.\n\n\n\n\n\n\n Careful\n\n\n\nThere is a connective in logic, called “material conditional”, which is also often translated as “if… then…”. But it is not the same as the inference relation discussed above. “If… then…” in natural language usually denotes an inference rather than a material conditional.\nResearch is still ongoing on these topics. If you are curious and in for a headache, look over The logic of conditionals.\n\n\n\n\n\nWe are now equipped with all the notions and symbolic notation to deal with our next task: learning the rules for drawing correct inferences.\n@@ TODO: add connections to impossibility of large language models to learn maths (Gödel & Co.)."
  },
  {
    "objectID": "truth_inference.html#sec-trivial-inference",
    "href": "truth_inference.html#sec-trivial-inference",
    "title": "6  Truth inference",
    "section": "6.1 A trivial inference",
    "text": "6.1 A trivial inference\nConsider again the assembly-line scenario of § 1, and suppose that an inspector has the following information about an electric component:\n\nThis electric component had an early failure (within a year of use). If an electric component fails early, then at production it didn’t pass either the heating test or the shock test. This component passed the shock test.\n\nThe inspector wants to assess whether the component did not pass the heating test.\nFrom the data and information given, the conclusion is that the component for sure did not pass the heating test. This conclusion is certain and somewhat trivial. But how did we obtain it? Which rules did we follow to arrive at it from the given data?\nFormal logic, with its deduction systems, is the huge field that formalizes and makes rigorous the rules that a rational person or an artificial intelligence should use in drawing sure inferences like the one above. We’ll now get a glimpse of it, as a trampoline for jumping towards more general and uncertain inferences."
  },
  {
    "objectID": "truth_inference.html#analysis-and-representation-of-the-problem",
    "href": "truth_inference.html#analysis-and-representation-of-the-problem",
    "title": "6  Truth inference",
    "section": "6.2 Analysis and representation of the problem",
    "text": "6.2 Analysis and representation of the problem\nFirst let’s analyse our simple problem and represent it with more compact symbols.\n\nAtomic sentences\nWe can introduce the following atomic sentences and symbols:\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{I}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\n\n\nProposal\nThe proposal is \\(\\lnot\\mathsfit{h}\\), but in the present case we could also have chosen \\(\\mathsfit{h}\\).\n\n\nConditional\nThe bases for the inference are two known facts in the present case: \\(\\mathsfit{s}\\) and \\(\\mathsfit{f}\\). There may also be other obvious facts implicitly assumed in the inference, which we denote by \\(\\mathsfit{I}\\).\n\n\nStarting inferences\nLet us emphasize again that any inference is drawn from other inferences, which are either taken for granted, or drawn in turn from others. In the present case we are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. We write this as\n\\[\n\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\nand we shall take this to be true (that is, to have probability \\(100\\%\\)).\nBut our scenario actually has at least one more, hidden, inference. We said that the component failed early, and that it did pass the shock test. This means, in particular, that it must be possible for the component to pass the shock test, even if it fails early. This means that\n\\[\n\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}\n\\]\ncannot be false.\n\n\nTarget inference\nThe inference that the inspector wants to draw can be compactly written:\n\n\\[\n\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}\n\\]"
  },
  {
    "objectID": "truth_inference.html#truth-inference-rules",
    "href": "truth_inference.html#truth-inference-rules",
    "title": "6  Truth inference",
    "section": "6.3 Truth-inference rules",
    "text": "6.3 Truth-inference rules\n\nDeduction systems; a specific choice\nFormal logic gives us a set of rules for correctly drawing sure inferences, when such inferences are possible. These rules can be formulated in different ways, leading to a wide variety of deduction systems (each one with a wide variety of possible notations). The picture on the margin, for instance, shows how a proof of how our inference would look like, using the so-called sequent calculus, which consists of a dozen or so inference rules.\n\n\n\n\n\nThe bottom formula is the target inference. Each line denotes the application of an inference rule, from one or more inferences above the line, to one below the line. The two formulae with no line above are our starting inference, and a tautology.\n\n\n\n\nWe choose to compactly encode all truth-inference rules in the following way.\nFirst, represent true by the number \\(\\mathbf{1}\\), and false by \\(\\mathbf{0}\\).\nSecond, symbolically write that a proposal \\(\\mathsfit{Y}\\) is true, given a conditional \\(\\mathsfit{X}\\), as follows:\n\\[\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 1\n\\]\nor “\\(=0\\)” if it’s false.\nThe rules of truth-inference are then encoded by the following equations, which must always hold for any atomic or complex sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nRule for “not”:\n\n\\[\\mathrm{T}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1 \\tag{6.1}\\]\n\nRule for “and”:\n\n\\[\n\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{6.2}\\]\n\nRule for “or”:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\tag{6.3}\\]\n\nRule of self-consistency:\n\n\\[\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\tag{6.4}\\]\n\n\n\nHow to use the rules: Each equality can be rewritten in different ways according to the usual rules of algebra. Then the resulting left side can be replaced by the right side, and vice versa. The numerical values of starting inferences can be replaced in the corresponding expressions.\n\n\n\nLet’s see two examples:\n\nfrom one rule for “and” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\\frac{\\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\nprovided that \\(\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) \\ne 0\\). Then wherever we see the left side, we can replace it with the fraction on the right side, and vice versa.\nfrom the rule for “or” we can obtain the equality\n\\[\n{\\color[RGB]{102,204,238}\n\\mathrm{T}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n=\\color[RGB]{204,187,68}\n\\mathrm{T}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) - \\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nAgain wherever we see the left side, we can replace it with the sum on the right side, and vice versa.\n\n\n\nTarget inference in our scenario\nLet’s see how these rules allow us to arrive at our target inference,\n\\[\n\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})\n\\]\nstarting from the given ones\n\\[\n\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) = 1\n\\ ,\n\\qquad\n\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I}) \\ne 0\n\\]\nOne possibility is to work backwards from the target inference:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I})&&\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\color[RGB]{34,136,51}\\underbracket[1pt]{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}_{\\ne 0}}\n&&\\text{\\small ∧-rule and starting inference}\n\\\\[1ex]\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ∧-rule}\n\\\\\n&\\qquad=\\frac{\\bigl[1-\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\bigr]\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\lnot\\mathsfit{h}\\land \\mathsfit{f}\\land \\mathsfit{I})\\cdot\n\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small algebra}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∧-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}-\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small  ∨-rule}\n\\\\\n&\\qquad=\\frac{{\\color[RGB]{34,136,51}1} -\n\\mathrm{T}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})\n}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small starting inference}\n\\\\\n&\\qquad=\\frac{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n{\\mathrm{T}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})}\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad=\\color[RGB]{238,102,119}1\n&&\\text{\\small algebra}\n\\end{aligned}\n\\]\n\nTherefore \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\). We find that, indeed, the electronic component must for sure have failed the heating test!\n\n\n\n\n\n\n Exercise\n\n\n\nRetrace the proof above step by step. At each step, how was its particular rule (indicated on the right) used?\n\n\n\n\nThe way in which the rules can be applied to arrive at the target inference is not unique. In fact, in some concrete applications it can require a lot of work to find how to connect target inference with starting ones via the rules. The result, however, will always be the same:\n\n\n\n\n\n\n\n\n\n\n\nThe rules of truth-inference are self-consistent: even if applied in different sequences of steps, they always lead to the same final result.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nProve the target inference \\(\\color[RGB]{238,102,119}\\mathrm{T}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{s}\\land \\mathsfit{f}\\land \\mathsfit{I}) = 1\\) using the rules of truth-inference, but beginning from the starting inference \\(\\color[RGB]{34,136,51}\\mathrm{T}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{I})=1\\).\n\n\n\n\n[Optional] Equivalence with truth-tables\nIf you have studied Boolean algebra, you may be familiar with truth-tables; for instance the one for “and” displayed on the side. The truth-inference rules (6.1)–(6.4) contain the truth-tables that you already know as special cases.\n\n\n\n\n\n\\(\\mathsfit{X}\\)\n\\(\\mathsfit{Y}\\)\n\\(\\mathsfit{X}\\land \\mathsfit{Y}\\)\n\n\n\n\n1\n1\n1\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUse the truth-inference rules for “or” and “and” to build the truth-table for “or”. Check if it matches the one you already knew.\n\n\nThe truth-inference rules (6.1)–(6.4) are more complicated than truth-tables, but have two important advantages First, they allow us to work with conditionals, and to move sentences between proposals and conditionals. Second, they provide a smoother transition to the rules for probability-inference."
  },
  {
    "objectID": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "href": "truth_inference.html#logical-ai-agents-and-their-limitations",
    "title": "6  Truth inference",
    "section": "6.4 Logical AI agents and their limitations",
    "text": "6.4 Logical AI agents and their limitations\nThe truth-inference discussed in this section are also the rules that a logical AI agent should follow. For example, the automated control and fault-management programs in NASA spacecrafts, mentioned in § 5.1, are programmed according to these rules.\n\n\n\n\n\n\n Study reading\n\n\n\nLook over Ch. 7 in Artificial Intelligence.\n\n\nMany – if not most – inference problems that human and AI agents must face are, however, of the uncertain kind: it is not possible to surely infer the truth of some outcome, and the truth of some initial data or initial inferences may not be known either. We shall now see how to generalize the truth-inference rules to uncertain situations.\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOur cursory visit of formal logic only showed a microscopic part of this vast field. The study of truth-inference rules continues still today, with many exciting developments and applications. Feel free take a look at\n\nLogic in Computer Science\nMathematical Logic for Computer Science\nNatural Deduction Systems in Logic"
  },
  {
    "objectID": "probability_inference.html#sec-probability-def",
    "href": "probability_inference.html#sec-probability-def",
    "title": "7  Probability inference",
    "section": "7.1 When truth isn’t known: probability",
    "text": "7.1 When truth isn’t known: probability\nWhen we cross a busy city street we look left and right to check whether any cars are approaching. We typically don’t look up to check whether something is falling from the sky. Yet, couldn’t it be false that cars are approaching at that moment? and couldn’t it be true that some object is falling from the sky? Of course both events are possible. Then why do we look left and right, but not up?\nThe main reason is that we believe strongly that cars might be approaching, and believe very weakly that some object might be falling from the sky. In other words, we consider the first occurrence to be very probable, and the second extremely improbable.\nWe shall take the notion of probability as intuitively understood (just as we did with the notion of truth). Terms equivalent for “probability” are degree of belief, plausibility, credibility.\n\n\n\n\n\n\n Beware of likelihood as a synonym for probability\n\n\n\nIn technical discourse, “likelihood” means something different and is not a synonym of “probability”, as we’ll explain later.\n\n\nProbabilities are quantified between \\(0\\) and \\(1\\), or equivalently between \\(0\\%\\) and \\(100\\%\\). Assigning to a sentence a probability 1 is the same as saying that it is true; and a probability 0, that it is false. A probability of \\(0.5\\) represents a belief completely symmetric with respect to truth and falsity.\nLet’s emphasize and agree on some important facts about probabilities:\n\n Probabilities are assigned to sentences. We already discussed this point in § 5.3, but let’s reiterate it. Consider an engineer working on a problem of electric-power distribution in a specific geographical region. At a given moment the engineer may believe with \\(75\\%\\) probability that the measured average power output in the next hour will be 100 MW. The \\(75\\%\\) probability is assigned not to the quantity “100 MW”, but to the sentence\n\\[\n\\textsf{\\small`The measured average power output in the next hour will be 100\\,MW'}\n\\]\nThis difference is extremely important. Consider the alternative sentence\n\\[\n\\textsf{\\small`The average power output in the next hour will be \\emph{set} to 100\\,MW'}\n\\]\nthe numerical quantity is the same, but the meaning is very different. The probability can therefore be very different (if the engineer is the person deciding how to set that output, then the probability is \\(100\\%\\), because has decided and knows what the output is). The probability depends not only on a number, but on what it’s being done with that number – measuring, setting, third-party reporting, and so on. Often we write simply “\\(O \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}10\\,\\mathrm{W}\\)” provided that the full sentence behind this kind of shorthand is understood.\n Probabilities are agent- and context-dependent. A coin is tossed, comes down heads, and is quickly hidden from view. Alice sees that it landed heads-up. Bob instead doesn’t manage to see the outcome and has no clue. Alice considers the sentence \\(\\textsf{\\small`Coin came down heads'}\\) to be true, that is, to have \\(100\\%\\) probability. Bob considers the same sentence to have \\(50\\%\\) probability.\nNote how Alice and Bob assign two different probabilities to the same sentence; yet both assignments are completely rational. If Bob assigned \\(100\\%\\) to \\(\\textsf{\\small`heads'}\\), we would suspect that he had seen the outcome after all; if he assigned \\(0\\%\\) to \\(\\textsf{\\small`heads'}\\), we would consider that unreasonable (he didn’t see the outcome, so why exclude \\(\\textsf{\\small`heads'}\\)?). At the same time we would be baffled if Alice assigned only \\(50\\%\\) to \\(\\textsf{\\small`heads'}\\), because she actually saw that the outcome was heads; maybe we would hypothesize that she feels unsure about what she saw.\nAn omniscient agent would know the truth or falsity of every sentence, and assign only probabilities 0 or 1. Some authors speak of “actual (but unknown) probabilities”. But if there were “actual” probabilities, they would be all 0 or 1, and it would be pointless to speak about probabilities at all – every inference would be a truth-inference.\n Probabilities are not frequencies. Consider the fraction of defective mechanical components to total components produced per year in some factory. This quantity can be physically measured and, once measured, would be agreed upon by every agent. It is a frequency, not a degree of belief or probability.\nIt is important to understand the difference between probability and frequency: mixing them up may lead to sub-optimal decisions. Later we shall say more about the difference and the precise relations between probability and frequency.\nFrequencies can be unknown to some agents. Probabilities cannot be “unknown”: they can only be difficult to calculate. Be careful when you read authors speaking of an “unknown probability”: they actually mean either “unknown frequency”, or a probability that has to be calculated (it’s “unknown” in the same sense that the value of \\(1-0.7 \\cdot 0.2/(1-0.3)\\) is “unknown” to you right now).\n Probabilities are not physical properties. Whether a tossed coin lands heads up or tails up is fully determined by the initial conditions (position, orientation, momentum, rotational momentum) of the toss and the boundary conditions (air velocity and pressure) during the flight. The same is true for all macroscopic engineering phenomena (even quantum phenomena have never been proved to be non-deterministic, and there are deterministic and experimentally consistent mathematical representations of quantum theory). So we cannot measure a probability using some physical apparatus; and the mechanisms underlying any engineering problem boil down to physical laws, not to probabilities.\n\n\n\n\n\n\n\n Study reading\n\n\n\nDynamical Bias in the Coin Toss. \n\n\n\n\nThese points listed above are not just a matter of principle. They have important practical consequences. A data scientist who is not attentive to the source of the data (measured? set? reported, and so maybe less trustworthy?), or who does not carefully assess the context of a probability, or who mixes a probability with a frequency, or who does not take advantage (when possible) of the physics involved in the a problem – such data scientist will design systems with sub-optimal performance1 – or even cause deaths.1 This fact can be mathematically proven."
  },
  {
    "objectID": "probability_inference.html#sec-uncertain-inference",
    "href": "probability_inference.html#sec-uncertain-inference",
    "title": "7  Probability inference",
    "section": "7.2 An unsure inference",
    "text": "7.2 An unsure inference\nConsider now the following variation of the trivial inference problem of § 6.1.\n\nThis electric component had an early failure. If an electric component fails early, then at production it either didn’t pass the heating test or didn’t pass the shock test. The probability that it didn’t pass both tests is 10%. There’s no reason to believe that the component passed the heating test, more than it passed the shock test.\n\nThe inspector wants to assess, also in this case, whether the component did not pass the heating test.\nFrom the data and information given, what would you say is the probability that the component didn’t pass the heating test?\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to argue why a conclusion cannot be drawn with certainty in this case. One way to argue this is to present two different scenarios that fit the given data but have opposite conclusions.\nTry to reason intuitively and assess the probability that the component didn’t pass the heating test. Should it be larger or smaller than 50%? Why?"
  },
  {
    "objectID": "probability_inference.html#probability-notation",
    "href": "probability_inference.html#probability-notation",
    "title": "7  Probability inference",
    "section": "7.3 Probability notation",
    "text": "7.3 Probability notation\nFor this inference problem we can’t find a true or false final value. The truth-inference rules (6.1)–(6.4) therefore cannot help us here. In fact even the “\\(\\mathrm{T}(\\dotso \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\dotso)\\)” notation is unsuitable, because it only admits the values \\(1\\) (true) and \\(0\\) (false).\nLet us first generalize this notation in a straightforward way:\nFirst, let’s represent the probability or degree of belief of a sentence by a number in the range \\([0,1]\\), that is, between \\(\\mathbf{1}\\) (certainty or true) and \\(\\mathbf{0}\\) (impossibility or false). The value \\(0.5\\) represents that the belief in the truth of the sentence is as strong as that in its falsity.\nSecond, let’s symbolically write that the probability of a proposal \\(\\mathsfit{Y}\\), given a conditional \\(\\mathsfit{X}\\), is some number \\(p\\), as follows:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = p\n\\]\nNote that this notation includes the notation for truth-values as a special case:\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\quad\\Longleftrightarrow\\quad\n\\mathrm{T}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}) = 0\\text{ or }1\n\\]"
  },
  {
    "objectID": "probability_inference.html#sec-fundamental",
    "href": "probability_inference.html#sec-fundamental",
    "title": "7  Probability inference",
    "section": "7.4 Inference rules",
    "text": "7.4 Inference rules\nExtending our truth-inference notation to probability-inference notation has been straightforward. But which rules should we use for drawing inferences when probabilities are involved?\nThe amazing result is that the rules for truth-inference, formulae (6.1)–(6.4), extend also to probability-inference. The only difference is that they now hold for all values in the range \\([0,1]\\), rather than for \\(0\\) and \\(1\\) only.\nThis important result was taken more or less for granted at least since Laplace in the 1700s. But was formally proven for the first time in the 1946 by R. T. Cox. The proof has been refined since then. What kind of proof is it? It shows that if we don’t follow the rules we are doomed to arrive at illogical conclusions; we’ll show some examples later.\n\n\nFinally, here are the fundamental rules of all inference. They are encoded by the following equations, which must always hold for any atomic or complex sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\):\n\n\n\n\n\n\n\n   THE FUNDAMENTAL LAWS OF INFERENCE   \n\n\n\n\n\n“Not” \\(\\boldsymbol{\\lnot}\\) rule\n\n\\[\\mathrm{P}(\\lnot \\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n+ \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= 1\\]\n\n\n“And” \\(\\boldsymbol{\\land}\\) rule\n\n\\[\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) \\cdot\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\n“Or” \\(\\boldsymbol{\\lor}\\) rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n= \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n- \\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\n\n\nSelf-consistency rule\n\n\\[\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z})\n= 1\n\\]\n\n\n\n\n\n\nIt is amazing that ALL inference is nothing else but a repeated application of these four rules – billions of times or more in some cases. All machine-learning algorithms are just applications or approximations of these rules. Methods that you may have heard about in statistics are just specific applications of these rules. Truth inferences are also special applications of these rules. Most of this course is, at bottom, just a study of how to apply these rules in particular kinds of problems.\n\n\n\n\n\n\n Study reading\n\n\n\n\nProbability, Frequency and Reasonable Expectation\nCh. 2 of Bayesian Logical Data Analysis for the Physical Sciences\nCh. 1 of Probability\n§§ 1.0–1.2 of Data Analysis\nFeel free to skim through §§ 2.0–2.4 of Probability Theory\n\n\n\n \nThe fundamental inference rules are used in the same way as their truth-inference special case: Each equality can be rewritten in different ways according to the usual rules of algebra. Then left and right side of the equality thus obtained can replace each other in a proof."
  },
  {
    "objectID": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "href": "probability_inference.html#solution-of-the-uncertain-inference-example",
    "title": "7  Probability inference",
    "section": "7.5 Solution of the uncertain-inference example",
    "text": "7.5 Solution of the uncertain-inference example\nArmed with the fundamental rules of inference, let’s solve our earlier inference problem. As usual we first analyse it, find what are its proposal and conditional, and which starting inferences are given in the problem.\n\nAtomic sentences\n\\[\n\\begin{aligned}\n\\mathsfit{h}&\\coloneqq\\textsf{\\small`The component passed the heating test'}\n\\\\\n\\mathsfit{s}&\\coloneqq\\textsf{\\small`The component passed the shock test'}\n\\\\\n\\mathsfit{f}&\\coloneqq\\textsf{\\small`The component had an early failure'}\n\\\\\n\\mathsfit{J}&\\coloneqq\\textsf{\\small (all other implicit background information)}\n\\end{aligned}\n\\]\nThe background information in this example is different from the previous, truth-inference one, so we use the different symbol \\(\\mathsfit{J}\\) for it.\n\n\nProposal, conditional, and target inference\nThe proposal is \\(\\lnot\\mathsfit{h}\\), just like in the truth-inference example.\nThe conditional is different now. We know that the component failed early, but we don’t know whether it passed the shock test. Hence the conditional is \\(\\mathsfit{f}\\land \\mathsfit{J}\\).\nThe target inference is therefore\n\\[\n\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\n\n\nStarting inferences\nWe are told that if an electric component fails early, then at production it didn’t pass either the heating test or the shock test. Let’s write this as\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\lor \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 1\n\\]\nWe are also told that there is a \\(10\\%\\) probability that both tests fail\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\land \\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = 0.1\n\\]\nFinally the problem says that there’s no reason to believe that the component didn’t pass the heating test, more than it didn’t pass the shock test. This can be written as follows:\n\\[\n\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J}) = \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n\\]\nNote this interesting situation: we are not given the numerical values of these two probabilities, we are only told that they are equal. This is an example of application of the principle of indifference, which we’ll discuss more in detail later.\n\n\nFinal inference\nAlso in this case there is no unique way of applying the rules to reach our target inference, but all ways lead to the same result. Let’s try to proceed backwards:\n\n\\[\n\\begin{aligned}\n&\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})&&\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\lor \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n+ {\\color[RGB]{34,136,51}\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ∨-rule}\n\\\\[1ex]\n&\\qquad= {\\color[RGB]{34,136,51}1}\n+ {\\color[RGB]{34,136,51}0.1}\n- \\mathrm{P}(\\lnot\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inferences}\n\\\\[1ex]\n&\\qquad= 0.1 + \\color[RGB]{34,136,51}\\mathrm{P}(\\mathsfit{s}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\\\[1ex]\n&\\qquad= 0.1 + \\mathrm{P}(\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small starting inference}\n\\\\[1ex]\n&\\qquad= 0.1 + 1 -\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\n&&\\text{\\small ¬-rule}\n\\end{aligned}\n\\]\n\nThe target probability appears on the left and right side with opposite signs. We can solve for it:\n\\[\n\\begin{aligned}\n2\\,{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.1 + 1\n\\\\[1ex]\n{\\color[RGB]{238,102,119}\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})} &= 0.55\n\\end{aligned}\n\\]\nSo the probability that the component didn’t pass the heating test is \\(55\\%\\).\n\n\n\n\n\n\n Exercises\n\n\n\n\nTry to find an intuitive explanation of why the probability is 55%, slightly larger than 50%. If your intuition says this probability is wrong, then\n\nCheck the proof of the inference for mistakes, or try to find a proof with a different path.\nExamine your intuition critically and educate it.\n\nCheck how the target probability \\(\\mathrm{P}(\\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) changes if we change the value of the probability \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})\\) from \\(0.1\\).\n\nWhat result do we obtain if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=0\\)? Can it be intuitively explained?\nWhat if \\(\\mathrm{P}(\\lnot\\mathsfit{s}\\land \\lnot\\mathsfit{h}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{f}\\land \\mathsfit{J})=1\\)? Does the result make sense?"
  },
  {
    "objectID": "probability_inference.html#how-the-inference-rules-are-used",
    "href": "probability_inference.html#how-the-inference-rules-are-used",
    "title": "7  Probability inference",
    "section": "7.6 How the inference rules are used",
    "text": "7.6 How the inference rules are used\nIn the solution above you noticed that the equations of the fundamental rules are not only used to obtain some of the probabilities appearing in them from the remaining probabilities.\nThe rules represent, first of all, constraints of logical consistency2 among probabilities. For instance, if we have probabilities  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{Z})=0.1\\),  \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.7\\),  and \\(\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})=0.2\\),  then there’s an inconsistency somewhere, because these values violate the and-rule:  \\(0.2 \\ne 0.1 \\cdot 0.7\\).  In this case we must find the inconsistency and solve it. However, since probabilities are quantified by real numbers, it’s possible and acceptable to have slight discrepancies within numerical round-off errors.2 The technical term is coherence.\nThe rules also imply more general constraints. For example we must always have\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}\\land \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\le \\min\\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\\\\n\\mathrm{P}(\\mathsfit{X}\\lor \\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) \\ge \\max \\set[\\big]{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}),\\  \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z})}\n\\end{gathered}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nTry to prove the two constraints above."
  },
  {
    "objectID": "probability_inference.html#derived-rules",
    "href": "probability_inference.html#derived-rules",
    "title": "7  Probability inference",
    "section": "7.7 Derived rules",
    "text": "7.7 Derived rules\nThe fundamental rules above are in principle all we need to use to draw inferences from other inferences. But from them it is possible to derive some “shortcut” rules.\n\nBoolean algebra\nFirst, it is possible to show that all rules you may know from Boolean algebra are a consequence of the fundamental rules. So we can always make the following convenient replacements anywhere in a probability expression:\n\n\n\n\n\n\nBoolean algebra\n\n\n\n\\[\n\\begin{gathered}\n\\lnot\\lnot \\mathsfit{X}= \\mathsfit{X}\n\\qquad\n\\mathsfit{X}\\land \\mathsfit{X}= \\mathsfit{X}\\lor \\mathsfit{X}= \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land \\mathsfit{Y}= \\mathsfit{Y}\\land \\mathsfit{X}\n\\qquad\n\\mathsfit{X}\\lor \\mathsfit{Y}= \\mathsfit{Y}\\lor \\mathsfit{X}\n\\\\[1ex]\n\\mathsfit{X}\\land (\\mathsfit{Y}\\lor \\mathsfit{Z}) = (\\mathsfit{X}\\land \\mathsfit{Y}) \\lor (\\mathsfit{X}\\land \\mathsfit{Z})\n\\\\[1ex]\n\\mathsfit{X}\\lor (\\mathsfit{Y}\\land \\mathsfit{Z}) = (\\mathsfit{X}\\lor \\mathsfit{Y}) \\land (\\mathsfit{X}\\lor \\mathsfit{Z})\n\\\\[1ex]\n\\lnot (\\mathsfit{X}\\land \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\lor \\lnot \\mathsfit{Y}\n\\qquad\n\\lnot (\\mathsfit{X}\\lor \\mathsfit{Y}) = \\lnot \\mathsfit{X}\\land \\lnot \\mathsfit{Y}\n\\end{gathered}\n\\]\n\n\n\n\n\n\nLaw of total probability or “extension of the conversation”\nSuppose we have a set of \\(n\\) sentences \\(set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) having these two properties:\n\nThey are mutually exclusive, meaning that the “and” of any two of them is false, given a conditional \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_2\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_1\\land\\mathsfit{Y}_3\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\\ , \\quad\n\\dotsc \\ , \\quad\n  \\mathrm{P}(\\mathsfit{Y}_{n-1}\\land\\mathsfit{Y}_n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 0\n  \\]\nThey are exhaustive, meaning that the “or” of all of them is true, given a conditional \\(\\mathsfit{Z}\\):\n\\[\n  \\mathrm{P}(\\mathsfit{Y}_1\\lor \\mathsfit{Y}_2 \\lor \\dotsb \\lor \\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{Z}) = 1\n  \\]\n\nThen the probability of a sentence \\(\\mathsfit{X}\\), conditional on \\(\\mathsfit{Z}\\), is equal to a combination of probabilities conditional on \\(\\mathsfit{Y}_1,\\mathsfit{Y}_2,\\dotsc\\):\n\n\n\n\n\n\nDerived rule: extension of the conversation\n\n\n\n\n\\[\n\\begin{aligned}\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) =\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_2 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +{}&\n\\\\[2ex]\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})&\n\\end{aligned}\n\\]\n\n\n\nThis rule is useful when it is difficult to assess the probability of a sentence conditional on the background information, but it is easier to assess the probabilities of that sentence conditional on several auxiliary sentences – often representing hypotheses that exclude one another, and of which we know at least one is true. The name extension of the conversation for this derived rule comes from the fact that we are able to call the additional sentences into play.\nThis situation occurs very often in concrete applications, especially in problems where the probabilities of several competing hypotheses have to be assessed.\nThe next derived rule is used extremely often, so we discuss it separately."
  },
  {
    "objectID": "probability_inference.html#sec-bayes-theorem",
    "href": "probability_inference.html#sec-bayes-theorem",
    "title": "7  Probability inference",
    "section": "7.8 Bayes’s theorem",
    "text": "7.8 Bayes’s theorem\nThe probably most famous – or infamous – rule derived from the laws of inference is Bayes’s theorem. It allows us to relate the probability where two sentences \\(\\mathsfit{Y},\\mathsfit{X}\\) appear in the proposal and the conditional, with one where they are exchanged:\n\n\n\n\n\nBayes’s theorem guest-starring in The Big Bang Theory\n\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}\\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}\n\\]\n\n\n\nObviously this rule can only be used if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) &gt; 0\\), that is, if the sentence \\(\\mathsfit{X}\\) is not false conditional on \\(\\mathsfit{Z}\\).\n\n\n\n\n\n\n Exercise\n\n\n\nProve Bayes’s theorem from the fundamental rules of inference.\n\n\nBayes’s theorem is extremely useful when we want to assess the probability of a sentence, typically a hypothesis, given some conditional, typically data; and we can easily assess the probability of the data conditional on the hypothesis. Note, however, that the sentences \\(\\mathsfit{Y}\\) and \\(\\mathsfit{X}\\) in the theorem can be about anything whatsoever: \\(\\mathsfit{Y}\\) does not always need to be a “hypothesis”, and \\(\\mathsfit{X}\\) “data”.\n\nCombining with the extension of the conversation\nBayes’s theorem is often with several sentences \\(\\set{\\mathsfit{Y}_1, \\mathsfit{Y}_2, \\dotsc, \\mathsfit{Y}_n}\\) that are mutually exclusive and exhaustive. Typically these represent competing hypotheses. In this case the probability of the sentence \\(\\mathsfit{X}\\) in the denominator can be expressed using the rule of extension of the conversation:\n\n\n\n\n\n\n\nDerived rule: Bayes’s theorem with extension of the conversation\n\n\n\n\n\\[\n\\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) =\n\\frac{\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})}{\n\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_1 \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z}) +\n\\dotsb + \\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Y}_n \\land \\mathsfit{Z})\\cdot \\mathrm{P}(\\mathsfit{Y}_n \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n}\n\\]\n\nand similarly for \\(\\mathsfit{Y}_2\\) and so on.\n\n\n\nWe will use this form of Bayes’s theorem very frequently.\n\n\nMany facets\nBayes’s theorem is a very general result of the fundamental rules of inference, valid for any sentences \\(\\mathsfit{X},\\mathsfit{Y},\\mathsfit{Z}\\). This generality leads to many uses and interpretations.\nThe theorem is often proclaimed to be the rule according to which we “update our beliefs”. The meaning of this proclamation is the following. Let’s say that at some point \\(\\mathsfit{Z}\\) represents all your knowledge. Your degree of belief about some sentence \\(\\mathsfit{Y}\\) is then (at least in theory) the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\\). At some later point, let’s say that you get to know – maybe thanks to an observation you made – that the sentence \\(\\mathsfit{X}\\) is true. Your whole knowledge at that point is represented no longer by \\(\\mathsfit{Z}\\), but by \\(\\mathsfit{X}\\land \\mathsfit{Z}\\). Your degree of belief about \\(\\mathsfit{Y}\\) is then given by the value of \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land\\mathsfit{Z})\\). Bayes’s theorem allows you to find your degree of belief about \\(\\mathsfit{Y}\\) conditional on your new state of knowledge, from the one conditional on your old state of knowledge.\nThis chronological element, however, comes only from this particular way of using Bayes’s theorem. The theorem can more generally be used to connect any two states of knowledge \\(\\mathsfit{Z}\\) and \\(\\mathsfit{X}\\land\\mathsfit{Z}\\), no matter their temporal order, even if they happen simultaneously, and even if they belong to two different agents.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing Bayes’s theorem and the fundamental laws of inference, prove that if \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})=1\\), that is, if you already know that \\(\\mathsfit{X}\\) is true in your current state of knowledge \\(\\mathsfit{Z}\\), then\n\\[\n\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{X}\\land \\mathsfit{Z}) = \\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{Z})\n\\]\nthat is, your degree of belief about \\(\\mathsfit{Y}\\) doesn’t change.\nIs this result reasonable?\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 4.1–4.3 in Medical Decision Making give one more point of view on Bayes’s theorem.\nCh. 3 of Probability\nA graphical explanation of how Bayes’s theorem works mathematically (using a specific interpretation of the theorem):"
  },
  {
    "objectID": "probability_inference.html#consequences-of-not-following-the-rules",
    "href": "probability_inference.html#consequences-of-not-following-the-rules",
    "title": "7  Probability inference",
    "section": "7.9 Consequences of not following the rules",
    "text": "7.9 Consequences of not following the rules\n@@ §12.2.3 of AI\n\nExercise: Monty-Hall problem & variations\nExercise: clinical test & diagnosis"
  },
  {
    "objectID": "probability_inference.html#remarks-on-terminology-and-notation",
    "href": "probability_inference.html#remarks-on-terminology-and-notation",
    "title": "7  Probability inference",
    "section": "7.10 Remarks on terminology and notation",
    "text": "7.10 Remarks on terminology and notation\n\nLikelihood\nIn everyday language, “likely” is often a synonym of “probable”; and “likelihood”, of “probability”. But in technical writings about probability, inference, and decision-making, “likelihood” has a very different meaning. Beware of this important difference in definition:\n\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\) is:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\) (or conditional on \\(\\mathsfit{X}\\)),\nthe likelihood of \\(\\mathsfit{X}\\) in view of \\(\\mathsfit{Y}\\).\n\nLet’s express this also in a different way:\n\nthe probability of \\(\\mathsfit{Y}\\) given \\(\\mathsfit{X}\\), is \\(\\mathrm{P}({\\color[RGB]{68,119,170}\\mathsfit{Y}}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\).\nthe likelihood of \\(\\mathsfit{Y}\\) in view of \\(\\mathsfit{X}\\), is \\(\\mathrm{P}(\\mathsfit{X}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}{\\color[RGB]{68,119,170}\\mathsfit{Y}})\\).\n\n\n\n\n\n\n\n\n\n\n\nA priori there is no relation between the probability and the likelihood of a sentence \\(\\mathsfit{Y}\\): this sentence could have very high probability and very low likelihood, and vice versa.\n\n\nIn these notes we’ll avoid the possibly confusing term “likelihood”. All we need to express can be phrased in terms of “probability”.\n\n\nOmitting background information\nIn the analyses of the inference examples of § 6.1 and § 7.2 we defined sentences (\\(\\mathsfit{I}\\) and \\(\\mathsfit{J}\\)) expressing all background information, and always included these sentences in the conditionals of the inferences – because those inferences obviously depended on that background information.\nIn many concrete inference problems the background information usually stays there in the conditional from beginning to end, while the other sentences jump around between conditional and proposal as we apply the rules of inference. For this reason the background information is often omitted from the notation, being implicitly understood. For instance, if the background information is denoted \\(\\mathsfit{I}\\), one writes\n\n“\\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{X}\\land \\mathsfit{I})\\)\n“\\(\\mathrm{P}(\\mathsfit{Y})\\)”  instead of  \\(\\mathrm{P}(\\mathsfit{Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\\)\n\nThis is what’s happening when you see in books probabilities “\\(P(x)\\)” without conditional.\nSuch practice may be convenient, but be wary of it, especially in particular situations:\n\nIn some inference problems we suddenly realize that we must distinguish between cases that depend on hypotheses, say \\(\\mathsfit{H}_1\\) and \\(\\mathsfit{H}_2\\), that were buried in the background information \\(\\mathsfit{I}\\). If the background information \\(\\mathsfit{I}\\) is explicitly reported in the notation, this is no problem: we can rewrite it as\n\\[ \\mathsfit{I}= (\\mathsfit{H}_1 \\lor \\mathsfit{H}_2) \\land \\mathsfit{I}'\\]\nand proceed, for example using the rule of extension of the conversation. If the background information was not explicitly written, this may lead to confusion and mistakes. For instance there may suddenly appear two instances of \\(\\mathrm{P}(\\mathsfit{X})\\) with different values, just because one of them is invisibly conditional on \\(\\mathsfit{I}\\), the other on \\(\\mathsfit{I}'\\).\nIn some inference problems we are considering several different instances of background information – for example because more than one agent is involved. It’s then extremely important to write the background information explicitly, lest we mix up the different agents’s degrees of belief.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nA once-famous paper published in the quantum-theory literature arrived at completely wrong results simply by omitting background information, mixing up probabilities having different conditionals.\n\n\nThis kind of confusion from poor notation happens more often than one thinks, and even appears in scientific literature.\n\n\n“Random variables”\nSome texts speak of the probability of a “random variable”, or more precisely of the probability “that a random variable takes on a particular value”. As you notice, we have just expressed that idea by means of a sentence. The viewpoint and terminology of random variables is therefore a special case of that based on sentences, which we use here.\nThe dialect of “random variables” does not offer any advantages in concepts, notation, terminology, or calculations, but it has several shortcomings:\n\n\n\n\n\nJames Clerk Maxwell is one of the main founders of statistical mechanics and kinetic theory (and electromagnetism). Yet he never used the word “random” in his technical writings. Maxwell is known for being very clear and meticulous with explanations and terminology.\n\n\n\nAs discussed in § 7.1, in concrete applications it is important to know how a quantity “takes on” a value: for example it could be directly measured, indirectly reported, or purposely set to that specific value. Thinking and working in terms of sentences, rather than of random variables, allows us to account for these important differences.\nVery often the object (proposal) of a probability is not a “variable”: it is actually a constant value that is simply unknown.\nWhat does “random” (or “chance”) mean? Good luck finding an understandable and non-circular definition in texts that use that word; strangely enough, they never define it. In these notes, if the word “random” is ever used, it stands for “unpredictable” or “unsystematic”.\n\nIt’s a question for sociology of science why some people keep on using less flexible points of view or terminologies. Probably they just memorize them as students and then a fossilization process sets in.\n\n\nFinally, some texts speak of the probability of an “event”. For all purposes an “event” is just what’s expressed in a sentence."
  },
  {
    "objectID": "1st_connection_ML.html",
    "href": "1st_connection_ML.html",
    "title": "8  A first connection with machine learning",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\n\n\n\nIn these first chapters we have been developing notions and methods about agents drawing inferences and making decisions, sentences expressing facts and information, and probabilities expressing uncertainty and certainty. Let’s draw some first qualitative connections between these notions and notions typically used in machine learning.\nA machine-learning algorithm is usually presented in textbooks as something that first “learns” from some training data, and thereafter performs some kind of task – typically it yields a response or outcome of some kind. More precisely, the training data are instances or examples of the task that the algorithm is expected to perform. These instances have a special status because their details are fully known, whereas new instances, where the algorithm will be applied, have some uncertain elements: typically they have an ideal or optimal outcome, but this outcome is unknown beforehand. The response given by the algorithm in new instances depend on the algorithm’s internal architecture and parameters; for brevity we shall just use “architecture” to mean both.\nLet’s try to rephrase this description from the point of view developed in the past chapters. A machine-learning algorithm is given known pieces of information (the training data), and then forms some kind of connection with another piece of information of a similar kind (the outcome in a new application) that was not known beforehand. The connection depends on the algorithm’s architecture.\nThis has strong similarities to what an agent does when drawing an inference: it uses known pieces of information, expressed by sentences \\({\\color[RGB]{34,136,51}\\mathsfit{D}_1}, {\\color[RGB]{34,136,51}\\mathsfit{D}_2}, {\\color[RGB]{34,136,51}\\dots}, {\\color[RGB]{34,136,51}\\mathsfit{D}_N}\\), together with some background or built-in information \\(\\color[RGB]{204,187,68}\\mathsfit{I}\\), in order to calculate the probability of a piece of information of a similar kind, expressed by a sentence \\(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\):\n\\[\n\\mathrm{P}(\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}\\color[RGB]{0,0,0}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\mathsfit{D}_{N} \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1 \\color[RGB]{0,0,0}\\land {\\color[RGB]{204,187,68}\\mathsfit{I}})\n\\]\nWe can thus consider a first tentative correspondence:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThis correspondence seems convincing with regard to architecture and training data: in both cases we’re speaking about the use of pre-existing or built-in information, combined with additional one.\nBut it is less convincing with regarding to the outcome, because an agent gives the probabilities for several possible “outputs”, it doesn’t just yield one. This indicates that there must be also be some decision involved among the possible outcomes.\nWe’ll return to this tentative connection later."
  },
  {
    "objectID": "quantities_types.html#quantities",
    "href": "quantities_types.html#quantities",
    "title": "9  Quantities and data types",
    "section": "9.1 Quantities",
    "text": "9.1 Quantities\n\nQuantities, values, domains\nMost decisions and inferences in engineering and data science involve things or properties of real things. We represent them by mathematical objects – most often, collections of numbers – with particular mathematical properties and operations. The mathematical properties reflect the kind of activities that we can do with these things. For instance, colours are represented by particular tuples of numbers, and these tuples can be multiplied by some numeric weights and added, to obtain another tuple. This mathematical operation represents the fact that colours can be obtained by mixing other colours in different proportions. Physics and engineering are founded on this approach.\nIt’s difficult to find a general term to denote any instance of such “things” and their mathematical representation, but it’s convenient to find one for presenting the general theory without getting bogged down in individual cases. We’ll borrow the term quantity from physics and engineering.\nWe distinguish between a quantity and its value. For instance, a quantity could be “The temperature at the point with coordinates  60.3775029, 5.3869233, 643,  at time 1895-10-04T10:03:14Z”; and its value could be \\(24\\,\\mathrm{°C}\\).\nThis distinction is necessary in inference and decision problems, because we may not know the value of a particular quantity. We then consider every possible value that quantity could have, and we can assign a probability to each. The set of possible values is called the domain of the quantity. In our temperature example, the domain is the set of all possible temperatures from \\(0\\,\\mathrm{K}\\) and above.\nAnother example:\n\nquantity: the image taken by a particular camera at a particular time, represented by a specific collection of numbers (say 128 × 128 × 3$ integers between \\(0\\) and \\(255\\))\nvalues: one possible value is this:  (corresponding to a grid of 128 × 128 × 3 specific numbers), another possible value is this: , and there are many other possible values\ndomain: the collection of \\(256^{3\\times128\\times128} \\approx 10^{118 370}\\) possible images (corresponding to the collection of possible grids of numeric values)\n\nOther examples of quantities and domains:\n\nThe distance between two objects in the Solar System at a specific time. The domain could be, say, all values from \\(0\\,\\mathrm{m}\\) to \\(6\\cdot10^{12}\\,\\mathrm{m}\\) (Pluto’s average orbital distance).\nThe number of total views of some online video (at a specific time), with a domain, say, from 0 to 20 billions.\nThe force on an object (at a specific time and place). The domain could be, say, 3D vectors with components in \\([-100\\,\\mathrm{N},\\,+100\\,\\mathrm{N}]\\).\nThe degree of satisfaction in a customer survey, with five possible values Not at all satisfied, Slightly satisfied, Moderately satisfied, Very satisfied, Extremely satisfied.\nThe graph representing a particular social network. Individuals are represented by nodes, and different kinds of relationships by directed or undirected links between nodes, possibly with numbers indicating their strength. The domain consists of all possible graphs with, say, \\(0\\) to \\(10000\\) nodes and all possible combinations of links and weights between the nodes.\nThe relationship between the input voltage and output current of an electric component. The domain could be all possible continuous curves from \\([0\\,\\mathrm{V}, 10\\,\\mathrm{V}]\\) to \\([0\\,\\mathrm{A}, 1\\,\\mathrm{A}]\\).\nA 1-minute audio track recorded by a device with a sampling frequency of 48 kHz. The domain could be all possible 2 880 000 values in \\([0,1]\\).\nThe subject of an image, with domain of three possible values cat, dog, something else.\nThe roll, pitch, yaw of a rocket (at a specific time and place), with domain \\((-180°,+180°]\\times(-90°,+90°]\\times(-180°,+180°]\\).\n\n\n\nThe vague term “data” typically means the values of a collection of quantities.\n\n\n\n\n\n\n Quantity vs variate\n\n\n\nIn these notes, we agree that a quantity has one, and only one, value.\nWe can consider something that changes with time, or with space, or from individual to individual, or from unit to unit. This “something” is then a collection of quantities: one for each time, or space, or individual. We call this collection a variate, especially when it refers to individuals or unit; or a variable.\nThese are just terminological conventions adopted in these notes. Different texts often adopt different terms. What matters is not the terms, but that you have a clear understanding of the difference between the two notions that here we call “quantity” and “variate”.\n\n\n\n\nNotation\nWe shall denote quantities by italic letters, such as \\(X\\) for example. The sentences that appear in decision-making and inferences are therefore often of the kind “the quantity \\(X\\) was observed to have value \\(x\\)”, where “\\(x\\)” stands for a specific value, for instance . This kind of sentences are often abbreviated like “\\(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x\\)”.\n\n\n\n\n\n\n\n\n\n\n but keep in mind our discussion from § 5.3: we must make clear what that “\\(=\\)” means; it could mean “observed”, “set”, “reported”, and so on."
  },
  {
    "objectID": "quantities_types.html#sec-basic-types",
    "href": "quantities_types.html#sec-basic-types",
    "title": "9  Quantities and data types",
    "section": "9.2 Basic types of quantities",
    "text": "9.2 Basic types of quantities\nAs the examples above show, quantities and data come in all sorts and with different degrees of complexity. There is no clear-cut divide between different sorts of quantities. The same quantity can moreover be viewed and represented in many different ways, depending on the specific context, problem, purpose, and background information.\nIt is possible, however, to roughly differentiate between a handful of basic types of quantities, from which more complex types are built. Here is one kind of differentiation that is useful for inference problems about quantities:\n\nNominal\nA nominal or categorical quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, and do not have any specific order.\nThis means that it does not make sense to say, for instance, that some value is “twice” or “1.5 times” another, or “larger” or “later” than another one. Nor does it make sense to “add” two quantities. In particular, there is no notion of average for a nominal quantity.\nExamples: the possible breeds of a dog, or the characters of a film.\nIt is of course possible to represent the values of a nominal quantity with numbers; say 1 for Dachshund, 2 for Labrador, 3 for Dalmatian, and so on. But that doesn’t mean that\nDalmatian\\({}-{}\\)Labrador\\({}={}\\)Labrador\\({}-{}\\)Dachshund\nor similar nonsense.\n\n\nOrdinal\nAn ordinal quantity has a domain with a discrete and usually finite number of values. The values are not related by any mathematical property, but they do have a specific order.\nThis means that it does not make sense to say that some value is “twice” or “1.5 times” another, and we cannot “add” two values. But it does make sense to say, for any two values, which one has higher rank, for example “stronger”, or “later”, “larger”, and similar. Also in this case there is no notion of average for an ordinal quantity.\nExample: a pain-intensity scale. A patient can say whether some pain is more severe than another, but it isn’t clear what pain “twice as severe” as another would mean (although there’s a lot of research on more precise quantification of pain). Another example: the “strength of friendship” in a social network. We can say that we have a “stronger friendship” with a person than with another; but it doesn’t make sense to say that we are “four times stronger friends” with a person than with another.\nIt is possible to represent the values of an ordinal quantity with numbers which reflect the order of the values. But it’s important to keep in mind that differences or averages of such numbers does not make sense. For this reason the use of numbers can be deceptive at times. A less deceptive possibility is to represent ordered values by alphabet letters, for example.\n\n\nBinary\nA binary or dichotomous quantity has only two possible values. It can be seen as a special case of a nominal or ordinal quantity, but the fact of having only two values lends it some special properties in inference problems. This is why we list it separately.\nObviously it doesn’t make much sense to speak of the difference or average of the two values; and their ranking is trivial even if it makes sense.\nThere’s an abundance of examples of binary quantities: yes/no answers, presence/absence of something, and so on.\n\n\nInterval\nAn interval quantity has a domain that can be discrete or continuous, finite or infinite. The values do admit some mathematical operations, at least convex combination and subtraction. They also admit an ordering.\nThis means that we can say, at the very least, whether the interval or “distance” between a pair of values is the same, or larger, or smaller than the interval between another pair. For this reason we can also say whether a value is larger than another. We can also take weighted sums of values, called convex combinations (but simple addition of values may still be meaningless, though).\nOwing to these mathematical properties, it does make sense to speak of the average for an interval quantity.\nThe number of electronic components produced in a year by an assembly line is an example of a discrete interval quantity. The power output of a nuclear plant at a given time is an example of a continuous one.\nIt is also possible to speak of ratio quantities, which are a special case of interval quantities, but we won’t have use of this distinction in the present notes.\n\n\nHow to decide the basic type of a quantity?\nTo attribute a basic type to a quantity we must ultimately check how that quantity is defined, obtained, and used. In some cases the values of the quantity may give some clue; for example, if we see values “\\(2.74\\)”, “\\(8.23\\)”, “\\(3.01\\)”, then the quantity is probably of the interval kind. But if we see values “\\(1\\)”, “\\(2\\)”, “\\(3\\)”, it’s unclear whether the quantity is interval, ordinal, nominal, or maybe of yet some other kind.\nThe type of a quantity also depends on its use in the specific problem. A quantity of a more complex type can be treated as a simpler type if needed. For instance, the response time of some device is in principle an interval quantity; but in a specific situation we could simply label its values as slow, medium, fast, thus making it an ordinal quantity.\n@@ TODO: add examples for image spaces\n\n\n\n\n\n\n Exercises\n\n\n\n\nFor each example at the beginning of the present section, assess whether that quantity can be considered as being of a basic type, and which type.\nFor each basic type discussed above, find two more concrete examples of that type of quantity\n\n\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nOn the theory of scales of measurement"
  },
  {
    "objectID": "quantities_types.html#other-attributes-of-basic-types",
    "href": "quantities_types.html#other-attributes-of-basic-types",
    "title": "9  Quantities and data types",
    "section": "9.3 Other attributes of basic types",
    "text": "9.3 Other attributes of basic types\nIt is useful to consider other basic aspects of quantities that are somewhat transversal to “type”. These aspects are also important when drawing inferences.\n\nDiscrete vs continuous\nNominal and ordinal quantities have discrete domains. The domain of an interval quantity can be discrete or continuous. In practice all domains are discrete, since we cannot observe, measure, report, or store values with infinite precision. In a modern computer, for example, a real number can “only” take on \\(2^{64} \\approx 20 000 000 000 000 000 000\\) possible values. In many situations the available precision is so high that we can consider the quantity as continuous for all practical purposes and use the mathematics of continuous sets – derivation, integration, and so on – to our advantage.\n@@ TODO comment on repetition\n\n\nBounded vs unbounded\nOrdinal and interval quantities may have domains with no minimum value, or no maximum value, or neither. Typical terms for these situations are lower- or upper-bounded, or left- or right-bounded, and unbounded; or similar terms.\nWhether to treat a quantity domain as bounded or unbounded depends on the quantity, the specific problem, and the computational resources. For example, the number of times a link on a webpage has been clicked can in principle be (right-)unbounded. Another example is the distance between two objects: we can consider it unbounded, but in concrete problems might be bounded, say, by the size of a laboratory, or by Earth’s circumference, or the Solar System’s extension, and so on.\n\n\n\n\n\n\n Exercises\n\n\n\n\nIf you had to set a maximum number of times a web link can be clicked, what number would you choose? Try to find a reasonable number, considering factors such as how fast a person can repeatedly click on a link, how long can a website (or the Earth?) last, and how many people can live in such an extent of time.\nWhat about the age of a person? What bound would you set, if you had to treat it as a bounded quantity?\n\n\n\n\n\nFinite vs infinite\nThe domain of a discrete quantity can consist of a finite or – at least in theory – an infinite number of possible values (the domain of a continuous quantity always has an infinite number of values). A domain can be infinite and yet bounded: consider the numbers in the range \\([0,1]\\).\nWhether to treat a domain as finite or infinite depends on the quantity, the specific problem, and the computational resources. For example, the intensity of a base colour in a pixel of a particular image might really take on 256 discrete steps between \\(0\\) and \\(1\\): \\(0, 0.0039215686, 0.0078431373, \\dotsc, 1\\). But in some situations we can treat this domain as practically infinite, with any possible value between \\(0\\) and \\(1\\).\n\n\nRounded\nA continuous interval quantity may be rounded, owing to the way it’s measured. In this case the quantity could be considered discrete rather than continuous. Rounding can impact the way we do inferences about such a quantity.\n\n\n The Iris dataset from its original paper\nThe famous Iris dataset, for instance, consists of several lengths – continuous interval quantities – of parts of flowers. All values are rounded to the millimetre, even if in reality the lengths could have intermediate values, of course. The age of a person is another frequent example of an in-principle continuous quantity which is rounded, say to the year or the month.\nIn some situations it’s important to be aware of rounding, because it can lead to quantities with different unrounded values to have identical rounded ones.\n\n\nCensored\nThe measurement procedure of a quantity may have an artificial lower or upper bound. A clinical thermometer, for instance, could have a maximum reading of \\(45\\,\\mathrm{°C}\\). If we measure with it the temperature of a \\(50\\,\\mathrm{°C}\\)-hot body, we’ll read “\\(45\\,\\mathrm{°C}\\)”, not the real temperature.\nA quantity with this characteristic is called censored, more specifically left-censored or right-censored when there’s only one artificial bound. The bound is called the censoring value.\nA censoring value denotes an actual value that could also be greater or less. This is important when we draw inferences about this kind of quantities."
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-multiv",
    "href": "quantities_types_multi.html#sec-data-multiv",
    "title": "10  Joint quantities and complex data types",
    "section": "10.1 Joint quantities",
    "text": "10.1 Joint quantities\nSome sets of basic quantities are just that: simple collections of quantities, in the sense that they do not have new properties or allow for new kinds of operations. We shall call these joint quantities when we need to distinguish them from quantities of a basic kind; but usually they are also simply called quantities.\nThe values of a joint quantity are just tuples of values of its basic component quantities. Their domain is the Cartesian product of the domains of the basic quantities.\nConsider for instance the age, sex1, and nationality of a particular individual. They can be represented as an interval-continuous quantity \\(A\\), a binary one \\(S\\), and a nominal one \\(N\\). We can join them together to form the joint quantity  “(age, sex, nationality)”  which can be denoted by  \\((A,S,N)\\).  One value of this joint quantity is, for example, \\((25\\,\\mathrm{y}, \\texttt{\\small F}, \\texttt{\\small Norwegian})\\). The domain could be1 We define sex by the presence of at least one Y chromosome or not. It is different from gender, which involves how a person identifies.\n\\[\n[0,+\\infty)\\times\n\\set{\\texttt{\\small F}, \\texttt{\\small M}} \\times\n\\set{\\texttt{\\small Afghan}, \\texttt{\\small Albanian}, \\dotsc, \\texttt{\\small Zimbabwean}}\n\\]\n\nDiscreteness, boundedness, continuity\nA joint quantity may not be simply characterized as “discrete”, or “bounded”, or “infinite”, and so on. Usually we must specify these characteristics for each of its basic component quantities instead. Sometimes a joint quantity is called, for instance, “continuous” if all its basic components are continuous; but other conventions are also used.\n\n\n\n\n\n\n Exercises\n\n\n\nConsider again the examples of § 9.1.1. Do you find any examples of joint quantities?"
  },
  {
    "objectID": "quantities_types_multi.html#sec-data-complex",
    "href": "quantities_types_multi.html#sec-data-complex",
    "title": "10  Joint quantities and complex data types",
    "section": "10.2 Complex quantities",
    "text": "10.2 Complex quantities\nSome complex quantities can be represented as sets of quantities of basic types. These sets, however, are “more than the sum of their parts”: they possess new physical and mathematical properties and operations that do not apply or do not make sense for the single components.\nFamiliar examples are vectorial quantities from physics and engineering, such as location, velocity, force, torque. Another example are images, when represented as grids of basic quantities.\nConsider for example a 4 × 4 monochrome image, represented as a grid of 16 binary quantities \\(0\\) or \\(1\\). Three possible values could be these:\n    \nrepresented by the numeric matrices   \\(\\begin{psmallmatrix}1&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&1&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\end{psmallmatrix}\\), \\(\\begin{psmallmatrix}0&0&0&0\\\\0&0&0&0\\\\0&0&0&0\\\\0&0&0&1\\end{psmallmatrix}\\).\nFrom the point of view of the individual binary quantities, these three “values” are equally different from one another: where one of them has grid value \\(1\\), the others have \\(0\\). But properly considered as images, we can say that the first and the second are somewhat more “similar” or “closer” to each other than the first and the third. This similarity can be represented and quantified by a metric over the domain of all such images. This metric involves all basic binary quantities at once; it is not a property of each of them individually.\nMore generally, complex quantities have additional, peculiar properties, represented by mathematical structures, which distinguish them from simpler joint quantities; although there is not a clear separation between the two.\nThese properties and structures are very important for inference problems, and usually make them computationally very hard. The importance of machine-learning methods lies to a great extent in the fact that they allow us to do approximate inference on these kinds of complex data. The peculiar structures of these data, however, are often also the cause of striking failures of some machine-learning methods, for example the reason why they may classify incorrectly, or correctly but for the wrong reasons."
  },
  {
    "objectID": "probability_distributions.html#distribution-of-probabilities-among-values",
    "href": "probability_distributions.html#distribution-of-probabilities-among-values",
    "title": "11  Probability distributions",
    "section": "11.1 Distribution of probabilities among values",
    "text": "11.1 Distribution of probabilities among values\nWhen an agent is uncertain about the value of a quantity, its uncertainty is expressed and quantified by assigning a degree of belief, conditional on the agent’s knowledge, to all the possible cases regarding the true value.\nFor a temperature measurement, for instance, the cases could be “The temperature is measured to have value 271 K”, “The temperature is measured to have value 272 K”, and so on up to 275 K. These cases are expressed by mutually exclusive and exhaustive sentences. Denoting th temperature with \\(T\\), these sentences can be abbreviated as\n\\[\n{\\color[RGB]{34,136,51}T = 271\\,\\mathrm{K}} \\ , \\quad\n{\\color[RGB]{34,136,51}T = 272\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 273\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 274\\,\\mathrm{K} \\ ,} \\quad\n{\\color[RGB]{34,136,51}T = 275\\,\\mathrm{K}} \\ .\n\\]\nThe agent’s belief about the quantity is then expressed by the probabilities about these sentences, conditional on the agent’s state of knowledge \\({\\color[RGB]{204,187,68}\\mathsfit{I}}\\): \n\\[\\begin{aligned}\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.04} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}272\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.10} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}273\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.18} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}274\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.28} \\\\[1ex]\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) &= {\\color[RGB]{170,51,119}0.40}\n\\end{aligned}\n\\]\nthat sum up to one:\n\\[\n\\begin{aligned}\n&\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}271\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) +\n\\dotsb +\n\\mathrm{P}({\\color[RGB]{34,136,51}T \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}275\\,\\mathrm{K}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}0.04}+{\\color[RGB]{170,51,119}0.10}+{\\color[RGB]{170,51,119}0.18}+{\\color[RGB]{170,51,119}0.28}+{\\color[RGB]{170,51,119}0.40} \\\\\n&\\quad{}=\n{\\color[RGB]{170,51,119}1}\n\\end{aligned}\\]\nThis collection of probabilities is called a probability distribution.\n\n\n\n\n\n\n What’s “distributed”?\n\n\n\nThe probability is distributed among the possible values, not the quantity, as illustrated in the side picture. The quantity cannot be “distributed”: it has one, definite value, which is however unknown to us.\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nConsider three sentences \\(\\mathsfit{X}_1, \\mathsfit{X}_2, \\mathsfit{X}_3\\) that are mutually exclusive and exhaustive on conditional \\(\\mathsfit{I}\\), that is:\n\\[\n\\begin{gathered}\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_1 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) =\n\\mathrm{P}(\\mathsfit{X}_2 \\land \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 0\n\\\\\n\\mathrm{P}(\\mathsfit{X}_1 \\lor \\mathsfit{X}_2 \\lor \\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\end{gathered}\n\\]\nProve, using the fundamental rules of inferences and any derived rules from § 7, that we must then have\n\\[\n\\mathrm{P}(\\mathsfit{X}_1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) + \\mathrm{P}(\\mathsfit{X}_3 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = 1\n\\]\n\n\nLet’s see how probability distributions can be represented and visualized for the basic types of quantities discussed in § 9.\nWe start with probability distributions over discrete domains."
  },
  {
    "objectID": "probability_distributions.html#discrete-probability-distributions",
    "href": "probability_distributions.html#discrete-probability-distributions",
    "title": "11  Probability distributions",
    "section": "11.2 Discrete probability distributions",
    "text": "11.2 Discrete probability distributions\n\nTables and functions\nA probability distribution over a discrete domain can obviously be displayed as a table of values and their probabilities. For instance\n\n\n\n\n\n\n\n\n\n\n\nvalue\n271 K\n272 K\n273 K\n274 K\n275 K\n\n\n\n\nprobability\n0.04\n0.10\n0.18\n0.28\n0.40\n\n\n\nIn the case of ordinal or interval quantities it is sometimes possible to express the probability as a function of the value. For instance, the probability distribution above could be summarized by this function of the value \\({\\color[RGB]{34,136,51}t}\\):\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}t} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{204,187,68}\\mathsfit{I}}) =\n{\\color[RGB]{170,51,119}\\frac{({\\color[RGB]{34,136,51}t}/\\textrm{\\small K} - 269)^2}{90}}\n\\quad\\text{\\small (rounded to two decimals)}\n\\]\n\n\nA graphical representation is often helpful to detect features, peculiarities, and even inconsistencies in one or more probability distributions.\n\n\nHistograms and area-based representations\nA probability distribution for a nominal, ordinal, and discrete interval quantity can be neatly represented by a histogram.\n\n\n\n\n\nHistogram for the probability distribution over possible component failures\n\n\nThe possible values are placed on a line. For an ordinal or interval quantity, the sequence of values on the line should correspond to their natural order. For a nominal quantity the order is irrelevant.\nA rectangle is then drawn above each value. Typically the rectangles are contiguous. The bases of the rectangles are all equal, and the areas of the rectangles are proportional to the probabilities. Since the bases are equal, this implies that the heights of the rectangles are also proportional to the probabilities.\nSuch kind of drawing can of course be horizontal, vertical, upside-down, and so on, depending on convenience.\nSince the probabilities must sum to one, the total area of the rectangles represents the unit of area. So in principle there is no need of writing probability values on some vertical axis, or grid, or similar visual device, because the probability value can be visually read as the ratio of a rectangle area to the total area. An axis or grid can nevertheless be helpful. Alternatively the probabilities can be reported above or below each rectangle.\nNominal quantities do not have any specific order, so their values do not need to be ordered on a line. Other area-based representations, such as pie charts, can also be used for these quantities.\n\n\nLine-based representations\nHistograms give faithful representations of discrete probability distributions. Their graphical bulkiness, however, can be a disadvantage in some situations; for instance when we want to have a clearer idea of how the probability distribution varies across values (for ordinal or interval quantities); or when we want to compare several probability distributions over the same values.\n\n\n Representation of the same pair of probability distributions with a histogram plot and a line plot \nIn these cases we can use standard line plots, or variations thereof. Compare the examples on the margin figure: the line plot displays more cleanly the differences between the “before-inspection” and “after-inspection” probability distributions."
  },
  {
    "objectID": "probability_distributions.html#probability-distributions-over-infinite-discrete-values",
    "href": "probability_distributions.html#probability-distributions-over-infinite-discrete-values",
    "title": "11  Probability distributions",
    "section": "11.3 Probability distributions over infinite discrete values",
    "text": "11.3 Probability distributions over infinite discrete values\n@@ TODO"
  },
  {
    "objectID": "probability_distributions.html#sec-prob-densities",
    "href": "probability_distributions.html#sec-prob-densities",
    "title": "11  Probability distributions",
    "section": "11.4 Probability densities",
    "text": "11.4 Probability densities\nDistributions of probability over continuous domains present several counter-intuitive aspects, which essentially arise because we are dealing with uncountable infinities – while often still using linguistic expressions that make at most sense for countable infinities. Here we follow a practical and realistic approach for working with such distributions.\nConsider a quantity \\(X\\) with a continuous domain. When we say that such a quantity has some value \\(x\\) we really mean that it has a value somewhere in the range   \\(x -\\epsilon/2\\)  to  \\(x+\\epsilon/2\\),   where the width \\(\\epsilon\\) is usually extremely small. For example, for double-precision values stored in a computer, the width must be at least \\(\\epsilon \\approx 2\\cdot 10^{-16}\\) :\n## R code\n&gt; 1.234567890123456 == 1.234567890123455\n[1] FALSE\n\n&gt; 1.2345678901234567 == 1.2345678901234566\n[1] TRUE\nand a value 1.3 really represents a range between 1.29999999999999982236431605997495353221893310546875 and 1.300000000000000266453525910037569701671600341796875, this range coming from the internal binary representation of 1.3. Often the width \\(\\epsilon\\) is much larger than the computer’s precision, and comes from the precision with which the value is experimentally measured.\nProbabilities are therefore assigned to such small ranges, not to single values. Since these ranges are very small, they are also very numerous. The total probability assigned to all of them must still amount to \\(1\\); therefore each small range receives an extremely small amount of probability. A standard Gaussian distribution for a real quantity, for instance, assigns a probability of approximately \\(8\\cdot 10^{-17}\\), or \\(0.00000000000000008\\), to a range of width \\(2\\cdot 10^{-16}\\) around the value \\(0\\). All other ranges are assigned even smaller probabilities.\nIn would be impractical to work with such small probabilities. We use probability densities instead. As implied by the term “density”, a probability density is the amount of probability \\(P\\) assigned to a standard range of width \\(\\epsilon\\), divided by that width. For example, if the probability assigned to a range of width  \\(\\epsilon=2\\cdot10^{-16}\\)  around \\(0\\) is  \\(P=7.97885\\cdot10^{-17}\\),  then the probability density around \\(0\\) is\n\\[\n\\frac{P}{\\epsilon} =\n\\frac{7.97885\\cdot10^{-17}}{2\\cdot10^{-16}} = 0.398942\n\\]\nwhich is a simpler number to work with.\nProbability densities are convenient because they usually do not depend on the range width \\(\\epsilon\\), if it’s small enough. Owing to physics reasons, we don’t expect a situation where \\(X\\) is between \\(0.9999999999999999\\) and \\(1.0000000000000001\\) to be very different from one where \\(X\\) is between \\(1.0000000000000001\\) and \\(1.0000000000000003\\). The probabilities assigned to these two small ranges of width \\(\\epsilon=2\\cdot 10^{-16}\\) each will therefore be approximately equal, let’s say \\(P\\) each. Now if we use a small range of width \\(\\epsilon\\) around \\(X=1\\), the probability is \\(P\\), and the probability density is \\(P/\\epsilon\\). If we consider a range of double width \\(2\\,\\epsilon\\) around \\(X=1\\), then the probability is \\(P+P\\) instead, but the probability density is still\n\\[\\frac{P+P}{2\\,\\epsilon} =\n\\frac{1.59577\\cdot10^{-16}}{4\\cdot10^{-16}}\n= 0.398942 \\ .\n\\]\nAs you see, even if we consider a range with double the width as before, the probability density is still the same.\n\n\nIn these notes we’ll denote probability densities with a lowercase \\(\\mathrm{p}\\), with the following notation:\n\\[\n\\underbracket[0pt]{\\mathrm{p}}_{\\mathrlap{\\color[RGB]{119,119,119}\\!\\uparrow\\ \\textit{lowercase}}}(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) \\coloneqq\n\\frac{\n\\overbracket[0pt]{\\mathrm{P}}^{\\mathrlap{\\color[RGB]{119,119,119}\\!\\downarrow\\ \\textit{uppercase}}}(\\textsf{\\small`\\(X\\) has value between \\(x-\\epsilon/2\\) and \\(x+\\epsilon/2\\)'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n}{\\epsilon}\n\\]\nThis definition works even if we don’t specify the exact value of \\(\\epsilon\\), as long as it’s small enough.\n\n\n\n\n\n\n Probability densities are not probabilities\n\n\n\nIf \\(X\\) is a continuous quantity, the expression “\\(\\mathrm{p}(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2.5 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})=0.3\\)” does not mean “There is a \\(0.3\\) probability that \\(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2.5\\)”. The probability that \\(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2.5\\) exactly is, if anything, zero.\nThat expression means “There is a  \\(0.3\\cdot \\epsilon\\)   probability that \\(X\\) is between \\(2.5-\\epsilon/2\\) and \\(2.5+\\epsilon/2\\), for any \\(\\epsilon\\) small enough”.\nIn fact, probability densities can be larger than 1, because they are obtained by dividing by a number, the range width, that is in principle arbitrary. This fact shows that they cannot be probabilities.\nIt is important not to mix up probability and probability densities: we shall see later that densities have very different properties, for example with respect to maxima and averages.\n\n\nA helpful practice (though followed by few texts) is to always write a probability density as\n\\[\\mathrm{p}(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\\,\\mathrm{d}x\\]\nwhere “\\(\\mathrm{d}x\\)” stands for the width of a small range around \\(x\\). This notation is also helpful with integrals. Unfortunately it becomes a little cumbersome when we are dealing with more than one quantity."
  },
  {
    "objectID": "probability_distributions.html#sec-represent-probs",
    "href": "probability_distributions.html#sec-represent-probs",
    "title": "11  Probability distributions",
    "section": "11.5 Representation of probability densities",
    "text": "11.5 Representation of probability densities\n\nLine-based representations\nThe histogram and the line representations become indistinguishable for a probability density.\nIf we represent the probability \\(P\\) assigned to a small range of width \\(\\epsilon\\) as the area of a rectangle, and the width of the rectangle is equal to \\(\\epsilon\\), then the height \\(P/\\epsilon\\) of the rectangle is numerically equal to the probability density. The difference from histograms for discrete quantities lies in the values reported on the vertical axis: for discrete quantities the values are probabilities (the areas of the rectangles), but for continuous quantities they are probability densities (the heights of the rectangles). This is also evident from the fact that the values reported on the vertical axis can be larger than 1, as in the plots on the side.\nThe rectangles, however, are so thin (usually thinner than a pixel on a screen) that they appear just as vertical lines, and together they look just like a curve delimiting a coloured area. If we don’t colour the area underneath we just have a line-based representation of the probability density.\n\n\n   As the width \\(\\epsilon\\) of the small ranges is decreased, a histogram based on these widths become indistinguishable from a line plot\n\n\nScatter plots\nLine plots of a probability density are very informative, but they can also be slightly deceiving. Try the following experiment.\nConsider a continuous quantity \\(X\\) with the following probability density:\n\nWe want to represent the amount of probability in any small range, say between \\(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}0\\) and \\(X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}0.1\\), by drawing in that range a number of short thin lines, the number being proportional to the probability. So a range with 10 lines has twice the probability of a range with 5 lines. The probability density around a value is therefore roughly represented by the density of the lines around that value.\nSuppose that we have 50 lines available to distribute this way. Where should we place them?\n\n\n\n\n\n\n\n Exercise\n\n\n\n\nWhich of these plots shows the correct placement of the 50 lines? (NB: the position of the correct answer is determined by a pseudorandom-number generator.)\n\n\n\n\n\n\n(A)\n\n\n\n\n\n\n\n(B)\n\n\n\n\n\n\n\n\n\n(C)\n\n\n\n\n\n\n\n(D)\n\n\n\n\n\n\n\n\n\nIn a scatter plot, the probability density is (approximately) represented by density of lines, or points, or similar objects, as in the examples above (only one above, though, correctly matches the density represented by the curve).\nAs the experiment and exercise above may have demonstrated, line plots sometimes give us slightly misleading ideas of how the probability is distributed across the domain; for example, peaks at some values make us overestimate the probability density around those values. Scatter plots often give a less misleading representation of the probability density.\nScatter plots are also useful for representing probability densities in more than one dimension – sometimes even in infinite dimensions! They can moreover be easier to produce computationally than line plots.\n@@ TODO Behaviour of representations under transformations of data.\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 5.3–5.3.1 of Risk Assessment and Decision Analysis with Bayesian Networks"
  },
  {
    "objectID": "probability_distributions.html#sec-combined-probs",
    "href": "probability_distributions.html#sec-combined-probs",
    "title": "11  Probability distributions",
    "section": "11.6 Combined probabilities",
    "text": "11.6 Combined probabilities\nA probability distribution is defined over a set of mutually exclusive and exhaustive sentences. In some inference problems, however, we do not need the probability of those sentences, but of some other sentence that can be obtained from them by an or operation. The probability of this sentence can then be obtained by a sum, according to the or-rule of inference. We can call this a combined probability. Let’s explain this procedure with an example.\nBack to our initial assembly-line scenario from § 1, the inference problem was to predict whether a specific component would fail within a year or not. Consider the time when the component will fail (if sold), and represent it by the quantity \\(T\\) with the following 24 different values, where “\\(\\mathrm{mo}\\)” stands for “months”:\n\\[\\begin{aligned}\n&\\textsf{\\small`The component will fail during it 1st month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 2nd month of use'}\\\\\n&\\dotsc \\\\\n&\\textsf{\\small`The component will fail during it 23rd month of use'}\\\\\n&\\textsf{\\small`The component will fail during it 24th month of use or after'}\n\\end{aligned}\\]\nwhich we can shorten to  \\(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1 \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2 \\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}24\\); note the slightly different meaning of the last value.\n\n\n\n\n\n\n Exercise\n\n\n\nWhat is the basic type of the quantity \\(T\\)? Which other characteristics does it have? for instance discrete? unbounded? rounded? uncensored?\n\n\nSuppose that the inspection device – our agent – has internally calculated a probability distribution for \\(T\\), conditional on its internal programming and the results of the tests on the component, collectively denoted \\(\\mathsfit{I}\\). The probabilities, compactly written, are\n\\[\n\\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}), \\quad\n\\dotsc, \\quad\n\\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}24 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nTheir values are stored in this csv file and plotted in the histogram on the side.\n\n\n\nWhat’s important for the agent’s decision about rejecting or accepting the component, is not the exact time when it will fail, but only whether it will fail within the first year or not. That is, the agent needs the probability of the sentence \\(\\textsf{\\small`The component will fail within a year of use'}\\). But this sentence is just the or of the first 12 sentences expressing the values of \\(T\\):\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The component will fail within a year of use'}\n\\\\&\\qquad{}\\equiv\n\\textsf{\\small`The component will fail during it 1st month of use'}\n\\lor{}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The component will fail during it 2nd month of use'}\n\\lor \\dotsb\n\\\\&\\qquad\\qquad\n\\dotsb \\lor\n\\textsf{\\small`The component will fail during it 12th month of use'}\n\\\\[1ex]&\\qquad{}\\equiv\n(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1) \\lor (T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2) \\lor \\dotsb \\lor (T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}12)\n\\end{aligned}\n\\]\nThe probability needed by the agent is therefore\n\\[\n\\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1 \\lor T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2 \\lor \\dotsb \\lor T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nwhich can be calculated using the or-rule, considering that the sentences involved are mutually exclusive:\n\\[\n\\begin{aligned}\n&\\mathrm{P}(\\textsf{\\small`The component will fail within a year of use'} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1 \\lor T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2 \\lor \\dotsb \\lor T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}12\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\\\[1ex]&\\qquad{}=\n\\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) + \\dotsb + \\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}12 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\\\[1ex]&\\qquad{}= \\sum_{t=1}^{12} \\mathrm{P}(T\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}t \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{aligned}\n\\]\n\n\n\n\n\n\nSum notation\n\n\n\nWe shall often use the \\(\\sum\\)-notation for sums, as in the example above. A notation like “\\(\\displaystyle\\sum_{i=5}^{20}\\)” means: write multiple copies of what’s written on its right side, and in each copy replace the symbol “\\(i\\)” with values from \\(5\\) to \\(20\\), in turn; then sum up these copies. The symbol “\\(i\\)” is called the index of the sum. Sometimes the initial and final values, \\(5\\) and \\(20\\) in the example, are omitted if they are understood from the context, and the sum is written simply “\\(\\displaystyle\\sum_{i}\\)”.\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nUsing your favourite programming language:\n\nLoad the csv file containing the probabilities.\nInspect this file, find the headers of its columns and so on.\nCalculate the probability that the component will fail within a year of use.\nCalculate the probability that the component will fail “within two months or use or after a year of use”."
  },
  {
    "objectID": "joint_probability.html#joint-probabilities",
    "href": "joint_probability.html#joint-probabilities",
    "title": "12  Joint probability distributions",
    "section": "12.1 Joint probabilities",
    "text": "12.1 Joint probabilities\nA joint quantity, as discussed in § 10.1, is just a collection or set of quantities of basic types. Saying that a joint quantity has a particular value means that each basic component quantity has a particular value in its specific domain. This is expressed by an and of sentences.\nConsider for instance the joint quantity \\(X\\) consisting of the age \\(\\color[RGB]{102,204,238}A\\) and sex \\(\\color[RGB]{34,136,51}S\\) of a specific person. The fact that \\(X\\) has a particular value is expressed by a composite sentence such as\n\\[\n\\textsf{\\small`The person's age is 25 years and the sex is female'}\n\\]\nwhich we can compactly write with an and:\n\\[\n{\\color[RGB]{102,204,238}A\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathrm{f}}\n\\]\nAll the possible composite sentences of this kind are mutually exclusive and exhaustive.\nAn agent’s uncertainty about \\(X\\)’s true value is therefore represented by a probability distribution over all and-ed sentences of this kind, representing all possible joint values:\n\\[\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}25\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathrm{f}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\mathrm{P}\\bigl({\\color[RGB]{102,204,238}A \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}31\\,\\mathrm{y}} \\land {\\color[RGB]{34,136,51}S\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathrm{m}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}\\bigr) \\ , \\qquad\n\\dotsc\n\\]\nwhere \\(\\mathsfit{I}\\) is the agent’s state of knowledge, and the probabilities sum up to one. We call each of these probabilities a joint probability, and their collection a joint probability distribution. Usually these probabilities are written in much abbreviated form, and a comma “\\(\\mathbin{\\mkern-0mu,\\mkern-0mu}\\)” is used instead of “\\(\\land\\)” (§  5.4); for instance you can commonly find the following notation:\n\\[\n\\mathrm{P}(A\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}25 \\mathbin{\\mkern-0mu,\\mkern-0mu}S\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nor even just\n\\[\n\\mathrm{P}(25, \\mathrm{f} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]"
  },
  {
    "objectID": "joint_probability.html#joint-probability-densities",
    "href": "joint_probability.html#joint-probability-densities",
    "title": "12  Joint probability distributions",
    "section": "12.2 Joint probability densities",
    "text": "12.2 Joint probability densities\n@@ TODO"
  },
  {
    "objectID": "joint_probability.html#representation-of-joint-probability-distributions",
    "href": "joint_probability.html#representation-of-joint-probability-distributions",
    "title": "12  Joint probability distributions",
    "section": "12.3 Representation of joint probability distributions",
    "text": "12.3 Representation of joint probability distributions\nThere is a wide variety of ways of representing joint probability distributions, and new ways are invented (and rediscovered) all the time. In some cases, especially when the quantity has more than three component quantities, it can become impossible to graphically represent the probability distribution in a faithful way. Therefore one often tries to represent only some aspects or features of interest from the full distribution. Whenever you see a plot of a joint probability distribution, you should carefully read what the plot shows and how it was made. Here we only illustrate some examples and ideas for representations.\n\nTables\nWhen a quantity is bivariate and its two component quantities are both discrete and finite, the joint probabilities can be reported as a table.\nExample: Consider the next patient that will arrive at a particular hospital. There’s the possibility of arrival by ambulance, helicopter, or other transportation means; and the possibility that the patient will need urgent non-urgent care. These can be seen as two quantities \\(A\\) (nominal) and \\(U\\) (binary). When these two quantities are taken together; their joint probability distribution is as follows, conditional on the hospital’s data \\(\\mathsfit{I}_{\\text{H}}\\):\n\n\nTable 12.1: Joint probability distribution for transportation at arrival and urgency\n\n\n\n\n\n\n\n\n\n\n\\(\\mathrm{P}(A\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}a \\mathbin{\\mkern-0mu,\\mkern-0mu}U\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}u\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}_{\\text{H}})\\)| |arrival \\(A\\)\n\n\n\n\nambulance\nhelicopter\nother\n\n\n\nurgency \\(U\\)\nurgent\n0.11\n0.04\n0.03\n\n\n\nnon-urgent\n0.17\n0.01\n0.64\n\n\n\n\n\nWe see for instance that the most probable possibility is that the next patient will arrive by transportation means other than ambulance and helicopter, and won’t require urgent care.\nIt is also possible to replace the numerical probability values with graphical representations; for example as shades of a colour, or squares with different areas.\n@@ TODO ref to marginals section\n\n\nMulti-line plots\nJoint probability distributions over one discrete and one continuous quantity (or ordinal discrete with numerous values) can be represented as a collection of line plots: each line plot represent the probability density for the continuous quantity and a specific value of the discrete quantity.\nConsider for instance the probability that the next patient who arrives at a particular hospital has a given age (continuous quantity) and may require urgent care or not (binary quantity). The joint probability density can be visualized as in the plot in the margin. From the plot we can see that the probability of urgent patients is generally lower than non-urgent ones. A possibly disadvantage of this kind of plots is evident: the details, such as peaks, of the density for some values of the discrete quantity may be barely visible.\n\n\n\n\n\nSurface plots\n\n\nScatter plots\n@@ TODO work also for 3D\n\n\n\n\n\n\n Study reading\n\n\n\n\n§§ 5.3.2–5.3.3 of Risk Assessment and Decision Analysis with Bayesian Networks\n§ 12.2.2 of Artificial Intelligence\n§§ 5.1–5.5 of Probability"
  },
  {
    "objectID": "marginal_conditional_probability.html#sec-marginal-probs",
    "href": "marginal_conditional_probability.html#sec-marginal-probs",
    "title": "13  Marginal and conditional probabilities",
    "section": "13.1 Marginal probability distributions",
    "text": "13.1 Marginal probability distributions\nIn some situations an agent has the joint probability distribution for a joint quantity, but it needs the probability for one of the component quantities only, irrespective of what the values for the other components might be.\nConsider for instance the joint probability for the next-patient arrival scenario of table 12.1. We may be interested in the probability that the next patient will need urgent care, independently of how the patient gets to the hospital. This probability can be found, as usual, by analysing the problem in terms of sentences and using the basic rules of inference from § 7.4.\nThe sentence of interest is\n\\[\n\\textsf{\\small`The next patient will require urgent care'}\n\\]\nwhich is equivalent to\n\\[\n\\textsf{\\small`The next patient will require urgent care, and will arrive by ambulance, helicopter, or other means'}\n\\]\nThis last sentence can be written in terms of and and or connectives:\n\\[\n\\begin{aligned}\n&\\textsf{\\small`The next patient will require urgent care'} \\land{}\n\\\\[1ex]\n&\\qquad\\bigl(\\textsf{\\small`The next patient will arrive by ambulance'} \\lor {}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The next patient will arrive by helicopter'} \\lor {}\n\\\\&\\qquad\\qquad\n\\textsf{\\small`The next patient will arrive by other means'}\\bigr)\n\\end{aligned}\n\\]\nwhich we can compactly write as\n\\[\n\\begin{aligned}\n&\\textsf{\\small urgent}\\land\n(\\textsf{\\small ambulance} \\lor \\textsf{\\small helicopter} \\lor \\textsf{\\small other})\n\\\\[1ex]\n&\\qquad{}=\n(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small helicopter}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small other})\n\\end{aligned}\n\\]\nwhere the second, equivalent form comes from the derived rules of Boolean algebra of § 7.7.1.\nThe last sentence is an or of mutually exclusive sentences. Its probability is therefore given by the or rule:\n\\[\n\\begin{aligned}\n&\\mathrm{P}\\bigl[\n(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small helicopter}) \\lor\n(\\textsf{\\small urgent}\\land \\textsf{\\small other})\n\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} \\bigr]\n\\\\[1ex]\n&\\qquad{}=\n\\mathrm{P}(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} ) +\n\\mathrm{P}(\\textsf{\\small urgent}\\land \\textsf{\\small ambulance}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} ) +\n\\mathrm{P}(\\textsf{\\small urgent}\\land \\textsf{\\small other}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}} )\n\\end{aligned}\n\\]\nThe probability for a value of the urgency quantity, independently of the value of the arrival-means quantity, can be found by summing all joint probabilities with all possible arrival-means values. Using the \\(\\sum\\)-notation we can write \n\\[\n\\mathrm{P}(\\textsf{\\small urgent} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}}) =\n\\sum_{\\mathclap{a=\\textsf{\\small ambulance}}}^{\\mathclap{\\textsf{\\small other}}}\n\\mathrm{P}(\\textsf{\\small urgent}, A\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}a \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}_{\\text{H}})\n\\]\nThis is called a marginal probability.\n\n\n\n\n\n\n Exercise\n\n\n\nUsing the values from table 12.1, calculate:\n\nthe marginal probability that the next patient will need urgent care\nthe marginal probability that the next patient will arrive by helicopter\n\n\n\nConsidering now a more abstract case for a bivariate quantity with component quantities \\(X\\) and \\(Y\\), the probability for a specific value of \\(X\\), conditional on some information \\(\\mathsfit{I}\\) and irrespective of what the value of \\(Y\\) might be, would be given by\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\]\nYou may notice the similarity with the expression for a combined probability from § 11.6. Indeed a marginal probability is just a special case of a combined probability: we are combining all probabilities that exhaust the possibilities for the quantity \\(Y\\).\n\n\n\n\n\n\n Exercise: test your understanding\n\n\n\nUsing again the values from table 12.1, calculate the probability that the next patient will need urgent care and will arrive either by ambulance or by helicopter."
  },
  {
    "objectID": "marginal_conditional_probability.html#sec-conditional-probs",
    "href": "marginal_conditional_probability.html#sec-conditional-probs",
    "title": "13  Marginal and conditional probabilities",
    "section": "13.2 Conditional probability distributions",
    "text": "13.2 Conditional probability distributions\nA joint probability distribution quantifies an agent’s uncertainty about all the quantities that compose a joint quantity. This means that in general the agent has no sure certainty about any of them (there are of course special cases where probabilities can be \\(0\\) or \\(1\\), but they are not the general case). Suppose that the agent has a joint probability for the quantities \\(X\\) and \\(Y\\), conditional on a state of knowledge \\(\\mathsfit{I}\\):\n\\[ \\mathrm{P}({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I}) \\]\nNow consider these two situations:\n\nthe agent acquires knowledge that \\(X\\) has true value \\(x\\), so its state of knowledge has changed;\nfor inference purposes, the agents needs to hypothetically assume that the quantity \\(X\\) has value \\(x\\) (even if that might not be the case in reality);\n\nand, in either situation, the agent needs the probability that \\(Y\\) has value \\(y\\). This probability is written, in either situation as\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\]\nIt is usually called a conditional probability. It is somewhat a misnomer, because all probabilities are conditional on something. The implicit understanding here is that new information has been added to the conditional with respect to some base state of knowledge.\nWhat is the numerical value of the conditional probability above? We simply use the and-rule:\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) =\n\\frac{\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})}{\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})}\n\\]\nThe denominator is the marginal probability for \\(X\\), which can also be calculated from the joint distribution as discussed in § 13.1:\n\\[\n\\mathrm{P}({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I}) = \\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})\n\\]\nWe can thus rewrite the conditional probability as\n\\[\n\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) =\n\\frac{\\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})}{\\sum_{\\color[RGB]{238,102,119}y} \\mathrm{P}({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\mathsfit{I})}\n\\]\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the next-patient problem with the joint probability of table 12.1:\n\nCalculate the probability that the next patient needs urgent care, knowing that the patient will arrive by helicopter.\nCompare the conditional probability you just found with the marginal probability that the next patient needs urgent care, independently of transportation means (calculated in a previous exercise). Which is higher? Why?\nCalculate the probability that the next patient arrives by helicopter, knowing that the patient will need urgent care.\nCompare the conditional probability from 3. with that from 1. Why are they so different? Why is the probability “\\(\\textsf{\\small helicopter}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\textsf{\\small urgent}\\)” so low, when “\\(\\textsf{\\small urgent}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\\textsf{\\small helicopter}\\)” is so high instead? find an intuitive explanation."
  },
  {
    "objectID": "marginal_conditional_probability.html#the-importance-of-marginal-distributions-for-drawing-inferences",
    "href": "marginal_conditional_probability.html#the-importance-of-marginal-distributions-for-drawing-inferences",
    "title": "13  Marginal and conditional probabilities",
    "section": "13.3 The importance of marginal distributions for drawing inferences",
    "text": "13.3 The importance of marginal distributions for drawing inferences\n@@ TODO"
  },
  {
    "objectID": "2nd_connection_ML.html",
    "href": "2nd_connection_ML.html",
    "title": "14  A second connection with machine learning",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]\n\n\n\n\nIn chapter  8 we made a first tentative connection between the notions about probability explored until then, and notions from machine learning. We considered the possibility that a machine-learning algorithm is like an agent that has some built-in background information (corresponding to the algorithm’s architecture), has received pieces of information (corresponding to the data about perfectly known instances of the task), and is assessing a not-previously known piece of information (the outcome in a new task instance):\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}\\mathsfit{D}_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{\\mathsfit{D}_N \\land \\dotsb \\land \\mathsfit{D}_2 \\land \\mathsfit{D}_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\land \\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nThe correspondence about training data and architecture seems somewhat convincing, the one about outcome will need more exploration, because it seems to involve some decision process – and we haven’t fully explored the machinery of decision-making yet.\nHaving introduced the notion of quantity in the latest chapters 9 and 10, we recognize that training data about a task instance concern some quantity and its value, so they can be expressed by a sentence like \\(D_i\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_i\\), where\n\n\\(i\\) is the instance: \\(1,2,\\dotsc,N\\)\n\\(D_i\\) describes the kind of data at instance \\(i\\), for example “128 × 128 image with 24-bit colour depth”\n\\(d_i\\) is the value of the data at instance \\(i\\), for example the one here at the margin\n\n\n\n\nAnd similarly for the outcome of a new task instance where the algorithm is applied for real, which we consider as instance \\(N+1\\). So we can rewrite the correspondence above as follows:\n\\[\n\\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}D_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n\\color[RGB]{34,136,51}\\underbracket[0ex]{ D_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}D_2 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_2 \\mathbin{\\mkern-0mu,\\mkern-0mu}D_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n\\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n\\]\nLet’s extend this tentative connection even further.\n\n\nMachine-learning textbooks usually make a distinction between “supervised learning” and “unsupervised learning”. Unfortunately the explanation given for this distinction is sometimes misleading:\n\n  Some books say that in supervised learning the algorithm “learns a functional relationship between some kind of input and some kind of output”. This is usually not true: in the vast majority of applications there isn’t any functional relationship between input and output at all; at most only a statistical or probabilistic one. This is clear from the fact that two training datapoints can have identical inputs but different outputs (here is an example); and you remember from Calculus I that we can’t speak of a function in this case. It’s unclear how something that doesn’t exist can be learned.\nBooks that give this kind of explanation are unfortunately oversimplifying things, to the point of being incorrect1. The algorithm is actually doing something more complex – which we shall analyse in detail later.\n  Yet other books say that the distinction rests in the kind of data used for training: “input-output” pairs for supervised learning, and only “inputs” for unsupervised learning. It’s good that this description doesn’t mention “functions”, but it is still unsatisfactory, because it confuses the means with the purpose. It’s a little like saying that the difference between car and aeroplane is that the latter has wings. Sure – but why? This description misses the essential difference between these two means of transportation: they operate through different material media and exploit different kinds of physics; that’s why the second has wings.\nBooks that give this kind of explanation focus on a more “operational” kind of knowledge, which is sufficient for their specific goals. The very terms “supervised learning” and “unsupervised learning” indeed emphasize this operational side. But for our goal we need to look beyond operational differences and to understand their reasons.\n  More enlightening books explain that the distinction rests in what the algorithm needs for each new application: in supervised learning, it uses features – that is, additional information – available at each new application instance; whereas in unsupervised learning it doesn’t: no new instance-dependent information is given.\nFrom this point of view, we also see that the distinction between “supervised” and “unsupervised” becomes less sharp: we can imagine to increase the information that’s used at each new instance from zero (“unsupervised”) to larger and larger amounts (“supervised”).\n\n1 Paraphrasing M. W. Zemansky:\n“Teaching machine learning\nIs as easy as a song:\nYou think you make it simpler\nWhen you make it slightly wrong!”Going back to our tentative correspondence with inference and decision-making agents, we see a strong similarity between unsupervised & supervised learning and two kinds of inference:\n\nIn the unsupervised case, even if the quantities \\({\\color[RGB]{34,136,51}D_1}, {\\color[RGB]{34,136,51}D_2}, {\\color[RGB]{34,136,51}\\dotsc}, {\\color[RGB]{34,136,51}D_N}\\) in the known instances and in the new instance \\({\\color[RGB]{238,102,119}D_{N+1}}\\) might consist of joint or complex quantities (chapter  10), we are not interested in their possible decomposition into component quantities. So we still have the tentative connection above:\n\\[\n  \\mathrm{P}(\\underbracket[0ex]{\\color[RGB]{238,102,119}D_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_{N+1}}_{\\mathclap{\\color[RGB]{238,102,119}\\text{outcome?}}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{}\n  \\color[RGB]{34,136,51}\\underbracket[0ex]{ D_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsb  \\mathbin{\\mkern-0mu,\\mkern-0mu}D_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_1}_{\\mathclap{\\color[RGB]{34,136,51}\\text{training data?}}}\n  \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\underbracket[0ex]{\\color[RGB]{204,187,68}\\mathsfit{I}}_{\\mathrlap{\\color[RGB]{204,187,68}\\uparrow\\ \\text{architecture?}}})\n  \\]\nFor example, the agent has been given information about a collection of images, and then tries to guess what the next image could be.\nIn the supervised case, the quantities in the known instances and in the new application are joint quantities:\n\\[\n  \\begin{gathered}\n  {\\color[RGB]{34,136,51}D_N} = ({\\color[RGB]{34,136,51}Y_N}, {\\color[RGB]{34,136,51}X_N}),\n  \\ {\\color[RGB]{34,136,51}\\dotsc},\\\n  {\\color[RGB]{34,136,51}D_1} = ({\\color[RGB]{34,136,51}Y_1}, {\\color[RGB]{34,136,51}X_1})\n  \\\\[1ex]\n  {\\color[RGB]{238,102,119}D_{N+1}} = ({\\color[RGB]{238,102,119}Y_{N+1}}, {\\color[RGB]{238,102,119}X_{N+1}})\n  \\end{gathered}\n  \\]\nand we are interested in the \\(X\\) and \\(Y\\) component quantities separately. For instance, the \\(X\\)-quantity could be an image, as in the example above, and the \\(Y\\)-quantity could be a label with domain \\(\\set{\\texttt{\\small Muppet}, \\texttt{\\small non-Muppet}}\\). The reason we make this separation is that, upon applying the algorithm in a new task instance, one of these component quantities, say \\(\\color[RGB]{238,102,119}X_{N+1}\\), can actually be observed by the agent; so it is known. It’s the other component quantity, \\(\\color[RGB]{238,102,119}Y_{N+1}\\), that the agent is uncertain about. The agent therefore needs to draw the following inference:\n\\[\n  \\mathrm{P}\\bigl(\n  {\\color[RGB]{238,102,119}Y_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_{N+1}}\n  \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n  {\\color[RGB]{238,102,119}X_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_{N+1}}\\, \\mathbin{\\mkern-0mu,\\mkern-0mu}\\,\n  \\color[RGB]{34,136,51}Y_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n  \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n  Y_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_1\n  \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n  \\]\n\nAn interesting aspect of this new tentative correspondence is that there isn’t any essential difference between unsupervised and supervised cases. Consider the last probability above; using the formula for a conditional probability (§  13.2) we have\n\n\\[\n\\begin{aligned}\n    &\\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_{N+1}}\n    \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_{N+1}}\\, \\mathbin{\\mkern-0mu,\\mkern-0mu}\\,\n    \\color[RGB]{34,136,51}Y_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n    \\\\[2ex]\n    &\\qquad{}=\n    \\frac{\n        \\mathrm{P}\\bigl(\n    \\color[RGB]{187,187,187}\\overbracket[1px]{\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_{N+1}} \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_{N+1}}\n    }^\\mathclap{{\\color[RGB]{238,102,119}D_{N+1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_{N+1}}}\n    \\color[RGB]{0,0,0}\n        \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n        \\color[RGB]{187,187,187}\\overbracket[1px]{\n    \\color[RGB]{34,136,51}Y_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_N\n        }^\\mathclap{{\\color[RGB]{34,136,51}D_{N}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_{N}}}\\color[RGB]{34,136,51}\n    \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n        \\color[RGB]{187,187,187}\\overbracket[1px]{\n    \\color[RGB]{34,136,51}Y_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_1\n        }^\\mathclap{{\\color[RGB]{34,136,51}D_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}d_{1}}}\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n}{\n     \\sum_{\\color[RGB]{238,102,119}y_{N+1}} \\mathrm{P}\\bigl(\n    {\\color[RGB]{238,102,119}Y_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_{N+1}} \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    {\\color[RGB]{238,102,119}X_{N+1} \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_{N+1}}\n        \\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n    \\color[RGB]{34,136,51}Y_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_N \\mathbin{\\mkern-0mu,\\mkern-0mu}X_N \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_N \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    \\dotsb \\mathbin{\\mkern-0mu,\\mkern-0mu}\n    Y_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y_1 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_1 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x_1\n    \\color[RGB]{0,0,0}\\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{204,187,68}\\mathsfit{I}} \\bigr)\n}\n\\end{aligned}\n\\]\n\nThis is a fraction between the probability for the “unsupervised case” (which we wrote above compactly in terms of \\(D\\) rather than \\((Y,X)\\)), and the same probability summed up for all possible values of \\(\\color[RGB]{238,102,119}Y_{N+1}\\), which in the image-example above would be a sum over \\(\\color[RGB]{238,102,119}Y_{N+1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small Muppet}\\) and \\(\\color[RGB]{238,102,119}Y_{N+1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small non-Muppet}\\).\nSo if the agent has the probability for the “unsupervised case”, then with a simple computation it also has the probability for the “supervised case”.\n\n\nOne important question arise: how are these probabilities calculated by the agent? This is what we discuss in the next chapters."
  },
  {
    "objectID": "populations_variates.html#sec-collections",
    "href": "populations_variates.html#sec-collections",
    "title": "15  Populations and variates",
    "section": "15.1 Collections of similar quantities",
    "text": "15.1 Collections of similar quantities\nIn engineering and data science, we often face inference problems involving not just one quantity (simple, joint, or complex), but collections of quantities. And in many problems all quantities in the collection have the same domain, so they can somehow be considered as “instances” of the same kind of quantity. Consider the following examples:\n\n\nStock exchange\n\nWe are interested in the daily change in closing price of a stock, during 100 days. Each day the change can be positive (or zero), or negative.\n\n\nThe daily change on any day can clearly be considered as a binary quantity, say with domain \\(\\set{+, -}\\). The daily changes in 100 days are a set of 100 binary quantities with exactly the same domain – even if each one can have a different value.\n\n\n\n\n\n\n\n\nMars prospecting\n\nSome robot examines 100 similar-sized rocks in a large crater on Mars. Each rock either contains hematite, or it doesn’t.\n\n\nThe hematite-content of any rock can be considered as a binary quantity, say with domain \\(\\set{\\texttt{\\small Y}, \\texttt{\\small N}}\\). The hematite contents of the 100 rocks are again a set of 100 binary quantities with exactly the same domain – even if each one can have a different value.\n\n\n\n\nIt is easy to think of many other and very diverse examples that are similar to the two above. We shall now try to abstract and generalize this similarity."
  },
  {
    "objectID": "populations_variates.html#sec-variates-populations",
    "href": "populations_variates.html#sec-variates-populations",
    "title": "15  Populations and variates",
    "section": "15.2 Units, variates, statistical populations",
    "text": "15.2 Units, variates, statistical populations\nConsider a large collection of entities that are somehow similar to one another. We call these entities units. These units could be, for instance:\n\nphysical objects such as cars, windmills, planets, or rocks from a particular place;\ncreatures such as animals of a particular species, or human beings, maybe with something in common such as geographical region; or plants of a particular kind;\nautomatons having a particular application;\nsoftware objects such as photos;\nabstract objects such as functions or graphs;\nthe rolls of a particular die or the tosses of a particular coin;\nthe weather conditions in several days.\n\nThese units are similar to one another in that they have some attribute1 common to all. This attribute can present itself in a specific number of mutually-exclusive guises, which can be different from unit to unit. For instance, the attribute could be:1 The term feature is used in machine learning\n\n“colour”, each unit being, say, green, blue, or yellow;\n“mass”, each unit having a mass between \\(0.1\\,\\mathrm{kg}\\) and \\(10\\,\\mathrm{kg}\\);\n“health condition”, each unit (an animal or human in this case) being healthy or ill; or maybe being affected by one of a specific set of diseases;\ncontaining something, for instance a particular chemical substance;\n“having a label”, each unit having one of the labels A, B, C;\na complex combination of several simpler attributes like the ones above.\n\nThe units also have additional attributes (they must, otherwise we wouldn’t be able to distinguish each unit from all others), which we simply don’t consider or can’t measure. We’ll discuss this possibility later.\nFrom this description it’s clear that the attribute of each unit is a quantity, as defined in § 9.1.1. Once the units and their attribute are specified, we have a set of as many quantities as there are units. All these quantities have identical domains.\nA quantity which is an “attribute” of a set of units is called a variate. So when we speak about a variate it is understood that this is a quantity that appears, replicated, in some set of units.\n\n\nWe call a collection of units so defined a statistical population, or just population when there’s no ambiguity. The number of units is called the size of the population.\nThe notion of statistical population is extremely general and encompassing; so many different things can be thought of as a population. In speaking of “data”, what is often meant is a particular statistical population. The specification of a population requires precision, especially when it is used to draw inferences, as we shall see later. A statistical population has not truly been specified until two things are precisely given:\n\na way to determine whether something is a unit or not: inclusion and exclusion criteria, means of collection, and so on\na definition of the variate considered, its possible values, and how it is measured\n\n\n\n\n\n\n\n Exercises\n\n\n\n\nWhich of the following descriptions does define a statistical population? explain why it does or does not.\n\nPeople.\nElectronic components produced in a specific assembly line, since the line became operational until its discontinuation, and measured for their electric resistance, with possible values in \\([0\\,\\mathrm{\\Omega}, \\infty\\,\\mathrm{\\Omega}]\\), and result of a shock test, with possible values \\(\\set{\\texttt{\\small pass}, \\texttt{\\small fail}}\\).\nPeople born in Norway between 1st January 1990 and 31st December 2010.\nThe words contained on all websites of the internet.\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater (as defined by contours on a map), and tested to contain hematite, with possible values \\(\\set{\\texttt{\\small Y}, \\texttt{\\small N}}\\).\n\nBrowse some datasets at the UC Irvine Machine Learning repository. Each dataset is a statistical population. The variate in most of these populations is a joint variate (to be discussed soon), that is, a collection of several variates.\nExamine and discuss the specification of some of those datasets:\n\nIs it well-specified what constitutes a “unit”? Are the criteria for including or excluding datapoints, their origin, and so on, well explained?\nAre the variates well-defined? Is it explained what they mean, how they were measured, what is their domain, and so on?\n\n\n\n\n\n\n\n\n\n\n\n\n Subtleties in the notion of statistical population\n\n\n\n\nA statistical population is only a conceptual device to simplify and face some decision or inference problem. There is no objectively-defined population “out there”.\nAny entity, object, person, and so on has characteristic that makes it completely unique (say, its space-time coordinates), otherwise we wouldn’t be able to distinguish it from others. From this point of view any entity is just be a one-member population in itself. If we consider two or more entities as being “similar” and belonging to the same population, it’s because we have decided to disregard some characteristics and focus on some others. This decision is arbitrary, a matter of convention, and depends on the specific inference and decision problem.\nTo “test” whether an entity belong to a given population, means to check if that entity satisfies the agreed-upon definition of that population.\nAny physical entity, object, person, etc. can represent different units in different and even non-overlapping statistical populations. For instance, a 100 cm³ rock found in the Schiaparelli crater on Mars could be a unit in these populations:\n\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and tested for hematite\nRocks, of volume between 10 cm³ and 200 cm³, found in the Schiaparelli crater and tested for hematite\nRocks, of volume between 10 cm³ and 200 m³, found in any crater on any planet of the solar system, and tested for hematite\nRocks, of volume between 1 cm³ and 1 m³, found in the Schiaparelli crater and measured for the magnitude of their magnetic field.\n\nPopulations a. b. c. above have the same variate but differ in their definition of “unit”. Populations a. and d. have the same definition of unit but different variates. Population b. is a subset of population a.: they have the same variate, and any unit in b. is also a unit in a. Populations a. and c. have some overlap: they have the same variate, and some units of a. are also units of c., and vice versa."
  },
  {
    "objectID": "populations_variates.html#populations-with-joint-variates",
    "href": "populations_variates.html#populations-with-joint-variates",
    "title": "15  Populations and variates",
    "section": "15.3 Populations with joint variates",
    "text": "15.3 Populations with joint variates\nThe definition of statistical population (§  15.2) makes it clear that the quantity associated with each unit can be of arbitrary complexity. In particular it could be a joint quantity (§  10.1), that is, a collection of quantities of a simpler type.\nAs an example, consider the following population, relevant for glass forensics:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{RI}, \\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of four simple variates:\n\n\\(\\mathit{R}\\)efractive \\(\\mathit{I}\\)ndex of the glass fragment (interval continuous variate), with domain from \\(1\\) (included) to \\(+\\infty\\)\nweight percent of \\(\\mathit{Ca}\\)lcium in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\nweight percent of \\(\\mathit{Si}\\)licon in the fragment (interval discrete variate), with domain from \\(0\\) to \\(100\\) in steps of 0.01\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values building_windows_float_processed, building_windows_non_float_processed, vehicle_windows_float_processed, vehicle_windows_non_float_processed, containers, tableware, headlamps\n\n\n\n\nTable 15.1: Glass fragments\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\ntableware\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\nheadlamps\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\nbuilding_windows_non_float_processed\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\nheadlamps\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\nbuilding_windows_float_processed\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\nbuilding_windows_non_float_processed\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\nvehicle_windows_float_processed\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\nbuilding_windows_non_float_processed\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\ncontainers\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\nheadlamps\n\n\n\n\nThe joint-variate value for unit 4, for instance, is\n\\[\n\\mathit{RI}_{\\color[RGB]{204,187,68}4}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1.52247 \\land\n\\mathit{Ca}_{\\color[RGB]{204,187,68}4}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}9.76 \\land\n\\mathit{Si}_{\\color[RGB]{204,187,68}4}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}70.26 \\land\n\\mathit{Type}_{\\color[RGB]{204,187,68}4}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}\n\\]\n\n\n\n\n\n\n\n\n\n\nThe units’ IDs don’t need to be consecutive numbers; in fact they don’t even need to be numbers: any label that completely distinguishes all units will do.\n\n\n\nJoint frequencies\nThe frequency distribution for the joint variate of the population above gives the frequencies of all possible combinations of variate values. In this population, ten joint values appear each only once, and the remaining values never appear; this is because of the population’s small size and the large number of possible variate values. Denoting by \\(\\boldsymbol{f}\\) the joint frequency distribution, we have for example22 For the continuous variate \\(\\mathit{RI}\\), the frequency refers to a small range around the value; see the discussion in (§ Section 11.4). In the present case the range seems to be \\(\\pm 0.000 01\\)\n\\[\\begin{aligned}\n&f(\\mathit{RI}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1.51651  \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}9.76 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Si}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}73.61  \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}\n) = \\frac{1}{10}\n\\\\\n&f(\\mathit{RI}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1.0  \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}50.00  \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Si}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}50.00  \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}\n) = 0\n\\end{aligned}\\]"
  },
  {
    "objectID": "populations_variates.html#marginal-populations",
    "href": "populations_variates.html#marginal-populations",
    "title": "15  Populations and variates",
    "section": "15.4 Marginal populations",
    "text": "15.4 Marginal populations\nWhen a population has a joint variate, we may be interested in only a subset of the simpler variates that constitute the joint one. In the glass-forensics population above, for instance, we might be interested only in the \\(\\mathit{Ca}\\)lcium and \\(\\mathit{Type}\\) variates. These two variates together are called marginal variates and define what we can call a marginal population of the original one. A marginal population has the same units as the original one, but only a subset of the variates of the original. It is a statistical population in its own right.\nThe notion of “marginalization” is a relative notion. Any population can often be considered as the marginal of a population with the same units but additional attributes."
  },
  {
    "objectID": "populations_variates.html#marginal-frequencies",
    "href": "populations_variates.html#marginal-frequencies",
    "title": "15  Populations and variates",
    "section": "15.5 Marginal frequencies",
    "text": "15.5 Marginal frequencies\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\), we define the marginal frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\) as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the marginal population with the variate \\({\\color[RGB]{238,102,119}Y}\\) alone. This frequency is simply written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y})\n\\]\nA conditional frequency can be calculated as the sum of the joint frequencies for all values \\({\\color[RGB]{34,136,51}x}\\), in a way analogous to marginal probabilities (§  13.1):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y}) = \\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x})\n\\]\nFor example, if from the population of table  15.1 we consider the marginal population with variates \\((\\mathit{Ca}, \\mathit{Type})\\) marginal population, we find the marginal frequencies\n\\[\\begin{aligned}\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}9.76 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}\n) = \\frac{2}{10}\n\\\\\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}10.09 \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small containers}\n) = \\frac{1}{10}\n\\\\\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}50.00  \\mathbin{\\mkern-0mu,\\mkern-0mu}\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}\n) = 0\n\\end{aligned}\\]"
  },
  {
    "objectID": "statistics.html#the-difference-between-probability-theory-and-statistics",
    "href": "statistics.html#the-difference-between-probability-theory-and-statistics",
    "title": "16  Statistics",
    "section": "16.1 The difference between Probability Theory and Statistics",
    "text": "16.1 The difference between Probability Theory and Statistics\n“Probability theory” and “statistics” are often mentioned together. We shall soon see why and what are the relationship between them. But first let’s try to define them more precisely:\n\nProbability theory\n\nis the theory that describes and norms the quantification and propagation of uncertainty, as we saw in § 7.1.\n\nStatistics\n\nis the study of collective properties of the variates of populations or, more generally, of collections of data.\n\n\nThere are clear and crucial differences between the two:\n\nThe fact that uncertain about something doesn’t mean that there are populations or replicas involved. We can apply probability theory without doing any statistics.\nIf we have full information about a population – the value of the variate for each unit – then we can calculate summaries and other properties of the variate. And there’s no uncertainty involved: at all times we can exactly calculate any information we like about the variate. So we do statistics but probability theory plays no role (except perhaps in the form of propositional logic).\n\nMany texts do not clearly distinguish between probability and statistics. The distinction is important for us because we will have to solve problems involving the uncertainty about particular statistics, so the two must be kept clearly separate. This distinction was observed by James Clerk Maxwell who used it to develop the theories of statistical mechanics and kinetic theory.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nMaxwell explains the statistical method and its use in the molecular description of matter:\n\nIntroductory Lecture on Experimental Physics\nMolecules\n\n\n\nIn many concrete problems, however, these do theories do go hand in hand and interact. This happens mainly in two non-mutually exclusive ways:\n\nthe statistics of a population give information that can be used in the conditional of an inference\nwe want to draw inferences about some statistics of a population, whose values we don’t know.\n\n\n\nLet’s now discuss some important statistics."
  },
  {
    "objectID": "statistics.html#sec-freq-distribs",
    "href": "statistics.html#sec-freq-distribs",
    "title": "16  Statistics",
    "section": "16.2 Frequency distributions",
    "text": "16.2 Frequency distributions\nConsider a statistical population of \\(N\\) units, with a variate \\(X\\) having a finite set of \\(K\\) values as domain. To keep things simple let’s just say these values are \\(\\set{1, 2, \\dotsc, K}\\) (without any ordering implied); the discussion applies for any finite set. The variate \\(X\\) could be of any non-continuous type: nominal, ordinal, interval, binary (§  9.2), or of a joint or complex type (§  10). Let’s denote the variate associated with unit \\(i\\) with \\(X_i\\). For instance, we express that unit #3 has variate value \\(5\\) and unit #7 has variate value \\(1\\) by writing\n\\[\nX_3 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}5 \\land X_7 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1\n\\quad\\text{\\small or more compactly}\\quad\nX_3 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}5 \\mathbin{\\mkern-0mu,\\mkern-0mu}X_7 \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}1\n\\]\nFor each value \\(a\\) in the domain of the variate, we count how many units in the population have that particular value, let’s call the number we find \\(n_a\\). This is the absolute frequency of the value \\(a\\) in this population. Obviously \\(n_a\\) must be an integer between \\(0\\) (included) and \\(K\\) (included). The set of absolute frequencies of all values is called the absolute frequency distribution of the values (or of the variate) in the population. We must have\n\\[\\sum_{a=1}^K n_a = N \\ .\\]\nIt is often useful to give not the absolute count of the times the value \\(a\\) appears in a population, but the fraction with respect to the population size, which we denote by \\(f_a\\):\n\\[f_a \\coloneqq n_a/N\\]\nThis is called the relative frequency of the value \\(a\\). Obviously \\(0 \\le f_a \\le 1\\). The collection of relative frequencies for all values, \\(\\set{f_1, f_2, \\dotsc, f_K}\\), satisfies\n\\[\\sum_{a=1}^K f_a = 1 \\ .\\]\nWe call this collection the relative frequency distribution. We shall denote it with the boldface symbol \\(\\boldsymbol{f}\\) (boldface indicates that it is a tuple of numbers):\n\\(\\boldsymbol{f} \\coloneqq(f_1, f_2, \\dotsc, f_K)\\)\nwith an analogous convention if other letters are used instead of “\\(f\\)”.\nIn the following we shall call relative frequencies simply “frequencies”.\n\n\nThe frequency distribution of values in a population does not give us full information about the latter, because it doesn’t tell which unit has which value. In many situations, however, this is all we need to know.\nFrequencies and frequency distributions are quantities in the technical sense of §  9.1.1. In fact we can say, for instance, “The frequency of the value \\(7\\) is 0.3”, or “The frequency distribution for the values \\(1,2,3\\) is \\((0.2, 0.7, 0.1)\\)”. We shall denote the quantity, as separate from its value, by the corresponding capital letter, for example \\(F_1\\), so that we can write sentences about frequencies in our usual abbreviated form. For instance\n\\[\nF_3\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}f_3\n\\]\nmeans “The frequency of the variate value \\(3\\) is equal to \\(f_3\\)”, where \\(f_3\\) must be a specific number.\n\n\n\n\n\n\n Exercise\n\n\n\nA statistical population is defined as follows:\n\nunits: the bookings at a specific hotel during a specific time period\nvariate: the market segment of the booking\nvariate domain: the set of five values \\(\\set{\\texttt{\\small Offline},\\  \\texttt{\\small Online},\\  \\texttt{\\small Corporate},\\  \\texttt{\\small Aviation},\\  \\texttt{\\small Complementary},\\  \\texttt{\\small Complementary}}\\)\n\nThe population data is stored in this csv file. Each row of the file corresponds to a unit, and lists the unit id (this is not a variate in the present population) and the market segment.\nUse any method you like (a script in your favourite programming language, counting by hand, or whatever) to answer these questions:\n\nHow many units are in the population?\nWhat are the absolute frequencies of the five values?\nWhat are their relative frequencies?\nWhich units have the value \\(\\texttt{\\small Corporate}\\)?\n\n\n\n\nDifferences between frequencies and probabilities\nThe fact that frequencies are non-negative and sum up to 1 makes them somewhat similar to probabilities from a purely numerical point of view. The two notions, however, are completely different and have different uses. Here is a list of some important differences:\n\n\nNot few works in machine learning tend to call “probabilities” any set of positive numbers that sum up to one. Be careful when reading them. Mentally replace probability with degree of belief and see if the text mentioning “probabilities” still makes sense.\n\n\nA probability expresses a degree of belief.\nA frequency is just the count of how many times something appears.\n\n\nThe probability of a sentence depends on an agent’s state of knowledge and background information. Two agents can assign different probabilities to the same sentence.\nThe frequency of a value in a population is an objective physical quantity. All agents agree on the frequency (if they know it).\n\n\n\n\nProbabilities refer to sentences.\nFrequencies refer to values in a population, not to sentences (unless we are speaking of how many times a sentence appears in, say, a book; but this is a completely different and peculiar case.)\n\n\nA probability can refer to a specific unit in a population. An agent can consider, for instance, the probability that the variate for unit #7 has value 3.\nA frequency cannot refer to a specific unit in a population. It is somewhat meaningless to “count how many times the value 3 appears in unit #7”.\n\n\n\n\nLimit frequencies\nIn discussing statistical populations we said (§ 18.1) that we shall often focus on practically infinite populations, that is, populations whose number of units is much larger than the number which will be used as data or on which inferences will be drawn.\nRelative frequencies are ratios of two integers, the denominator being the population size \\(N\\). So a frequency \\(f\\) can only take on \\(N+1\\) rational values \\(0/N, \\dotsc, N/N\\) between \\(0\\) and \\(1\\). As the population size increases, the number of distinct, possible frequencies increases and eventually can be considered practically continuous. Frequencies in this case are sometimes called limit frequencies and they are treated as real numbers between \\(0\\) and \\(1\\)."
  },
  {
    "objectID": "statistics.html#quantiles",
    "href": "statistics.html#quantiles",
    "title": "16  Statistics",
    "section": "16.3 Quantiles",
    "text": "16.3 Quantiles\n@@ TODO\n\nMedian and interquartile range\n@@ TODO"
  },
  {
    "objectID": "statistics.html#mean-and-standard-deviation",
    "href": "statistics.html#mean-and-standard-deviation",
    "title": "16  Statistics",
    "section": "16.4 Mean and standard deviation",
    "text": "16.4 Mean and standard deviation\n@@ TODO"
  },
  {
    "objectID": "subpopulations.html#sec-subpopulations",
    "href": "subpopulations.html#sec-subpopulations",
    "title": "17  Subpopulations and conditional frequencies",
    "section": "17.1 Subpopulations",
    "text": "17.1 Subpopulations\nWhen we have a statistical population with a joint variate, it is often of interest to focus on a subset of units that share the same value of a particular variate.\nConsider for instance the following population, a modified version of the glass-forensics example:\n\n\n\n\nunits: glass fragments (collected at specific locations)\nvariate: the joint variate \\((\\mathit{Ca}, \\mathit{Si}, \\mathit{Type})\\) consisting of three simple variates:\n\nweight fraction of \\(\\mathit{Ca}\\)lcium in the fragment (ordinal variate), with three possible values low, medium, high\nweight fraction of \\(\\mathit{Si}\\)licon in the fragment (ordinal variate), with three possible values low, medium, high\n\\(\\mathit{Type}\\) of glass fragment (nominal variate), with seven possible values building_windows_float_processed, building_windows_non_float_processed, containers, tableware, headlamps\n\n\n\n\nTable 17.1: Glass fragments simplified\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\nlow\nhigh\nheadlamps\n\n\n2\nlow\nmedium\nbuilding_windows_non_float_processed\n\n\n3\nmedium\nmedium\ntableware\n\n\n4\nmedium\nmedium\nbuilding_windows_float_processed\n\n\n5\nlow\nmedium\nheadlamps\n\n\n6\nmedium\nmedium\ncontainers\n\n\n7\nlow\nmedium\nbuilding_windows_non_float_processed\n\n\n8\nlow\nhigh\ntableware\n\n\n9\nmedium\nmedium\ntableware\n\n\n10\nmedium\nmedium\ntableware\n\n\n\n\nLet’s say we are interested only on those units that have the \\(\\mathit{Type}\\) variate equal to tableware. Discarding all others we obtain a new, smaller population with four units:\n\n\nTable 17.2: Selection according to \\(\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n3\nmedium\nmedium\ntableware\n\n\n8\nlow\nhigh\ntableware\n\n\n9\nmedium\nmedium\ntableware\n\n\n10\nmedium\nmedium\ntableware\n\n\n\n\nwere a bar “\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\)” indicates the variate used for the selection.\nAs another example, we could be interested instead in those units that have both \\(\\mathit{Ca}\\) and \\(\\mathit{Si}\\) variates equal to medium. We obtain a smaller population with five units:\n\n\nTable 17.3: Selection according to \\(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small medium}\\) and \\(\\mathit{Si}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small medium}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n3\nmedium\nmedium\ntableware\n\n\n4\nmedium\nmedium\nbuilding_windows_float_processed\n\n\n6\nmedium\nmedium\ncontainers\n\n\n9\nmedium\nmedium\ntableware\n\n\n10\nmedium\nmedium\ntableware\n\n\n\n\nPopulations formed in this way are called subpopulations of the original one. They are statistical populations in their own right. The notion of “subpopulation” is a relative notion. Any population can often be considered as a subpopulation of some larger population having additional variates.\n\n\n\n\n\n\n Exercise\n\n\n\n\nFrom the population of table  17.1:\n\nConstruct he marginal population with variate \\(\\mathit{Ca}\\)\nReport the frequency distribution for the marginal population above (remember that \\(\\mathit{Ca}\\) has three possible values)\nConstruct the subpopulation with variate \\(\\mathit{Si}\\) equal to high\nConstruct the subpopulation with variate \\(\\mathit{Type}\\) equal to headlamps and the variate \\(\\mathit{Si}\\) equal to medium\n\n\nCheck your understanding of the reasoning behind the notions of marginal population and subpopulation with this exercise:\n\nFrom the population of table  17.1, construct the subpopulation with variate \\(\\mathit{Type}\\) equal to either headlamps or tableware."
  },
  {
    "objectID": "subpopulations.html#sec-conditional-freqs",
    "href": "subpopulations.html#sec-conditional-freqs",
    "title": "17  Subpopulations and conditional frequencies",
    "section": "17.2 Conditional frequencies",
    "text": "17.2 Conditional frequencies\nGiven a statistical population with joint variates \\({\\color[RGB]{34,136,51}X}, {\\color[RGB]{238,102,119}Y}\\) (and possibly others), we define the conditional frequency of the value \\({\\color[RGB]{238,102,119}y}\\) of \\({\\color[RGB]{238,102,119}Y}\\), given or “conditional on” the value \\({\\color[RGB]{34,136,51}x}\\) of \\({\\color[RGB]{34,136,51}X}\\), as the frequency of the value \\({\\color[RGB]{238,102,119}y}\\) in the subpopulation selected by \\({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x}\\). This frequency is usually written\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x})\n\\]\nwhere \\(f\\) is the symbol for the joint frequency of the population.\nConsider for instance the glass-fragment population of table  17.1. The conditional frequency of \\({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}}\\) given \\({\\color[RGB]{34,136,51}\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}}\\) is the (marginal) frequency of \\(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}\\) in the subpopulation of table  17.2, from which we find\n\\[\nf({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}}) = \\frac{1}{4}\n\\]\nThe collection of these conditional frequencies for all values of \\({\\color[RGB]{238,102,119}Y}\\) constitutes the conditional frequency distribution of \\({\\color[RGB]{238,102,119}Y}\\) conditional on \\({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x}\\). In our example this distribution has three conditional frequencies:\n\\[\\begin{aligned}\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}}) = \\frac{1}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small medium}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}}) = \\frac{3}{4}\n\\\\\n&f({\\color[RGB]{238,102,119}\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small high}} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}}) = 0\n\\end{aligned}\\]\nwhich sum up to \\(1\\) as they should.\n\n\n\n\n\n\n Conditional on a value of a variate\n\n\n\nIt doesn’t make sense to speak of the conditional frequency distribution of \\(Y\\) “conditional on \\(X\\)”. Conditional frequencies and frequency distributions are always conditional on some value of a variate. If we consider all possible values of \\(Y\\) and of \\(X\\) we obtain a collection of frequencies that is not a distribution.\n\n\nA conditional frequency can be calculated as the ratio of a joint and a marginal frequencies, in a way analogous to conditional probabilities (§  13.2):\n\\[\nf({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} {\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x}) =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x})}{f({\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x})} =\n\\frac{f({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x})}{\n\\sum_{\\color[RGB]{238,102,119}y} f({\\color[RGB]{238,102,119}Y\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}y} \\mathbin{\\mkern-0mu,\\mkern-0mu}{\\color[RGB]{34,136,51}X\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}x})}\n\\]\n\n\n\n\n\n\n Exercises\n\n\n\nCalculate the conditional frequency distributions corresponding to the subpopulations of tables 17.2 and 17.3. For example, for table  17.2 this means calculating\n\\[\\begin{aligned}\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{Si}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware})\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{Si}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small medium} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware})\\ ,\n\\\\\n&\\dotsc\\ ,\n\\\\\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small high} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathit{Si}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small high} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware})\n\\end{aligned}\\]"
  },
  {
    "objectID": "subpopulations.html#associations",
    "href": "subpopulations.html#associations",
    "title": "17  Subpopulations and conditional frequencies",
    "section": "17.3 Associations",
    "text": "17.3 Associations\nThe analysis of subpopulations and conditional frequencies is important because it often reveals peculiar associations among different variates and groups of variates. Let’s illustrate what we mean with “association” – a term that we’ll use loosely, not with a strict technical definition – with an example.\nIf we extract the subpopulation having variate \\(\\mathit{Type}\\) equal to headlamps from the population of table  17.1:\n\n\nTable 17.4: Selection according to \\(\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Type}\\)\n\n\n\n\n1\nlow\nhigh\nheadlamps\n\n\n5\nlow\nmedium\nheadlamps\n\n\n\n\nwe notice that all units have variate \\(\\mathit{Ca}\\) equal to low. In terms of conditional frequencies, this means\n\\[\n\\begin{aligned}\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}) = 1\n\\\\\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small medium} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}) = 0\n\\\\\n&f(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small high} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}) = 0\n\\end{aligned}\n\\]\nIt is therefore impossible to observe other values of \\(\\mathit{Ca}\\) in this new population.11 We are not claiming that this fact will be true if new units are considered; this important question will be discussed later.\nOn the other hand, if we extract the subpopulation having variate \\(\\mathit{Ca}\\) equal to low we obtain\n\n\nTable 17.5: Selection according to \\(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}\\)\n\n\n\n\n\n\n\n\nunit\n\\(\\boldsymbol{\\nonscript\\:\\vert\\nonscript\\:\\mathopen{}}\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\n\n\n\n\n1\nlow\nhigh\nheadlamps\n\n\n2\nlow\nmedium\nbuilding_windows_non_float_processed\n\n\n5\nlow\nmedium\nheadlamps\n\n\n7\nlow\nmedium\nbuilding_windows_non_float_processed\n\n\n8\nlow\nhigh\ntableware\n\n\n\n\nwith conditional frequencies such as\n\\[\n\\begin{aligned}\n&f(\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}) = \\frac{2}{5}\n\\\\\n&f(\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}) = \\frac{1}{5}\n\\end{aligned}\n\\]\nand so on. The reverse is therefore not true: if \\(\\mathit{Ca}\\) is equal to low, that does not mean that it’s impossible to observe other \\(\\mathit{Type}\\) values besides headlamps. Note especially how these probabilities differ:\n\\[\n\\begin{gathered}\nf(\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}) = 1\n\\\\\nf(\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps} \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}) = \\frac{2}{5}\n\\end{gathered}\n\\]\nIn the original population we have, figuratively speaking, the following interesting association:\n\\[\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}\\ \\mathrel{\\color[RGB]{34,136,51}\\Rightarrow}\\\n\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}\n\\qquad\\text{\\small but}\\qquad\n\\mathit{Ca}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small low}\\  \\mathrel{\\color[RGB]{238,102,119}\\nRightarrow}\\\n\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small headlamps}\n\\]\nThis kind of associations is often useful. Suppose for instance that you are asked to pick a unit with \\(\\mathit{Ca}\\) equal to low in the original population; but it’s difficult to measure a unit’s \\(\\mathit{Ca}\\) value, while it’s easy to measure its \\(\\mathit{Type}\\) value. Then you could instead search for a unit having \\(\\mathit{Type}\\) equal to headlamps (easier search), and you would be sure that the unit you found also has \\(\\mathit{Ca}\\) equal to low.\n\n\nThe example above above, where some values of a variate completely exclude some values of another, is extreme. More often we find that there are small or large changes in the frequency distribution of some variate, depending on the subpopulation considered.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the (marginal) frequency distribution for the \\(\\mathit{Ca}\\) variate for the glass-fragment population of table  17.1, considering this variate to have values low and medium (so we disregard high). Is one value more frequent than the other?\nCalculate the frequency distribution for \\(\\mathit{Ca}\\), conditional on \\(\\mathit{Type}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small tableware}\\) (see table  17.2). How does this frequency distribution differ from the one calculated previously? Come up with possible ways to exploit this difference in concrete applications.\n\n\n\n\nAssociations can be very counter-intuitive\nIt is usually best to assess associations by explicitly calculating all relevant conditional frequencies, rather than jumping to intuitive conclusions after having examined just a few. Here’s an example.\n\n\nConsider the statistical population defined as follows:\n\n\n\n\nunits: all reparations done by a repair company on a particular kind of electronic components, which is extremely delicate and usually very difficult to repair. The population has 26 units (every unit actually represents a batch 100 reparations, so the population really refers to 2600 reparations).\na joint variate, consisting in three binary ones:\n\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\) of the repair procedure, with values \\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\) and \\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\);\nrepair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), with values \\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\) and \\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\), representing a traditional reparation method and one introduced more recently;\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) of the repair procedure, with values \\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\) and \\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\).\n\n\n\n\nTable 17.6: Reparations (each row is one unit, representing 100 reparations)\n\n\n\\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\)\n\\(\\mathit{\\color[RGB]{34,136,51}Method}\\)\n\\(\\mathit{\\color[RGB]{102,204,238}Location}\\)\n\n\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}success}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\\(\\texttt{\\small \\color[RGB]{238,102,119}fail}\\)\n\\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\)\n\\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\)\n\n\n\n\nThese population data are also stored in this csv file.\nThe repair company claims that, in this population, the \\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\) repair method is more effective than the \\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\). Can you back up their claims?:\n\n\n\n\n\n\n Exercise (one of the most fun of the course!)\n\n\n\nUse the population data above. The calculations can be done with any tools you like.\n\nExamine the whole population first:\n\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}new}\\) (note that we are disregarding the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\)).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}old}\\).\nCompare the two conditional frequency distributions above. Which of the two repair methods seems more effective?\nAre the claims of the repair company justified?\n\nNow examine the reparations that have been done \\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\):\n\nBefore doing any calculations, what do you expect to find? should the \\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\) repair method be more effective than the \\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\) one, for onsite reparations?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}new}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}old}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\).\nCompare the two conditional frequency distributions for this \\(\\texttt{\\small \\color[RGB]{102,204,238}onsite}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in step 3.?\n\nNow examine the reparations that have been done \\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\)ly:\n\nBefore doing any calculations, what do you expect to find? should the \\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\) repair method be more effective than the \\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\) one, for reparations done remotely?\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}new}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{102,204,238}remote}\\).\nCalculate the frequency distribution of the \\(\\mathit{\\color[RGB]{238,102,119}Outcome}\\) variate, conditional on \\(\\mathit{\\color[RGB]{34,136,51}Method}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}old}\\) and \\(\\mathit{\\color[RGB]{102,204,238}Location}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{102,204,238}remote}\\).\nCompare the two conditional frequency distributions for this \\(\\texttt{\\small \\color[RGB]{102,204,238}remote}\\) case. Which of the two repair methods seems more effective?\nHow do you explain this result in the light of what you found in steps 3. and 7.?\n\nSummarize and explain all your findings.\nCan the repair company claim that the \\(\\texttt{\\small \\color[RGB]{34,136,51}new}\\) repair method is better than the \\(\\texttt{\\small \\color[RGB]{34,136,51}old}\\)?\n\nSuppose you need to send an electronic component for repair to this company.\n\nIf you could choose both the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\) and the \\(\\mathit{\\color[RGB]{34,136,51}Method}\\) of the repair, which would you choose? why?\nIf you could only choose the repair \\(\\mathit{\\color[RGB]{34,136,51}Method}\\), but have no control over the \\(\\mathit{\\color[RGB]{102,204,238}Location}\\), which method would you choose? why?\n\nIs there other information, missing from the description of the population, that should be known before answering the questions above?\n\n\n\n\n\n\n\n\n\n\n\n\n Study reading\n\n\n\n\nThe role of exchangeability in inference This can be a difficult reading. Try to get the main message.\nSimpson’s paradox"
  },
  {
    "objectID": "samples.html#sec-infinite-populations",
    "href": "samples.html#sec-infinite-populations",
    "title": "18  Infinite populations and samples",
    "section": "18.1 Infinite populations",
    "text": "18.1 Infinite populations\nThe examples of populations that we explored so far comprised a small number of units, and all their data were exactly and fully known. In concrete inference and decision problems we usually deal with populations that are much larger, and often even potentially infinite; and some of their data might be unknown.\nIn the glass-forensic example (table  15.1), for instance, many more glass fragments could be examined beyond the 10 units reported there, with no clear bound on the total number. We could even extend that population considering glass fragments from past and future crime scenes:\n\n\n\nTable 18.1: Glass fragments, extended\n\n\n\n\n\n\n\n\n\n\nunit\n\\(\\mathit{RI}\\)\n\\(\\mathit{Ca}\\)\n\\(\\mathit{Si}\\)\n\\(\\mathit{Type}\\)\nnotes\n\n\n\n\n1\n\\(1.51888\\)\n\\(9.95\\)\n\\(72.50\\)\ntableware\n\n\n\n2\n\\(1.51556\\)\n\\(9.41\\)\n\\(73.23\\)\nheadlamps\n\n\n\n3\n\\(1.51645\\)\n\\(8.08\\)\n\\(72.65\\)\nbuilding_windows_non_float_processed\n\n\n\n4\n\\(1.52247\\)\n\\(9.76\\)\n\\(70.26\\)\nheadlamps\n\n\n\n5\n\\(1.51909\\)\n\\(8.78\\)\n\\(71.81\\)\nbuilding_windows_float_processed\n\n\n\n6\n\\(1.51590\\)\n\\(8.22\\)\n\\(73.10\\)\nbuilding_windows_non_float_processed\n\n\n\n7\n\\(1.51610\\)\n\\(8.32\\)\n\\(72.69\\)\nvehicle_windows_float_processed\n\n\n\n8\n\\(1.51673\\)\n\\(8.03\\)\n\\(72.53\\)\nbuilding_windows_non_float_processed\n\n\n\n9\n\\(1.51915\\)\n\\(10.09\\)\n\\(72.69\\)\ncontainers\n\n\n\n10\n\\(1.51651\\)\n\\(9.76\\)\n\\(73.61\\)\nheadlamps\n\n\n\n…\n…\n…\n…\n…\n…\n\n\n351\n\\(1.52101\\)\n\\(8.75\\)\n\\(71.78\\)\n?\nfrom unsolved-crime scene in 1963\n\n\n…\n…\n…\n…\n…\n…\n\n\n1027\n\\(1.51761\\)\n\\(7.83\\)\n\\(72.73\\)\n?\ncrime scene in 2063\n\n\n…\n…\n…\n…\n…\n…\n\n\n\n\n\nthe imaginary example above also shows that the values of some variates for some units might be unknown; this is a situation we shall discuss in depth later.\n\n\nWe shall henceforth focus on statistical populations with a number of units that is in principle infinite, or so large that it can be considered practically infinite. “Practically” means that the number of units we’ll use as data or draw inferences about is a very small fraction, say less than 0.1%, of the total population size.\nThis is often the case. Consider for example (as in § 9.1.1) the collection of all possible 128 × 128 images with 24-bit colour depth. This collection has \\(2^{24 \\times 128 \\times 128} \\approx 10^{118 370}\\) units. Even if we used 100 billions of such images as data, and wanted to draw inferences on another 100 billions, these would constitute only \\(10^{-118 357}\\,\\%\\) of the whole collection. This collection is practically infinite.\nNote that we can’t say whether a population, per se, is “practically infinite” or not. It could be practically infinite for a particular inference problem, but not for another.\nWhen we use the term “population” it will often be understood that we’re speaking about a statistical population that is practically infinite with respect to the inference or decision problem under consideration."
  },
  {
    "objectID": "samples.html#sec-samples",
    "href": "samples.html#sec-samples",
    "title": "18  Infinite populations and samples",
    "section": "18.2 Samples",
    "text": "18.2 Samples\nIn the glass-forensic example above (table  18.1), the units and data we had initially (table  15.1) have been considered as part or a much larger population. Such a part is called a population sample or “sample” for short. Almost all data considered in engineering and data-science problems can be considered to be population samples.\nIt is extremely important to specify how a sample is extracted from a population. For instance, if we consider table  15.1 to be a full population, we could extract a sample in such a way that \\(\\mathit{Type}\\) only has value \\(\\texttt{\\small headlamps}\\) (similarly to when we construct a subpopulation, §  17.1, but for a subpopulation we would select all units having that variate value). The marginal frequency of the value \\(\\texttt{\\small headlamps}\\) in the sample would then be \\(1\\), whereas in the original population it is \\(3/10 \\approx 0.333\\) – two very different frequencies.\n\n“Representative” and biased samples\nIn inference and decision problems we would like to use samples in which the various frequencies didn’t differ very much from those in the original population. Such a sample is called a “representative sample”. This is a difficult notion; the International Organization for Standardization for instance warns (item 3.1.14):\n\nThe notion of representative sample is fraught with controversy, with some survey practitioners rejecting the term altogether.\n\nIn many cases it is impossible for a sample of given size to be “representative”:\n\n\n\n\n\n\n Exercise\n\n\n\nConsider the following population of 16 units, with four binary variates \\(W,X,Y,Z\\), each with values \\(0\\) and \\(1\\):\n\n\n\n\nTable 18.2: Four-bit population\n\n\n\\(W\\)\n\\(X\\)\n\\(Y\\)\n\\(Z\\)\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n0\n1\n0\n0\n\n\n1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n1\n0\n\n\n0\n1\n1\n0\n\n\n1\n1\n1\n0\n\n\n0\n0\n0\n1\n\n\n1\n0\n0\n1\n\n\n0\n1\n0\n1\n\n\n1\n1\n0\n1\n\n\n0\n0\n1\n1\n\n\n1\n0\n1\n1\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n\n\n\n\nThe joint variate \\((W,X,Y,Z)\\) has 16 possible values, from \\((0,0,0,0)\\) to \\((1,1,1,1)\\). Each of these values appear exactly once in the population, so it has frequency \\(1/16\\). The marginal frequency distribution for each binary variate is also uniform, with frequencies of 50% for both \\(0\\) and \\(1\\).\n\nExtract a representative sample of four units. In particular, the marginal frequency distributions of the four variates should be as close to 50%/50% as possible.\n\n\n\n\n\nObviously we cannot expect a population sample to exactly reflect all frequency distributions – joint, marginal, conditional – of the original population; some discrepancy is to be expected. How much discrepancy should be allowed? And what is the minimal size for a sample not to exceed such discrepancy?\nInformation Theory gives reasonable answers to these questions; we summarize them here.\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\n\nChapters 1–10 of Information Theory, Inference, and Learning Algorithms\nVideo lectures 1–9 from the Course on Information Theory, Pattern Recognition, and Neural Networks\n\n\n\nFirst we need to define the Shannon entropy of a discrete frequency distribution. Lets say the distribution is \\((f_1,f_2, \\dotsc)\\). Its Shannon entropy \\(H(\\boldsymbol{f})\\) is then defined as\n\\[\nH(f) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n\\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nand is measured in bits (when the base of the logarithm is 2).\nIf we have a population with joint frequency distribution \\(\\boldsymbol{f}\\), then a representative sample from it must have at least size\n\\[\n2^{H(\\boldsymbol{f})} \\equiv\n\\frac{1}{{f_1}^{f_1}\\cdot {f_2}^{f_2}\\cdot {f_3}^{f_3}\\cdot \\dotsb}\n\\] \nThis particular number has important practical consequences; for example it is related to the maximum rate at which a communication channel can send symbols (which can be considered as values of a variate) with an error as low as we please.\n\n\n\n\n\n\n Exercise\n\n\n\n\nCalculate the Shannon entropy of the joint frequency distribution for the four-bit population of table  18.2.\nCalculate the minimum representative-sample size according to the Shannon-entropy formula. Is the result intuitive?\n\n\n\n\n\nBiases\nFor many populations it is difficult or impossible to obtain a “representative” sample, even when the sample is large. As the exercises above show, this impossibility remains even if we extract the sample in an unsystematic and unpredictable way (“random sample”), by shuffling or similar techniques.\nA sample that presents some aspects, such as frequency distributions, which are at variance with the original population, is sometimes called biased (this term is used in many different ways by different authors). Unfortunately, most samples are “biased” in this sense."
  },
  {
    "objectID": "recap.html#sec-recap",
    "href": "recap.html#sec-recap",
    "title": "19  A general inference problem",
    "section": "19.1 Recap",
    "text": "19.1 Recap\nOn the “inference side” we gave a concrete and operational explanation of what “drawing an inference” means: it is the calculation of the probabilities – the degrees of belief – of some sentences, from the probabilities of others. This means that in principle we can draw inferences and apply the probability calculus to literally anything that can be expressed by language.\nWe also saw the following points:\n\nThe calculation of probabilities only uses four fundamental – and mathematically quite simple – rules (§  7.4). Any inference, even those make by the most complex machine-learning algorithms, is just a repeated application of those four rules (sometimes with approximating shortcuts for the sake of speed).\nNo inference can be drawn unless some probabilities are first posited as a starting point. This is just another face of the formal-logic fact that no theorem (besides tautologies) can be derived by logic, unless some axioms are given first.\nThe four fundamental rules are determined by basic requirements of logical consistency. Modifying the rules would lead to inconsistent inferences and sub-optimal decisions.\n\n\n\n\n\n\n\n\n\nflowchart BT\n  A[quantity] --o B([sentences]) --&gt; C{{probability calculus}}\n  D[value] ---o B\n  E[population] --o B\n  F[...] -..-o B\n  G[(problem)] --x A & D & E\n  G[(problem)] -.-x F\n\n\n\n\n\nOn the “data side” we introduced a handful of notions, such as “quantity”, “having a value”, “domain”, “quantity type”, “population”, “variate”, “frequency”, and others. We learned how to use them, and paid attention to some of their counter-intuitive properties.\nThese notions allow us to speak, in a precise way, about typical situations and problems that arise in engineering and other scientific contexts. In essence we have introduced a particular kind of sentences to express engineering problems – so that we can apply the probability calculus to them and draw inferences about them.\nThis specific language has its roots in physics and mathematics, where it has been successfully used and refined for several centuries. But it might evolve in different directions in the future, to make allowance for new inferential problems. Later we shall indeed expand it in a couple of directions to cover particular operations that we do with data."
  },
  {
    "objectID": "population_inference.html#induction",
    "href": "population_inference.html#induction",
    "title": "20  Inference and populations",
    "section": "20.1 Induction",
    "text": "20.1 Induction\nIt is a fact of everyday experience that we expect things to happen in particular ways because of our previous experience with similar things. If we observe that ten electronic components in a row come out damaged from an assembly line, then we expect that the next one will be damaged as well. If you regularly go hiking in a particular area and often see deer, and sometimes hares, then you think it likely to see deer also at your next hike and, if you’re lucky, a hare. You would probably be surprised to see a moose if you had never seen one in those areas. This doesn’t only concern the future: if someone asks you whether you saw a deer during a hike from long time ago, which you don’t quite remember anymore, you’d say “probably I did”.\nThis familiar experience is called induction, especially in the philosophical literature. It has generated a lot of thought and research since the times of Hume (18th century), who apparently was the first to ask how and why this experience is possible.\nBut this kind of regularities does not have clear-cut circumstances. In some cases it is not clear what should be considered “similar” things – in fact, sometimes we circularly reverse the reasoning: if something does not respect an observed regularity then we say it wasn’t “similar”. Also the circumstances in which this regularity should occur are often not-well-defined: should you expect to see deer if you hike in a somewhat different area?\nAnd sometimes this kind of regularities simply fails. Something expected doesn’t happen any longer, even if the circumstances and the “similarity” are clearly the same. Jeffreys aptly said:\n\nA common argument for induction is that induction has always worked in the past and therefore may be expected to hold in the future. It has been objected that this is itself an inductive argumentand cannot be used in support of induction. What is hardly ever mentioned is that induction has often failed in the past and that progress in science is very largely the consequence of direct attention to instances where the inductive method has led to incorrect predictions.\n\n\n\n\n\n\n\n\n\n For the extra curious\n\n\n\nChapter I of Scientific Inference (3rd ed.) is an extremely insightful reading about inference, probability, and science.\n\n\n\nInduction and data science\nInduction, with the mentioned caveats that accompany it, is at the basis of how we approach and solve many engineering problems, and at the heart of data science: from data about particular things or phenomena we try to infer new or old instances of “similar” things or phenomena.\nWe shall now see how this is done in a quantitative manner with the probability calculus. It is important to clarify what the probability calculus, and all approximate inference methods derived from it, can and cannot do:\n\n It allows us to make inductive inferences in a quantitative and guaranteed self-consistent way.\n It does not explain why there are regularities and why induction in some cases works.\n It does not tell us which things or phenomena should be considered “similar”. In fact, what’s similar and what’s not is something that we must input into the probability calculus.\n If we formulate the “similarity” and “non-similarity” of an instance as well-defined hypotheses, then the probability calculus allows us to calculate their probabilities (and make a decision as to accept, if necessary)."
  },
  {
    "objectID": "population_inference.html#sec-two-populations",
    "href": "population_inference.html#sec-two-populations",
    "title": "20  Inference and populations",
    "section": "20.2 Inferences and populations",
    "text": "20.2 Inferences and populations\n\nStock exchange and Mars prospecting, again\nConsider the following two sketches of inference problems, related to the scenarios of §  15.1:\n\n\nStock exchange\n\nIn 100 days, the daily change in closing price of a stock has been positive 74 times, and negative 26 times, according a particular sequence; for instance:\n\n\n\nIn which of the subsequent 3 days will the closing-price change be positive, and in which negative?\n\n\n\n\n \n\n\nMars prospecting\n\nOf the last 100 similar-sized rocks examined in a large crater on Mars, 74 contained hematite (Y), and 26 did not (N). For instance, the data could be:\n\n\n\nWhich, among the next 3 rocks that will be examined, will contain hematite, and which will be hematite-free?\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nDiscuss:\n\nWhich of the two inferences above seems more difficult?…\n…Why? Speculate on which factors make one inference more difficult than the other.\nWhich differences and similarities do you find between the two inferences?\nWhich additional information could be important for drawing more precise inferences?\nWhich type of quantities appear in the two inferences?\n\n\n\n\n\nTranslation into sentences\nLet us remember chapter 7 that in order to set up and solve an inference problem we must first define appropriate sentences which we’ll use in probability supposals and conditionals.\nFor the two problems above it looks obvious that we can use sentences involving the language of statistical populations. We must then try to define the units and the variates. Tentatively let’s try the following definitions:\n\nStock exchange\n\n\nUnits: the problem is only sketched, so let’s assume that a precise definition can be found somewhere. There are at least 100 + 3 units, but the problem suggests that we might consider an infinite population.\nVariates: the change in closing price seems an obvious binary variate for the population. Let’s denote it \\(C\\), with domain \\(\\set{\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}},\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}}\\).\n\n\nMars prospecting\n\n\nUnits: again the problem is only sketched, so let’s assume that a precise description exists of how the rocks to be examined should be chosen. There are at least 100 + 3 units, but the problem suggests that we might consider a practically infinite population.\nVariates: the presence of hematite is an obvious binary variate. Let’s denote it with \\(H\\), with domain \\(\\set{\\texttt{\\small \\color[RGB]{34,136,51}Y},\\texttt{\\small \\color[RGB]{238,102,119}N}}\\).\n\n\n\nThe population data can be represented as follows, where question marks ? indicate the units about which we want to draw inferences, and the ellipses “…” indicate that the populations possibly extend to infinite other units:\n\n\n\n\nTable 20.1: Stock exchange\n\n\nunit\n\\(C\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n2\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n3\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n4\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n5\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n6\n\\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n96\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n97\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n98\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n99\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n100\n\\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\nTable 20.2: Mars prospecting\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n2\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n3\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n4\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n5\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n6\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n(omitted)\n(omitted)\n\n\n95\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n96\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n97\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n98\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n99\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n100\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n101\n?\n\n\n102\n?\n\n\n103\n?\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\nWould you define these two populations in a different way? Do you think other variates should be included, for example?\n\n\n\n\nDesired probabilities\nThe stock-exchange problem asks “In which of the subsequent 3 days will the closing-price change be positive, and in which negative?”. In terms of the populations just introduced, this can be translated into:\n\n“will the \\(C\\) variate for units #101, #102, #103 have value \\(\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\) or \\(\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\)?”\nIn other words, we are asking which of the two following mutually exclusive sentences, expressed symbolically, will be true:\n\\[\nC_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\qquad C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\]\nSince one of them must be true, their probabilities form a probability distribution (§  11), in which for the moment we omit the conditional:\n\\[\n\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}  \\quad\\quad\n\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\]\nAnalogously for units #102 and #103, leading to two more probability distributions.\nWhen we and together sentences for the three units we obtain a joint probability distribution (§  12) over 2³ = 8 mutually exclusive and exhaustive composite sentences:\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\n\\end{aligned}\\]\n\n\nNow let’s focus on the conditional. The information we are given consists of the values of the \\(C\\) variate for units #1 to #100, which we can and together. We must also and all other information implicit in the stock-exchange problem, which we denote \\(\\mathsfit{I}_{\\text{s}}\\):\n\\[\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\]\nWe finally arrive at the eight probabilities (which constitute a probability distribution) that we want to calculate:\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\\\[1ex]\n&\\dotso\n\\\\[1ex]\n&\\mathrm{P}\\bigl(C_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{} C_{100}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\dotsc \\mathbin{\\mkern-0mu,\\mkern-0mu}\nC_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{34,136,51}\\boldsymbol{+}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{2}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}C_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\mathord{\\color[RGB]{238,102,119}\\boldsymbol{-}}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}_{\\text{s}} \\bigr)\n\\end{aligned}\\]\n\n\n\n\n\n\n\n Exercise\n\n\n\nDo a similar analysis for the Mars-prospecting problem, and write down the final probability distribution that we wish to calculate.\n\n\n\n\nThe next – fundamental – question is: how do we calculate these probabilities? Which other probabilities can we take as our starting point?"
  },
  {
    "objectID": "exchangeability.html#sec-exch-populations",
    "href": "exchangeability.html#sec-exch-populations",
    "title": "21  Exchangeability",
    "section": "21.1 Infinitely exchangeable populations",
    "text": "21.1 Infinitely exchangeable populations\n\nDefinition\nConsider a (practically) infinite statistical population with variate \\(X\\). This variate could be a joint one, consisting of variates \\(U,V,W,\\dots\\) of arbitrary types. For simplicity let’s assume that the domain of this variate is discrete. For instance, the variate could be of a binary type with domain \\(\\set{\\texttt{\\small Yes}, \\texttt{\\small No}}\\); or it could be the combination of such a binary variate and an another ordinal variate with domain \\(\\set{\\texttt{\\small low}, \\texttt{\\small medium}, \\texttt{\\small high}}\\); so the domain of the joint variable would be the set of 2 × 3 values \\(\\set[\\big]{(\\texttt{\\small Yes}, \\texttt{\\small low}),\\ (\\texttt{\\small No}, \\texttt{\\small low}),\\ \\dotsc,\\ (\\texttt{\\small No},\\texttt{\\small high})}\\).\nThis variate associates a quantity to each unit in the population. We denote by \\(X_1\\) the variate for unit #1, and so on.\nNow consider an agent, with background knowledge \\(\\mathsfit{I}\\), which must draw inferences about some population units.\nWe call the infinite population exchangeable if the agent’s inferences are unaffected by the units’ identities, and only depend on the units’ variate values. Said otherwise, exchanges among units that have the same values don’t matter for inference purposes.\n\n\nExample\nLet’s make this clear with a simplified version of the Mars-prospecting example.\nSuppose the agent knows the values of units #95–#99, and needs the probability that unit #101 has variate value \\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\), unit #102 value \\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\), and unit #103 value \\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\), as schematized below:\n\n\n\n\n\nagent’s 1st inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n2\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n3\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n4\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n5\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n6\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n7\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n8\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n…\n…\n\n\n95\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n96\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n97\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n98\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n99\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n100\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n101\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n102\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n103\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\\[\n\\mathrm{P}\\bigl(H_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{98}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{97}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{96}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\]\n\n\nIf the population is exchangeable, then the inference above is exactly the same – the probability values are identical – as the following one:\n\n\n\n\n\nagent’s 2nd inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n2\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n3\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n4\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n5\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n6\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n7\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n8\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n…\n…\n\n\n95\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n96\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n97\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{?}}\\)\n\n\n98\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n99\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n100\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n101\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n102\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n103\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\qquad\\textbf{given}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\\[\n\\mathrm{P}\\bigl(H_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{7}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{97}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{5}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{6}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\]\nWhy? Because both inferences have one \\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) and two \\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) in their supposals, and both have three \\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) and two \\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) in their conditionals. Or, from a different point of view, both inferences look the same if we reorder the units (each keeping its value), as shown below:\n\n\n\n\n2nd inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n2\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n3\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n4\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n5\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n6\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n7\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n8\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n…\n…\n\n\n95\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n96\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n97\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n98\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n99\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n100\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n101\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n102\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n103\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n2nd inference reordered\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n99\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n2\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n101\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n4\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n98\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n96\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n102\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n8\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n…\n…\n\n\n95\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n6\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n103\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n5\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n1\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n100\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n3\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n7\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n97\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n1st inference\n\n\nunit\n\\(H\\)\n\n\n\n\n…\n…\n\n\n1\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n2\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n3\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n4\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n5\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n6\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n7\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n8\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\)\n\n\n…\n…\n\n\n95\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n96\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n97\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n98\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n99\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{given}}\\)\n\n\n100\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\)\n\n\n101\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n102\n\\(\\texttt{\\small \\color[RGB]{34,136,51}Y}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n103\n\\(\\texttt{\\small \\color[RGB]{238,102,119}N}\\) \\(\\mathrlap{\\quad\\quad\\textbf{?}}\\)\n\n\n…\n…\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n&\\mathrm{P}\\bigl(H_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{98}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{97}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{96}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\\\[1ex]\n&=\\mathrm{P}\\bigl(H_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{7}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{97}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\n\\nonscript\\:\\big\\vert\\nonscript\\:\\mathopen{}\nH_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{5}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{95}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{6}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nH_{103}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}\\bigr)\n\\end{aligned}\\]\nThis equality under exchanges holds no matter how many units we consider in the supposal and in the conditional (the conditional could even be empty).\nAs two additional examples with the same population,\n\\[\\begin{gathered}\n\\mathrm{P}(H_{2}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} H_{101}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n= \\mathrm{P}(H_{98}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} H_{8}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I})\n\\\\[1ex]\n\\mathrm{P}(H_{99}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{7}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n=\\mathrm{P}(H_{4}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{102}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{34,136,51}Y}\\mathbin{\\mkern-0mu,\\mkern-0mu}H_{95}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small \\color[RGB]{238,102,119}N}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\mathsfit{I})\n\\end{gathered}\\]\n\n\nThe definition of exchangeability extends in an obvious way to populations with variates having arbitrary discrete domains. As an example consider a population with an ordinal variate \\(X\\) having domain of three values \\(\\set{\\texttt{\\small\\color[RGB]{34,136,51}low},\\texttt{\\small\\color[RGB]{102,204,238}med},\\texttt{\\small\\color[RGB]{204,187,68}high}}\\). If this population is exchangeable for an agent with state of knowledge \\(\\mathsfit{J}\\), then we must have for instance\n\\[\\begin{aligned}\n&\\mathrm{P}( X_5\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_2\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{204,187,68}high}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nX_{14}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{102,204,238}med}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{7}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{34,136,51}low}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_{1}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\n\\\\[1ex]\n&=\n\\mathrm{P}( X_1\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_{90}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{204,187,68}high}\\mathbin{\\mkern-0mu,\\mkern-0mu}\nX_{3}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{102,204,238}med}\\nonscript\\:\\vert\\nonscript\\:\\mathopen{} X_{7}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{34,136,51}low}\\mathbin{\\mkern-0mu,\\mkern-0mu}X_{5}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\texttt{\\small\\color[RGB]{102,204,238}med}\\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{J})\n\\end{aligned}\\]\n\n\n\n\n\n\n Important points about exchangeability\n\n\n\n\nStrictly speaking, exchangeability is not an intrinsic property of a population. It is a property of an agent’s state of knowledge about the population. For instance, if an agent knows that the units’ indices reflect some temporal order, then that agent generally won’t consider that population as exchangeable. Another agent, oblivious of the fact that the units’ indices carry information, may instead consider that population as exchangeable.\nThe probability calculus doesn’t tell us if a population is exchangeable; in fact, it requires exchangeability (or non-exchangeability) as an input.\n…However, if the possibility of exchangeability and non-exchangeability are formulated as two well-defined hypotheses, the probability calculus can tell us their probabilities.\nThere isn’t any clear-cut dichotomy between exchangeable and non-exchangeable populations. Rather, the discrepancy between probabilities under exchanges of units gradually increases from practically acceptable levels to unacceptable ones. What’s acceptable depends on the particular problem.\n\n\n\n\n\n\n\n\n\n Exercise\n\n\n\n@@ TODO"
  },
  {
    "objectID": "inference_exchangeability.html#sec-inference-known-freq",
    "href": "inference_exchangeability.html#sec-inference-known-freq",
    "title": "22  Inference under exchangeability",
    "section": "22.1 Inference when population frequencies are known",
    "text": "22.1 Inference when population frequencies are known\nSuppose we have a practically infinite statistical population with variate \\(X\\) having a discrete and finite domain. For simplicity let’s assume the domain consists in the integers \\(\\set{1,2,\\dotsc,K}\\) (we can always rename the actual domain values so as to put them into correspondence with a set of integers).\nAn agent with state of knowledge \\(\\mathsfit{I}\\) needs to draw inferences about some units, given the values of other units, as in the examples of the previous §  20.2 and chapter  21.\nNow we suppose that the agent additionally knows the joint frequency distribution for the variate \\(X\\) in the population. We express this knowledge with the sentence \\(\\boldsymbol{F}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\boldsymbol{f}\\), where \\(\\boldsymbol{f}\\) is a \\(K\\)-tuple of numerical frequencies. We denote the frequencies of the values \\(1,2,\\dotsc, K\\) by \\(f_1, f_2, \\dotsc, f_K\\).\nIf someone gave you the frequencies above (their exact numerical values), and asked you your degree of belief that the variate for some unit, say unit #47, has value \\(2\\), what would you answer?\nThis scenario has a very strong symmetry. A fraction \\(f_2\\) of all the units have value \\(2\\). Unit #47 could be one among those in this fraction, or one among those in the remaining  \\(1-f_2\\)  fraction. We are compelled to give probability \\(f_2\\) that unit #47 has value \\(2\\):\n\\[\n\\mathrm{P}(X_{47}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}2 \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\boldsymbol{f} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = f_2\n\\]\nThis is just one way of looking at this scenario. Alternative ways show similar symmetries, which lead to the same degree of belief.\nMore generally, the agent’s degree of belief on some unit #\\(u\\) to have value \\(i\\), when the joint frequency distribution \\(f\\) is known, must be\n\\[\n\\mathrm{P}(X_u \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}i \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\boldsymbol{f} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = f_i\n\\]\n\n\nWe can extend this reasoning to more than one unit. A crucial point here is that the population is practically infinite. If we know that the frequency of value \\(2\\) is \\(f_2\\), and then we observe a unit with that value, then we know that the total number of units having value \\(2\\) has slightly decreased. In a practically infinite population, however, this decrease is negligible, so we can think of \\(f_2\\) as remaining the same.\n\n\n\n\n\n\n Exercise\n\n\n\nSuppose the population is finite, with \\(N\\) units. The absolute frequency (§  16.2) of value \\(2\\) is then \\(Nf_2\\).\n\nCalculate the new absolute and relative frequencies of value \\(2\\) after we remove one unit with that value.\nCalculate the difference between the previous and new frequency.\nDo the same calculations also for the other variate values (\\(1,3,\\dotsc\\)).\n\n\n\nWe therefore find that the agent’s degree of belief on some unit #\\(u\\) to have value \\(i\\), and some other unit #\\(v\\) to have value \\(j\\), when the joint frequency distribution \\(f\\) is known, must be\n\\[\n\\mathrm{P}(X_u \\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}i \\mathbin{\\mkern-0mu,\\mkern-0mu}X_v\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}j \\nonscript\\:\\vert\\nonscript\\:\\mathopen{} \\boldsymbol{F}\\mathrel{\\nonscript\\mkern-2.5mu\\textrm{\\small=}\\nonscript\\mkern-2.5mu}\\boldsymbol{f} \\mathbin{\\mkern-0mu,\\mkern-0mu}\\mathsfit{I}) = f_i \\cdot f_j\n\\]\nAnd so on for more units.\n\n@@ TODO to be continued"
  },
  {
    "objectID": "missing_parts.html",
    "href": "missing_parts.html",
    "title": "23  To be written",
    "section": "",
    "text": "The final part, about population inference and the universal machine that does it, has yet to be written"
  },
  {
    "objectID": "glass_application_R.html#example-population-and-data",
    "href": "glass_application_R.html#example-population-and-data",
    "title": "24  The universal exchangeable-inference machine in action",
    "section": "24.1 Example population and data",
    "text": "24.1 Example population and data\nConsider the following population, which we consider to be exchangeable:\n\nUnits: glass fragments collected at particularly defined crime scenes.\nVariates:\n\n\\(\\mathit{RI}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Refractive Index of the fragment\n\\(\\mathit{Na}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Natrium content of the fragment\n\\(\\mathit{Mg}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Magnesium content of the fragment\n\\(\\mathit{Al}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Aluminium content of the fragment\n\\(\\mathit{Si}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Silicon content of the fragment\n\\(\\mathit{K}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Potassium content of the fragment\n\\(\\mathit{Ca}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Calcium content of the fragment\n\\(\\mathit{Ba}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Barium content of the fragment\n\\(\\mathit{Fe}\\), ordinal, domain \\(\\set{1,\\dotsc,4}\\): Iron content of the fragment\n\\(\\mathit{Type}\\), nominal, domain \\(\\set{\\texttt{\\small T1},\\dotsc,\\texttt{\\small T7}}\\): Type or origin of the glass fragment\n\nThe values for the \\(\\mathit{RI}\\) and content variates represent ranges of numeric values or percentages, which you can find in this metadata file. In the same file you also find the description of the glass types.\n\nWe have a sample of 214 units from this population; their variate values are stored in this data file. Here are the first five units:\n\nprint(head(fread('datasets/glass_data-4_lev.csv'), 5))\n\n   Id RI Na Mg Al Si K Ca Ba Fe Type\n1:  1  2  2  4  2  3 1  2  1  1   T2\n2:  2  2  1  3  2  3 1  2  1  1   T2\n3:  3  2  2  4  2  2 1  2  1  1   T1\n4:  4  1  2  2  2  3 1  2  1  1   T4\n5:  5  2  2  3  2  3 1  2  1  1   T1\n\n\n\n\nNow we imagine to prepare an agent for drawing inferences about the full population of glass fragments – which also means fragments from future or unsolved crime scenes. The agent uses – or is the embodiment of – the universal exchangeable-inference machine."
  },
  {
    "objectID": "glass_application_R.html#the-agents-initial-state-of-knowledge",
    "href": "glass_application_R.html#the-agents-initial-state-of-knowledge",
    "title": "24  The universal exchangeable-inference machine in action",
    "section": "24.2 The agent’s initial state of knowledge",
    "text": "24.2 The agent’s initial state of knowledge\n\nLoad the machine’s apparatus\nThe call\nsource('code/universal_exch-inference_machine.R')\nat the beginning of this chapter loaded several R functions that implement the universal machine: they draw inferences, calculate marginal and conditional probabilities, and plot probability distributions.\n\n\nLearning the background information\nOur agent at the moment doesn’t know anything at all, not even about the existence of the population above. If we were to ask it anything, we would just get a blank stare back.\nLet us give it the basic background information about the population: the variates’ names and domains. We do this through the function finfo(): it has a data argument, which we omit for the moment, and a metadata argument. The latter can simply be the name of the file containing the metadata (NB: this file must have a specific format):\n\npriorknowledge &lt;- finfo(metadata='datasets/glass_metadata-4_lev.csv')\n\nThe agent now possesses this basic background knowlege, encoded in the priorknowledge object. The encoding uses a particular mathematical representation which, however, is of no interest to us1. Other representations could also be used, but the knowledge would be the same. Think of this as encoding an image into a png or other lossless format: the representation of the image would be different, but the image would be the same.1 If you’re curious you can have a glimpse at it with the command str(priorknowledge), which displays structural information about an object.\n\n\nPreliminary inferences about the population\nNow the agent knows about the population, variates, and domains. But it has not seen any data, that is, the variate values for some units. Yet we can ask it some questions and to draw some inferences. Remember that the answer to a question is not just a value: it is the collection of all possible values, with a probability assigned to each. If the actual value is known, then it will have probability 1, and all others probability 0.\nLet’s ask the agent: what is the marginal frequency distribution for the variate \\(\\mathit{Type}\\), in the full (infinite!) population? Obviously the agent doesn’t know what the actual distribution is, nor do we. It will calculate a probability distribution over all possible marginal frequency distributions.\nThis probability distribution for the \\(\\mathit{Type}\\) variate is calculated by the function fmarginal(). It has arguments finfo: the agent’s information; and variates: the names of the variates of which we want the marginal frequencies:\n\npriorknowledge_type &lt;- fmarginal(finfo=priorknowledge, variates='Type')\n\nThe answer is stored in the object priorknowledge_type, which now contains only information pertinent to the \\(\\mathit{Type}\\) variate.\nWe would like to visualize this probability distribution over marginal frequency distributions. A complication is that we would need infinite dimensions to visualize this faithfully. One approximate way to represent this probability distribution is by showing, say, 100 representative samples from it. The idea is the same as for a scatter plot (§  11.5). In this case we would then have 100 different frequency distributions for the variate \\(\\mathit{Type}\\).\nThe function plotsamples1D() does this kind of visual representation. It has arguments finfo: the object encoding the probability distribution; n (default 100): the number of samples to show; and predict, which for the moment we set to FALSE and discuss in a moment.\nHow do you think this probability distribution will look like? what kind of marginal frequencies we do expect in the full population?\n\nplotsamples1D(finfo=priorknowledge_type, n=100, predict=FALSE)\n\n\n\n\nYou see that anything goes: Some frequency distributions give frequency almost 1 to a specific value, and almost 0 to the others. Other frequency distributions spread out the frequencies more evenly, with some peaks here or there.\nThis is a meaningful answer, because the agent hasn’t seen any data. From its point of view, everything is possible in this population.\n@@ TODO: add representation as quantiles\n\n\nPreliminary inferences about units\nUp to now the agent has drawn an inference regarding the full population, not regarding any specific unit. Now let’s ask it: what will be the value of the \\(\\mathit{Type}\\) variate in the next unit, or glass fragment, we observe? As usual, an answer consists in a probability distribution over all possible values.\n\n\n\n\n\n\n Exercise\n\n\n\nBefore continuing, ask yourself the same question: which probabilities would you give to the \\(\\mathit{Type}\\) values for the next unit, given that you only know that this quantity has seven possible values?\n\n\nThe agent’s answer this time is a probability distribution over seven values, which we can draw faithfully. The function plotsamples1D() can draw this probability as well, if we give the argument predict=TRUE (default):\n\nplotsamples1D(finfo=priorknowledge_type)\n\n\n\n\nThis plot shows the probability distribution for the next unit in blue, together with a sample of 100 possible frequency distributions for the \\(\\mathit{Type}\\) variate over the full population. Note that samples are drawn anew every time, so they can look somewhat differently from time to time.22 To have reproducible plots, use set.seed(314) (or any integer you like) before calling the plot function.\nThe agent’s answer is that in the next unit we can observe any \\(\\mathit{Type}\\) value with equal probability. Do you think this is a reasonable answer?\nThe plot above and the information it represents are very useful for inference purposes: not only they give the probability for the next observation, but also an idea of how such a probability could change in the future, as more data are collected and knowledge of the population’s frequency distribution becomes more precise.\n\n\n\n\n\n\n Exercise\n\n\n\nInspect the agent’s inferences for other variates."
  },
  {
    "objectID": "glass_application_R.html#the-agent-learns-from-data",
    "href": "glass_application_R.html#the-agent-learns-from-data",
    "title": "24  The universal exchangeable-inference machine in action",
    "section": "24.3 The agent learns from data",
    "text": "24.3 The agent learns from data\n\nLearning from the sample data\nNow let’s give the agent the data from the sample of 214 glass fragments. This is done again with the finfo() function, but providing the data argument, which can be the name of the data file:\n\npostknowledge &lt;- finfo(data='datasets/glass_data-4_lev.csv', metadata='datasets/glass_metadata-4_lev.csv')\n\nThe postknowledge object contains the agent’s knowledge from the metadata and the sample data. This object can be used in the same way as the object representing the agent’s background knowledge.\n\n\nInferences about the population\nNow that the agent has learned from the data, we can ask it again what is the marginal frequency distribution for the variate \\(\\mathit{Type}\\), in the full population.\nWe calculate the probability for the possible marginal frequency distributions, and then plot it as a set of 100 representative samples:\n\npostknowledge_type &lt;- fmarginal(finfo=postknowledge, variates='Type')\n\nplotsamples1D(finfo=postknowledge_type, predict=FALSE)\n\n\n\n\nThis plot shows two important aspects of this probability distribution and of the agent’s current state of knowledge:\n\nNot anything goes anymore. Some frequency distributions are clearly “excluded”, or more precisely they are extremely improbable. The most probable frequency distributions have a maximum at the \\(\\texttt{\\small T2}\\) value, another lower peak for the value \\(\\texttt{\\small T4}\\), and several other qualitative features that can be glimpsed from the plot.\nYet, the agent still has a degree of uncertainty, qualitatively shown by the width of the “bands” of frequency distributions. For example, the frequency for the \\(\\texttt{\\small T2}\\) value could be \\(0.40\\) as well as \\(0.30\\)\n\n@@ TODO: add code with function that reports the exact probabilities of the frequencies.\nAnd there are other more specific aspects that can be found by visual inspection. For instance:\n\nSome frequency distributions have their absolute maximum at \\(\\texttt{\\small T1}\\), and a lower value at \\(\\texttt{\\small T2}\\). Vice versa, others have their absolute maximum at \\(\\texttt{\\small T2}\\) and a lower value at \\(\\texttt{\\small T1}\\). So there’s still uncertainty as to which value is the most frequent in the full population.\nThe agent gives a very small but non-zero frequency to the value \\(\\texttt{\\small T7}\\). Yet, the data have no units at all with the \\(\\texttt{\\small T7}\\) value. Even if the agent has never seen this value in the data it was given, it knows nevertheless, from the metadata, that this is a possible value. So the agent doesn’t dogmatically say that its frequency in the full population should be zero (as in the sample); only that it should be extremely low.\n\n\n\n\n\n\n\n Exercise\n\n\n\nGiven that the value \\(\\texttt{\\small T7}\\) has not been observed in 214 units, what approximate upper bound to its frequency would you give? Does the agent’s inference agree with your intuition?\n\n\n\n\nInferences about units\nFinally we ask the agent what \\(\\mathit{Type}\\) value we should observe in the next glass fragment. The probability distribution answering this question is plotted by the same function with the argument predict=TRUE (default), as before:\n\nplotsamples1D(finfo=postknowledge_type)\n\n\n\n\nFigure 24.1: Frequency distributions for full population, and probability distribution for next unit"
  },
  {
    "objectID": "glass_application_R.html#conditional-discriminative-or-supervised-learning-inferences",
    "href": "glass_application_R.html#conditional-discriminative-or-supervised-learning-inferences",
    "title": "24  The universal exchangeable-inference machine in action",
    "section": "24.4 Conditional, “discriminative” or “supervised-learning” inferences",
    "text": "24.4 Conditional, “discriminative” or “supervised-learning” inferences\nThe inferences about a new units that the agent has made so far were of an “unsupervised-learning” or “generative” kind ([§ sec-2nd-connection-ML]): the agent did not receive or use any partial information about a new unit. Let’s now try a “supervised-learning” or “discriminative” kind of inference.\nImagine that we are at a new crime scene, a glass fragment is recovered, and tests are made about its refractive index and chemical composition. The following values are found, referred to the levels of our variates:\n\n\n\n\nnewfragment &lt;- c(RI=2, Na=2,  Mg=3,  Al=2,  Si=3,  K=1, Ca=2,  Ba=1,  Fe=1)\n\nThe detectives would like to know what’s the possible origin of this fragment, that is, its \\(\\mathit{Type}\\). Our agent can draw this inference.\nFirst, the agent can calculate the probability distribution over the conditional frequencies (§  17.2) of the \\(\\mathit{Type}\\) values for the subpopulation (§  17.1) of units having the specific variate values above. This calculation is done with the function fconditional(), with arguments finfo: the agent’s current knowledge, and unitdata: the partial data obtained from the unit.\n\ncondknowledge &lt;- fconditional(finfo=postknowledge, unitdata=newfragment)\ncondknowledge_type &lt;- fmarginal(finfo=condknowledge, variates='Type')\n\nThe condknowledge object contains the agent’s knowledge conditional on the variates given; this knowledge is about the remaining variates, which in this case are the single variate \\(\\mathit{Type}\\) (so the fmarginal() calculation is actually redundant in this case).\nSecond, the agent can calculate the probability distribution for the \\(\\mathit{Type}\\) values of this particular glass fragment, given the above information.\nBoth inferences can be visualized in the usual way:\n\nplotsamples1D(finfo=condknowledge_type)\n\n\n\n\nFigure 24.2: Conditional frequency distributions for full population, and conditional probability distribution for next unit\n\n\n\n\nThe agent thus gives a probability around \\(80\\%\\) to the fragment’s being of \\(\\texttt{\\small T1}\\) type, around \\(10\\%\\) of being \\(\\texttt{\\small T2}\\) type, and around \\(5\\%\\) of being \\(\\texttt{\\small T5}\\) type. It also shows that further training data could change these probabilities by even \\(\\pm 10\\%\\) or even \\(\\pm 15\\%\\).\nNote how the possible conditional frequency distributions for \\(\\mathit{Type}\\) and the probability distribution differ from the unconditional ones shown in fig.  24.1. The global maximum, \\(80\\%\\), is now sharper than the one for the unconditional case, \\(35\\%\\). This means that knowledge of the other variates for the present fragment has decreased the agent’s uncertainty as regards its type.\nShould the crime investigation proceed on the assumption that the fragment is of \\(\\texttt{\\small T1}\\) type? No, not necessarily. The best decision depends on the gains and costs involved in making correct or wrong assumptions. To this last decision-making problem we turn next.\n\n\n\n\n\n\n Exercise\n\n\n\n\nPerform the “discriminative” inferences above, but omitting from the newfragment data one variate in turn; so first omit only \\(\\mathit{RI}\\) and do the inferences, then omit only \\(\\mathit{Na}\\) and do the inferences, and so on.\nIn each of these inferences you will find that the agent becomes more ore less “confident” about the fragment type. Which of the variates above seem to be most important for making a confident inference?"
  },
  {
    "objectID": "making_decisions.html#decisions-possible-situations-and-consequences",
    "href": "making_decisions.html#decisions-possible-situations-and-consequences",
    "title": "25  Making decisions",
    "section": "25.1 Decisions, possible situations, and consequences",
    "text": "25.1 Decisions, possible situations, and consequences"
  },
  {
    "objectID": "making_decisions.html#gains-and-losses-utilities",
    "href": "making_decisions.html#gains-and-losses-utilities",
    "title": "25  Making decisions",
    "section": "25.2 Gains and losses: utilities",
    "text": "25.2 Gains and losses: utilities\n\nFactors that enter utility quantification\nUtilities can rarely be assigned a priori."
  },
  {
    "objectID": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "href": "making_decisions.html#making-decisions-under-uncertainty-maximization-of-expected-utility",
    "title": "25  Making decisions",
    "section": "25.3 Making decisions under uncertainty: maximization of expected utility",
    "text": "25.3 Making decisions under uncertainty: maximization of expected utility"
  },
  {
    "objectID": "machine_learning_overview.html",
    "href": "machine_learning_overview.html",
    "title": "26  Machine learning - an introduction",
    "section": "",
    "text": "27 TODO discuss functional form\nAs engineers, when faced with the task of using data to model the behaviour of some system, our job is twofold. First, we need to select a suitable method to serve as the function \\(f\\), and second, we need to find the optimal values for the parameters \\(\\theta\\). In ADA501 we will see how to choose an analytical \\(f\\) that corresponds to certain physical systems, while in our course, we will look at methods where \\(f\\) can be practically anything. In fact, we will not even require \\(f\\) to be a deterministic function – consider for instance generative models like ChatGPT or DALL-E, which creates different outputs for the same input, each time it is run.\nFinding the parameters \\(\\theta\\) is what takes us from a general method, to the specific solution to a problem. As we will see, the way of finding them differs between the various machine learning methods, but the principle is always the same: We need to choose a loss function, and then iteratively adjust \\(\\theta\\) so that the loss, when computed on known data, becomes as small as possible. The loss function should represent the difference between what the model outputs, and the correct answer – the better the model, the smaller the difference. The choice of this loss function depends on what kind of problem we wish to solve, and we will look at the common ones shortly. But at this point we can already define the three main types of machine learning:\nHaving decided on a method to represent \\(f\\) and found a set of parameters \\(\\theta\\), we say that these combined constitute our model. The model should have internalised the important correlations in the data and thereby allows us to make predictions, i.e. do modelling. If we do decision-making based on the output of the model as well, we typically call in an agent, since there is now some level of autonomy involved. In this chapter, however, we will stick to modelling problems in a supervised fashion."
  },
  {
    "objectID": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "href": "machine_learning_overview.html#hyperparameters-and-model-complexity",
    "title": "26  Machine learning - an introduction",
    "section": "27.1 Hyperparameters and model complexity",
    "text": "27.1 Hyperparameters and model complexity\nYou may have heard the quote by statistician George Box:\n\nAll models are wrong, but some are useful.\n\nAlthough coming off as a somewhat negative view on things, the quote still captures an important point about statistical modelling – our goal is not to make a complete, 100% accurate description of reality, but rather a simplified description that is meaningful in the context of our task at hand. The “correct” level of simplicity, in other words the optimal number of parameters \\(\\theta\\), can be hard to find. Often it will be influenced by practical considerations such as time and computing power, but it is always governed by the amount of data we have available. We will look at the theory of model selection later, but let us first consider a visual example, from which we can define some important concepts.\nImagine that you don’t have a thermometer that shows the outside temperature. Never knowing whether you should wear a jacket or not when leaving the house, you finally have an idea: If you can construct a mathematical model for the outside temperature as a function of the time of day, then a look at the clock yould be enough to decide for or against bringing a jacket. You manage to borrow a thermometer for a day, and make ten measurements at different times, which will form the basis for the model:\n\n\n\n\n\n\nFigure 27.1: Temperature measurements over the course of 24 hours.\n\n\n\n\nSource: over_and_underfitting.ipynb\nThe next step is to choose a function \\(f\\). For one-dimensional data like this, we could for instance select among the group of polynomials, of the form \\[\n    y (x, \\mathbf{w}) = w_0 + w_1 x + w_2 x^2 + \\dots + w_M x^M = \\sum_{j=0}^{M} w_j x^j \\,,\n\\] where \\(M\\) is the order of the polynomial. Recall that a zero’th order polymonial is just a constant value, so such a model would be represented with one single parameter. A first-order polynomial is a linear function (two parameters), and the higher in order (and number of parameters) we go, the more “curves” the function can have. This already presents us with a problem. Which order polynomial (i.e. which value of \\(M\\)) should we choose? Let us try different ones, and for each case, fit the parameters, meaning we find the parameter values that yield the smallest difference from the measured data points:\n\n\n\n\n\n\n\n\n(a) Zeroth-order polymonial\n\n\n\n\n\n\n\n(b) First-order polymonial\n\n\n\n\n\n\n\n\n\n(c) Fourth-order polymonial\n\n\n\n\n\n\n\n(d) Ninth-order polymonial\n\n\n\n\nFigure 27.2: Fitting polymonials of different orders to the set of measurements.\n\n\nSource: over_and_underfitting.ipynb\nObviously, the constant function does a poor job of describing our data, and likewise for the linear function. A fourth-order polynomial, on the other hand, looks very reasonable. Now consider the ninth-order polynomial: it matches the measurements perfectly, but surely, this does not match our expectation for what the temperature should look like.\nWe say that the first and second model underfit the data. This can happen for two reasons: Either the model has too little complexity to be able to describe the data (which is the case in this example), or, potentially, the optimal parameter values have not been found. The opposite case is overfitting, as shown for the last model, where the complexity is too high and the model adapts to artifacts in the data.\nThis concept is also called the bias-variance tradeoff. We will not go into too much detail on this yet, but qualitatively, we can say that bias (used in this setting) is the difference between the predicted value and the true value, when averaging over different data sets. Variance (again when used in this setting) indicates how big the change in parameters, and hence in model predictions, we get from fitting to different data sets. Let us say you measure the temperature on ten different days, and for each day, you fit a model, like before. These may be the results:\n\n\n\n\n\n\n\n\n(a) Caption 1\n\n\n\n\n\n\n\n(b) Caption 2\n\n\n\n\n\n\n\n(c) ?(caption)\n\n\n\n\nFigure 27.3: Caption 1\n\n\nSource: over_and_underfitting.ipynb\nThe blue dots are our “original” data points, plotted for reference, while the red lines corresponds to models fitted for each day’s measurements. Due to fluctuations in the measurements, they are different, but note how the difference is related to the model complexity. Our linear models (\\(M=1\\)) are all quite similar, but neither capture the pattern in the data very well, so they all have high bias. The overly complex models (\\(M=9\\)) have zero bias on their repective dataset, but high variance. The optimal choice is likely somewhere in-between (hence the tradeoff), as for the \\(M=4\\) models, which perform well without being overly sensitive to fluctuations in data. Since the value of \\(M\\) is chosen by us, we call it a hyperparameter, to separate it from the regular parameters which are optimised by minimising the loss function."
  },
  {
    "objectID": "machine_learning_overview.html#model-selection-in-practice",
    "href": "machine_learning_overview.html#model-selection-in-practice",
    "title": "26  Machine learning - an introduction",
    "section": "27.2 Model selection in practice",
    "text": "27.2 Model selection in practice\nMove or delete ??"
  },
  {
    "objectID": "decision_trees.html#decision-trees-for-categorical-classification",
    "href": "decision_trees.html#decision-trees-for-categorical-classification",
    "title": "27  Decision trees",
    "section": "27.1 Decision trees for categorical classification",
    "text": "27.1 Decision trees for categorical classification\nFor a trivial example, lets us say you want to go and spend the day at the beach. There are certain criteria that should be fullfilled for that to be a good idea, and some of them depend on each other. A decision tree could look like this:\n\n\n\n\n\n\n\n\nG\n\n \n\nA\n\n   Go to the beach?    \n\nB\n\n Workday?  \n\nC\n\n Don’t go  \n\nB-&gt;C\n\n  Yes  \n\nD\n\n Sunny?  \n\nB-&gt;D\n\n  No  \n\nE\n\n Temp &gt; 18 C  \n\nD-&gt;E\n\n  Yes  \n\nF\n\n Temp &gt; 23 C  \n\nD-&gt;F\n\n  No  \n\nG\n\n  Go to the beach   \n\nE-&gt;G\n\n  Yes  \n\nH\n\n  Don’t go   \n\nE-&gt;H\n\n  No  \n\nI\n\n  Go to the beach   \n\nF-&gt;I\n\n  Yes  \n\nJ\n\n  Don’t go   \n\nF-&gt;J\n\n  No  \n\n\nFigure 27.1: A very simple decision tree.\n\n\n\n\nDepending on input data such as the weather, we end up following a certain path from the root node, along the branches, down to the leaf node, which returns the final decision for these given observations. The botany analogies are not strictly necessary, but at least we see where the name decision tree comes from.\nStudying the above tree structure more closely, we see that there are several possible ways of structuring it, that would lead to the same outcome. We can choose to first split on the Sunny node, and split on Workday afterwards. Drawing it out on paper, however, would show that this structure needs a larger total number of nodes, since we always need to split on Workday. Hence, the most efficient tree is the one that steps through the observables in order of descending importance.\nThe basic algorithm for buiding a decision tree (or growing it, if you prefer) on categorical data, can be written out quite compactly. Consider the following pseudo-code: (ref https://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf)\nTODO rename attributes into features?\n\nfunction BuildTree(examples, target_attribute, attributes)\n  tree ← a single node, so far without any label\n  if all examples are of the same classification then \n    give tree a label equal to the classification\n    return tree\n  else if attributes is empty then\n    give tree a label equal the most common value of target_attribute in examples\n    return tree\n  else\n    A ← the attribute from attributes with highest Importance(examples)\n\n    for each value v of A do\n      Examples_v ← the subset of examples where A has the value v\n      subtree ← BuildTree(Examples_v, Target_attributes, Attributes - {A})\n      add a branch with label (A = v) to tree, and below it, add the tree subtree\n    \n    return tree\n\nThis is the original ID3 algorithm (Quinlan 1986). Note how it works recursively – for each new feature, the function calls itself to build a subtree.\n\nQuinlan, J. Ross. 1986. “Induction of Decision Trees.” Machine Learning 1: 81–106.\n\nThe same algorithm is shown and explained in section 19.3 in Russell and Norvig, although they fail to specify that this is ID3.\n\nWe start by creating a node, which becomes a leaf node either if it classifies all examples correctly (no reason to split), or if there are no more features left (not possible to split). Otherwise, we find the most important feature by calling Importance(examples), and proceed to make all possible splits. Now, the magic happens in the Importance function. How can we quantify which feature is best to discriminate on? We have in Section 18.1 met a useful definition from information theory, which was the Shannon entropy: \\[\n    H(f) \\coloneqq-\\sum_{i} f_i\\ \\log_2 f_i\n    \\qquad\\text{\\color[RGB]{119,119,119}\\small(with \\(0\\cdot\\log 0 \\coloneqq 0\\))}\n\\]\nwhere the \\(f_i\\) are frequency distributions. If we stick to the simple example of our target features being “yes” or “no”, we can write out the summation like so:\n\\[\n    H() = - f_{\\mathrm{yes}} \\log_2 f_{\\mathrm{yes}} - f_{\\mathrm{no}} \\log_2 f_{\\mathrm{no}}\n\\]\nLet us compute the entropy for two different cases, to see how it works. In the first case, we have 10 examples: 6 corresponding to “yes”, and four corresponding to “no”. The entropy is then\n\\[\n    H() = - (6/10) \\log_2 (6/10) - (4/10) \\log_2 (4/10) = 0.97\n\\]\nIn the second case, we still have 10 examples, but nearly all of the same class: 9 examples are “yes”, and 1 is “no”:\n\\[\n    H() = - (9/10) \\log_2 (9/10) - (1/10) \\log_2 (1/10) = 0.47\n\\]\nInterpreting the entropy as a measure of impurity in the set of examples, we can guess (or compute, using \\(0\\cdot \\log_2 0 = 0\\)) that the lowest possible entropy occurs for a set where all are of the same class. When doing classification, this is of course what we aim for – separating all examples into those corresponding to “yes” and those corresponding to “no”. A way of selecting the most important feature is then to choose the one where we expect the highest reduction in entropy, caused by splitting on this feature. This is called the information gain, and is generally defined as\n\\[\n    Gain(A, S) \\coloneqq H(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} H(S_v) \\,,\n\\] where \\(A\\) is the feature under consideration, \\(Values(A)\\) are all the possible values that \\(A\\) can have. Futher, \\(S\\) is the set of examples, and \\(S_v\\) is the subset containing examples where \\(A\\) has the value \\(v\\). Looking again at the binary yes/no case, it looks a little simpler. Using the feature Sunny as \\(A\\), we get:\n\\[\n    Gain(\\texttt{Sunny}, S) = H(S) - \\left(\\frac{S_{\\texttt{Sunny=yes}}}{S} H(S_{\\texttt{Sunny=yes}}) + \\frac{S_{\\texttt{Sunny=no}}}{S} H(S_{\\texttt{Sunny=no}})\\right) \\,.\n\\]\nThis equation can be read as “gain equals the original entropy before splitting on Sunny, minus the weighted entropy after splitting”, which is what we were after."
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "28  Neural networks",
    "section": "",
    "text": "\\[\n\\DeclarePairedDelimiters{\\set}{\\{}{\\}}\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\]"
  }
]